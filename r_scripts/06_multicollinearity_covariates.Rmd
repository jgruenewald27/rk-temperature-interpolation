---
title: "multicollinearity_covariates"
author: "Johannes Gruenewald"
date: "2025-07-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Load Required Libraries

# Spatial Data Handling & Processing
library(sf)
library(sp)
library(terra)
library(raster)
library(stars)
library(spdep)

# Spatial Analysis & Interpolation
library(gstat)
library(randomForest)
library(nlme)
library(automap)
library(mgcv)

# Data Manipulation & Wrangling
library(tidyverse)

# Data Visualization
library(ggplot2)
library(scales)
library(gridExtra)
library(corrplot)

# Mapping & Interactive Visualization
library(tmap)

# Additional
library(lattice)
library(car)
```

--------------------------------------------------------------------------------

- Loads full-year 2023 station temperature data and extracts aligned covariates
at station locations.
- Z-standardises covariates and visualises their distributions.
- Computes Pearson and Spearman correlations between yearly mean station 
temperature and each covariate, including regression-based scatterplots.
- Assesses multicollinearity among predictors using Variance Inflation Factors 
(VIF) within an MLR framework.

--------------------------------------------------------------------------------

```{r Load temp data}

# Complete data import and pre-processing of the temperature

# Read temperature data for the entire year of 2023
temp <- sf::read_sf('../data/temp_HD/sensor_data_year_2023.geojson')

# Removes rows where temp is NA
temp <- temp %>% filter(!is.na(temperature))

# Reproject the data for Germany
temp <- st_transform(temp, crs="EPSG:32632")

# Clean the data from outliers -> currently working with hard, manual breaks
# Currently nothing is removed by this step because no measurements are above 
# 40°C for the year 2023
temp_clean <- temp %>% filter(temperature >= -35, temperature <= 40)

# Drop all measurement stations which didn't collect data the entire year around
temp_clean_station <- temp_clean %>%
  mutate(dateobserved = as.POSIXct(dateobserved, tz = "Europe/Berlin")) %>%
  filter(!entity_id %in% c(
    "hd:DE_Heidelberg_69120_12:WeatherObserved",
    "hd:DE_Heidelberg_69120_34:WeatherObserved",
    "hd:DE_Heidelberg_69123_41:WeatherObserved",
    "hd:DE_Gaiberg_69251_21:WeatherObserved",
    "hd:DE_Heidelberg_46:WeatherObserved"
  ))

# Check if exactly 5 stations were dropped
cat("Number of stations BEFORE cleaning: ",
    length(unique(na.omit(temp_clean$entity_id))), "\n")

cat("Number of stations AFTER cleaning:  ",
    length(unique(na.omit(temp_clean_station$entity_id))), "\n")

# Calculate hourly mean temp for cleaned data
temp_clean_station_hm <- temp_clean_station %>%
    mutate(hour = floor_date(dateobserved, unit = "hour")) %>%   # round down
    group_by(entity_id, hour) %>%
    summarise(hm_temp = mean(temperature, na.rm = TRUE), .groups = "drop")
```

```{r Load and preprocess covariates}

# Import of pre-processed temp data and import and pre-processing of covariates 

#Shortcut -> import pre-processed hourly means for the entire year
temp_clean_station_hm <- sf::read_sf('../data/temp_HD/others/temp_clean_station_hm.gpkg')

# Reproject pre-processed temp data
temp_clean_station_hm <- st_transform(temp_clean_station_hm, crs="EPSG:32632")

# Drop Gaiberg station as it is outside of AOI
temp_clean_station_hm <- temp_clean_station_hm %>%
  filter(!entity_id %in% c(
    "hd:DE_Gaiberg_69251_21:WeatherObserved"
  ))

# Import AOI and station coords
hd_bounds <- sf::read_sf('../data/AOI/OSM_boundaries_HD.geojson')
station_coords_sf <- sf::read_sf('../data/temp_HD/station_coords.gpkg')

# Import the covariates
dem <- terra::rast("../data/elevation/hd_elevation_4326.tif")
tch <- terra::rast("../data/canopy_height/hd_canopy_height_4326.tif")
ghs_bh <- terra::rast("../data/building_height/hd_ANBH_4326.tif")

# Mask out all values which are assigned to water and have value 101
# Not part of this area. But needs to be masked as well: 102 Snow/ice; 103 No data
tch[tch == 101] <- NA

# Reproject the data
hd_bounds <- st_transform(hd_bounds, crs = st_crs(temp_clean_station_hm))
station_coords_sf <- st_transform(station_coords_sf, crs = st_crs(temp_clean_station_hm))
dem <- terra::project(dem, y = crs(temp_clean_station_hm))
tch <- terra::project(tch, y = crs(temp_clean_station_hm))
ghs_bh <- terra::project(ghs_bh, y = crs(temp_clean_station_hm))

# Check if any CRS mismatches
if (!identical(crs(tch), crs(dem))) {
  stop("CRS mismatch: Tree canopy height and DEM don't have the same CRS.")
}
if (!identical(crs(ghs_bh), crs(dem))) {
  stop("CRS mismatch: Building height and DEM don't have the same CRS.")
}
cat("All covariates share the same CRS.\n")

# Convert to SpatialPointsDataFrame
station_coords <- as(station_coords_sf, "Spatial")

# Create a Raster Stack for all covariate information
cat("Checking input raster resolutions in metres:\n")
cat(sprintf("DEM:    %.2f x %.2f\n", terra::res(dem)[1], terra::res(dem)[2]))
cat(sprintf("TCH:    %.2f x %.2f\n", terra::res(tch)[1], terra::res(tch)[2]))
cat(sprintf("GHS_BH: %.2f x %.2f\n\n", terra::res(ghs_bh)[1], terra::res(ghs_bh)[2]))

# Align canopy height and building height to DEM resolution and extent
tch_aligned    <- terra::resample(tch, dem, method = "bilinear")
ghs_bh_aligned <- terra::resample(ghs_bh, dem, method = "near")

# Check alignment after resampling
if (!terra::compareGeom(dem, tch_aligned, ghs_bh_aligned, stopOnError = FALSE)) {
  stop("Covariate rasters are not perfectly aligned after resampling. Check extent/resolution/CRS.")
}

# Combine layers into a single SpatRaster stack
env_stack <- c(dem, tch_aligned, ghs_bh_aligned)

# Rename layers for clarity
names(env_stack) <- c("elevation", "canopy_height", "building_height")

# Code to include lat and lon information in raster stack

# Also add lat and lon to the raster stack
# Create coordinate rasters
lon <- terra::xFromCell(env_stack, 1:ncell(env_stack))
lat <- terra::yFromCell(env_stack, 1:ncell(env_stack))

# Convert to raster layers
lon_raster <- dem
lat_raster <- dem
values(lon_raster) <- lon
values(lat_raster) <- lat
names(lon_raster) <- "lon"
names(lat_raster) <- "lat"

# Stack the three raster layers
grids <- c(env_stack, lon_raster, lat_raster)

# Preparation of grid as a covariate for running RK
grids_ras <- stack(grids)

# Convert to SpatialGridDataFrame
grids_sp <- as(grids_ras, "SpatialGridDataFrame")
```
--------------------------------------------------------------------------------

```{r Adding covariates to temperature data}

# Convert Spatial object to RasterBrick then to SpatRaster for terra workflows
r_brick   <- brick(grids_sp)
grid_terra <- rast(r_brick)

# Extract raster covariate values at the locations of the point features
ex <- terra::extract(grid_terra, temp_clean_station_hm)

# Aggregate extracted values for points with duplicated IDs
# Compute the mean value per ID
ex_agg <- aggregate(. ~ ID, data = ex, FUN = mean, na.rm = TRUE)

# Reorder aggregated rows to match the original feature order + drop ID column
ex_agg <- ex_agg[match(seq_len(nrow(temp_clean_station_hm)), ex_agg$ID), -1]

# Combine original station attributes with the extracted covariate data
temp_clean_station_hm_cov <- cbind(as.data.frame(temp_clean_station_hm), ex_agg)
```

--------------------------------------------------------------------------------

Z-Standardization: results in mean of 0 and standard deviation of 1

```{r z-standardization of covariates}

# Covariates to be z-standardized
covars <- c("elevation", "canopy_height", "building_height")

# Compute mean and standard deviation for each covariate
mu  <- sapply(temp_clean_station_hm_cov[, covars], 
              function(x) mean(x, na.rm = TRUE))
sdv <- sapply(temp_clean_station_hm_cov[, covars], 
              function(x) sd(x,   na.rm = TRUE))

# Check for covariates with zero or undefined variance and prevent division by zero
zero_sd <- sdv == 0 | is.na(sdv)
if (any(zero_sd)) {
  warning("Zero-variance covariate(s) set to 0: ",
          paste(names(sdv)[zero_sd], collapse = ", "))
  sdv[zero_sd] <- 1
}

# Create z-standardized versions of each covariate
for (nm in covars) {
  temp_clean_station_hm_cov[[paste0(nm, "_z")]] <-
    (temp_clean_station_hm_cov[[nm]] - mu[[nm]]) / sdv[[nm]]
}

# Store means and SDs for reproducibility
z_params <- data.frame(covariate = names(mu), 
                       mean = as.numeric(mu), 
                       sd = as.numeric(sdv))
```

```{r}
# Convert covariates to long format for faceted plotting
df_long <- temp_clean_station_hm_cov %>%
  select(all_of(covars)) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "value") %>%
  filter(!is.na(value))

# Create histograms of each covariate
ggplot(df_long, aes(x = value)) +
  geom_histogram(bins = 40, alpha = 0.85) +
  facet_wrap(~ variable, scales = "free", ncol = 1) +
  labs(
    title = "Distribution of Covariates",
    x = NULL, y = "Count"
  ) +
  theme_minimal(base_size = 13) +
  theme(panel.grid.minor = element_blank())
```

--------------------------------------------------------------------------------

Calculate correlation between the three covariates and the dependent variable 
temperature using a linear regression model

```{r Calculate corr between covariates and dependent variable}

# Pick relevant columns
df <- temp_clean_station_hm_cov %>%
  st_drop_geometry() %>%   # remove geometry column
  dplyr::select(entity_id, hm_temp, elevation, canopy_height, building_height)

# Aggregate temperature per station (mean across time)
station_df <- df %>%
  group_by(entity_id, elevation, canopy_height, building_height) %>%
  summarise(mean_temp = mean(hm_temp, na.rm = TRUE), .groups = "drop")

# Compute correlation matrix (Pearson + Spearman)
vars <- c("mean_temp", "elevation", "canopy_height", "building_height")

corr_p <- cor(station_df[vars], use = "pairwise.complete.obs", 
              method = "pearson")
corr_s <- cor(station_df[vars], use = "pairwise.complete.obs", 
              method = "spearman")

print("Pearson correlation:")
print(corr_p)

print("Spearman correlation:")
print(corr_s)

# Reshape to long format
station_long <- station_df %>%
  pivot_longer(cols = c(elevation, canopy_height, building_height),
               names_to = "covariate", values_to = "value")

# Compute Pearson + Spearman per covariate
cors <- station_long %>%
  group_by(covariate) %>%
  summarise(
    pearson  = cor(value, mean_temp, method = "pearson",  use = "pairwise.complete.obs"),
    spearman = cor(value, mean_temp, method = "spearman", use = "pairwise.complete.obs")
  ) %>%
  mutate(
    label_pearson  = sprintf("italic(r) == %.2f", pearson),
    label_spearman = sprintf("italic(rho) == %.2f", spearman)
  )

p_corr_temp_cov <- ggplot(station_long, aes(x = value, y = mean_temp)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = TRUE, color = "darkred") +
  facet_wrap(~ covariate, scales = "free_x", nrow = 1, labeller = labeller(
               covariate = c(
                 elevation = "Elevation [m]",
                 canopy_height = "Canopy Height [m]",
                 building_height = "Building Height [m]"
               ))) +
    geom_text(data = cors, aes(label = label_spearman),
            x = -Inf, y = Inf, hjust = -0.2, vjust = 2.3,
            parse = TRUE, size = 5, fontface = "bold",
            inherit.aes = FALSE) +
    geom_text(data = cors, aes(label = label_pearson),
            x = -Inf, y = Inf, hjust = -0.2, vjust = 4.5,
            parse = TRUE, size = 5,
            inherit.aes = FALSE) +
  scale_y_continuous(limits = c(9, 15)) +
  scale_x_continuous(expand = expansion(mult = c(0.02, 0.02))) +
  labs(x = NULL, y = "Yearly Mean Temperature [°C]") +
  theme_bw() +
  theme(
    strip.text = element_text(face = "bold", size = 14),
    plot.title = element_text(face = "bold", size = 20, hjust = 0.5,
                              margin = margin(t = 10, b = 10)),
    axis.text = element_text(size = 12),
    axis.title.y = element_text(margin = margin(r = 15))
  )

p_corr_temp_cov

# Save correlation plot to file
# ggsave("../plots_report/correlation_temperature_vs_covariates.png", 
#        p_corr_temp_cov, width = 12, height = 6, dpi = 300)
```

```{r Check for multicollineraity}

# Define MLR model
mlr_coll <- lm(hm_temp ~ elevation + canopy_height + building_height, data = temp_clean_station_hm_cov)

# Compute VIF for each predictor
vif_df <- car::vif(mlr_coll) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("covariate") %>%
  rename(VIF = 2)

# Plot VIF values
p_vif <- ggplot(vif_df, aes(x = covariate, y = VIF, fill = covariate)) +
  geom_col(color = "grey30", linewidth = 0.3, width = 0.5) +
  geom_hline(yintercept = 5, linetype = "dashed", color = "grey60", linewidth = 0.5) +
  scale_fill_manual(values = c("building_height" = "darkorange3", 
                               "canopy_height"   = "darkolivegreen",
                               "elevation"       = "burlywood2"),
                    guide = "none") +
  scale_x_discrete(labels = c("building_height" = "Building Height",
                              "canopy_height"   = "Canopy Height",
                              "elevation"       = "Elevation")) +
  geom_hline(yintercept = 5,  linetype = "dashed", 
             color = "black", linewidth = 0.8) +
  annotate("text", x = 0.5, y = 5.5, label = "VIF > 5 selected multicollinearity threshold",
           hjust = 0, size = 4, color = "grey40") +
  geom_text(aes(label = round(VIF, 2)),
          vjust = -0.5, size = 3.8) +
  labs(x = "Covariate", y = "VIF") +
  theme_minimal(base_size = 11) +
  theme(
    axis.text.x = element_text(angle = 30, hjust = 0.8),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.major.y = element_line(color = "grey85"),
    axis.title.x = element_text(margin = margin(t = 15)),
    axis.title.y = element_text(margin = margin(r = 15))
  )

p_vif

# Save VIF plot to file
# ggsave("../plots_report/Multicollinearity_covariates.png", 
#        p_vif, width = 7, height = 4, dpi = 300)
```
