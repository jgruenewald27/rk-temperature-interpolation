---
title: "test_RK_daily"
author: "Johannes Gruenewald"
date: "2025-04-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# Spatial Data Handling & Processing
library(sf)
library(sp)
library(terra)
library(stars)
library(spdep)

# Spatial Analysis & Interpolation
library(gstat)
library(randomForest)
library(nlme)
library(automap)
library(mgcv)

# Data Manipulation & Wrangling
library(tidyverse)
library(dplyr)
library(magrittr)

# Data Visualization
library(ggplot2)
library(scales)
library(gridExtra)

# Mapping & Interactive Visualization
library(mapview)
library(tmap)

# Additional
library(lattice)
```

--------------------------------------------------------------------------------

Load the temperature measurements as input data

```{r}
# Read temperature points
temp <- sf::read_sf('../data/temp_HD/sensor_data_20230107_31days.gpkg')

# Removes only rows where `temperature` is NA
temp <- temp[!is.na(temp$temperature), ] 

# Ensure dateobserved is in the correct date-time format with German timezone
temp <- temp %>%
  mutate(dateobserved = as.POSIXct(dateobserved, format="%Y-%m-%d %H:%M:%S", tz = "Europe/Berlin"))

# Calculate hour count since epoch, rounded down to current hour
temp$chour <- floor(as.numeric(temp$dateobserved) / 3600)

# Test the results
print(format(as.POSIXct(temp$chour[66163] * 3600, origin = "1970-01-01", tz = "Europe/Berlin"), "%Y-%m-%d %H:%M:%S %Z"))

# Reproject the temperature data
temp_hm <- st_transform(temp_hm, crs="EPSG:32632")
```

--------------------------------------------------------------------------------

Preprocessing of the temperature data
Calculate mean temperature for each measurement station for each day for the month of July

```{r}
# Calculate daily mean temperature for each station in July
temp_dm <- temp %>%
  filter(format(dateobserved, "%m") == "07") %>%
  mutate(date = as.Date(dateobserved)) %>%
  group_by(stationname, date) %>%
  summarise(daily_mean_temperature = mean(temperature, na.rm = TRUE), .groups = "drop")
```

Calculate hourly means for each unique stations for the month of July

```{r}
temp_hm <- temp %>%
  filter(format(dateobserved, "%m") == "07") %>%  # Keep only July data
  group_by(stationname, chour) %>%                # Group by station and hour
  summarise(mh_temp = mean(temperature, na.rm = TRUE), .groups = "drop") %>%
  mutate(hour = as.POSIXct(chour * 3600, origin = "1970-01-01", tz = "Europe/Berlin"))
```

--------------------------------------------------------------------------------

Extract coordinates of the measurement stations 
-> maybe use them as covariates

```{r}
# Extract coordinates and data into a data frame
coords <- st_coordinates(temp)
locs <- cbind(temp, coords)
locs <- as.data.frame(locs)

# 4. Keep only one row per station and select relevant columns
station_coords <- locs %>%
  distinct(stationname, .keep_all = TRUE) %>%
  select(stationname, X, Y)

# 5. Inspect the structure
str(station_coords)
```

--------------------------------------------------------------------------------

Load and process environmental covariates

Load Elevation

```{r}
# Load the elevation data
dem <- terra::rast("../data/DEM/hd_elevation_4326.tif")

# Reproject the elevation data to match correct CRS
dem <- terra::project(dem, y = crs(temp_hm))
```

Load Tree Canopy Height

```{r}
# Load the elevation data
tch <- terra::rast("../data/Canopy_height/clip_Forest_height_2019_NAFR.tif")

# Mask out all values which are assigned to water and have value 101
# Not part of this area. But needs to be masked as well: 102 Snow/ice; 103 No data
tch[tch == 101] <- NA

# Reproject the elevation data to match correct CRS
tch <- terra::project(tch, y = crs(temp_hm))

# Check if the two covariates have the same crs
crs(tch) == crs(dem)
```

Load Building Height

```{r}
# Load the elevation data
ghs_bh <- terra::rast("../data/building_height/clip_GHS_BUILT_H_AGBH_E2018_GLOBE_R2023A_54009_100_V1_0_R4_C19.tif")

# Reproject the elevation data to match correct CRS
ghs_bh <- terra::project(ghs_bh, y = crs(temp_hm))

# Check if CRS is correct
crs(ghs_bh) == crs(dem)
```

Create a Raster Stack for all the covariate information

```{r}
# Check resolution of each raster
terra::res(dem)
terra::res(tch)
terra::res(ghs_bh)

# Align TCH and building height to DEM resolution and extent
tch_aligned    <- terra::resample(tch, dem, method = "bilinear")
ghs_bh_aligned <- terra::resample(ghs_bh, dem, method = "near")

# Stack all layers
env_stack <- c(dem, tch_aligned, ghs_bh_aligned)

# Rename layers for clarity
names(env_stack) <- c("elev", "canopy", "bldg")

env_stack
```

The GHS_BH data has a much coarser resolution (about 95 m) compared to DEM (23 m). It represents building heights, which are:
- Discrete or categorical-like (often containing sharp edges)
- Possibly less spatially smooth
For such data, bilinear interpolation can introduce unrealistic intermediate values (e.g., building heights that don't exist in reality) because it averages neighboring cells.

!Use Nearest neighborto preserve building height values without smoothing!

```{r}
plot(env_stack$bldg)
```

Also add lat and lon to the raster stack

```{r}
# Create coordinate rasters
lon <- terra::xFromCell(env_stack, 1:ncell(env_stack))
lat <- terra::yFromCell(env_stack, 1:ncell(env_stack))

# Convert to raster layers
lon_raster <- dem
lat_raster <- dem
values(lon_raster) <- lon
values(lat_raster) <- lat
names(lon_raster) <- "lon"
names(lat_raster) <- "lat"
```

Stack the three raster layers

```{r}
grids <- c(env_stack, lon_raster, lat_raster)

grids
```

--------------------------------------------------------------------------------

Adding covariates to temperature data

```{r}
# Extract raster values at the point locations
extracted_vals <- terra::extract(grids, temp_hm)

# Remove the first column (ID)
extracted_vals <- extracted_vals[,-1]

# Combine the point data with the extracted raster values
temp_hm.ex <- cbind(as.data.frame(temp_hm), extracted_vals)

temp_hm.ex
```

Old version: Extract values from all the raster and add to temperature data at point locations

```{r}
# Extract raster values at the temp locations
elevation_values <- terra::extract(dem, temp_dm)

# Extract raster values at the temp locations
building_height_values <- terra::extract(ghs_bh, temp_dm)

# Extract tree canopy height values at the temp locations
canopy_heigth_values <- terra::extract(tch, temp_dm)

# Use dplyr to add the extracted values as a new column in the sf object
temp_dm <- temp_dm %>%
  mutate(elevation = elevation_values[[2]])

# Use dplyr to add the extracted values as a new column in the sf object
temp_dm <- temp_dm %>%
  mutate(building_height = building_height_values[[2]])

# Use dplyr to add the extracted values as a new column in the sf object
temp_dm <- temp_dm %>%
  mutate(tree_canopy_height = canopy_heigth_values[[2]])

# Check the updated sf object
print(temp_dm)
```

--------------------------------------------------------------------------------

Descriptive statistics and plots to analyse the temperature data

Example with a visualization of three measurement stations

```{r}
# Visualize three meteorological stations
Königstuhl_10 <- subset(temp_hm.ex, stationname=="69117_Königstuhl_10", select=c("mh_temp", "chour"))
Bergheimer_Str._1	<- subset(temp_hm.ex, stationname=="69115_Bergheimer_Str._1", select=c("mh_temp", "chour"))
In_den_Weinäckern <- subset(temp_hm.ex, stationname=="69251_In_den_Weinäckern", select=c("mh_temp", "chour"))

par(mfrow=c(1,3))

scatter.smooth(Königstuhl_10$chour, Königstuhl_10$mh_temp, xlab="Cumulative hours",
               ylab="Mean hourly temperature", ylim=c(-12,28), col="grey")
scatter.smooth(Bergheimer_Str._1$chour, Bergheimer_Str._1$mh_temp, xlab="Cumulative hours",
               ylab="Mean hourly temperature", ylim=c(-12,28), col="grey")
scatter.smooth(In_den_Weinäckern$chour, In_den_Weinäckern$mh_temp, xlab="Cumulative hours",
               ylab="Mean hourly temperature", ylim=c(-12,28), col="grey")
```

Plot for all measurement stations

```{r}
# Get all unique station names
stations <- unique(temp_hm.ex$stationname)

# Set up the plot layout: Adjust rows/cols depending on how many you want per page
n <- length(stations)
par(mfrow = c(3, 3))  # Show only 9 plots per page

# Loop over each station and plot
for (s in stations) {
  station_data <- subset(temp_hm.ex, stationname == s, select = c("mh_temp", "chour"))
  
  # Plot with scatter.smooth
  scatter.smooth(
    station_data$chour,
    station_data$mh_temp,
    main = s,
    xlab = "Cumulative hours",
    ylab = "Mean hourly temperature",
    ylim = c(-12, 28),
    col = "grey"
  )
}
```

Descriptive statistics for each station

```{r}
# Compute descriptive statistics for each station
station_stats <- temp_hm.ex %>%
  group_by(stationname) %>%
  summarise(
    n_obs        = n(),
    temp_mean    = mean(mh_temp, na.rm = TRUE),
    temp_sd      = sd(mh_temp, na.rm = TRUE),
    temp_min     = min(mh_temp, na.rm = TRUE),
    temp_max     = max(mh_temp, na.rm = TRUE),
    chour_min    = min(chour, na.rm = TRUE),
    chour_max    = max(chour, na.rm = TRUE)
  )

# View first few rows
print(station_stats)
```

Stations that didn't measure the entire time or had missing measurements:

Likely missing values:
- 69115_Czernyring_22/11-12	726
- 69117_Königstuhl_10	724
- 69117_Rodelweg	708
- 69118_Am_Büchsenackerhang_39	714
- 69118_B37	711
- 69118_Wendestelle_Hbw	711
- 69123_Gewannn_Die_Äußeren_Rauschen	724
- 69123_Grenzhöfer_Weg	711	
- 69251_In_den_Weinäckern	703

Started later:
- 69115_Poststraße_15	448	
- 69123_Richard-Drach-Straße_7	87

--------------------------------------------------------------------------------

Fit Linear Regression model as a test

CONTINUE UNDERSTANDING THIS PART AND FURTHER INVESTIGATE HOW HOUR OF THE DAY INFO CAN BE INCLUDED AS A COVARIATE
IF NOTHING WORK -> CONTINUE WITH HENGL 2009 AND FURTHER APPLY HIS PROCESS AND TRY TO UNDERSTAND THE CONNECTION BETWEEN 
THE REGRESSION PART AND THE KRIGING OF THE RESIDUALS

```{r}
lm.temp_hm <- lm(mh_temp ~ elev + canopy + bldg + lon + lat + chour,
                data = temp_hm.ex)

summary(lm.temp_hm)
```



--------------------------------------------------------------------------------

Test Random Forest (RF) instead of regression models to model complex relationships and see if the performance is similar to flexible regression models.

Splitting the data in train and test data

```{r}
nrow(temp_dm)

# Splitting data in train and test data
split_dm <- sample.split(temp_dm$daily_mean_temperature, SplitRatio = 0.7)
length(split_dm)

train_dm <- subset(temp_dm, split_dm == "TRUE")
test_dm <- subset(temp_dm, split_dm == "FALSE")
```

Fit the Random Forest Model

```{r}
#make this example reproducible
set.seed(10)

#fit the random forest model
rf_model_dm <- randomForest(
  formula = daily_mean_temperature ~ elevation + tree_canopy_height + building_height,
  data = train_dm,
  mtry = 3,
  importance = TRUE,
  na.action = na.omit
)

rf_model_dm
importance(rf_model_dm)

#find number of trees that produce lowest test MSE
which.min(rf_model_dm$mse)

#find RMSE of best model
sqrt(rf_model_dm$mse[which.min(rf_model_dm$mse)]) 

#plot the test MSE by number of trees
plot(rf_model_dm)

#Importance plot
importance(rf_model_dm)

#produce variable importance plot
varImpPlot(rf_model_dm) 
```

Tune the model
This function produces the following plot, which displays the number of predictors used at each split when building the trees on the x-axis and the out-of-bag estimated error on the y-axis:

```{r}
model_tuned_dm <- tuneRF(
  x = data.frame(
    elevation = train_dm$elevation,
    building_height = train_dm$building_height,
    tree_canopy_height = train_dm$tree_canopy_height
  ),
  y = train_dm$daily_mean_temperature,  # define response variable
  ntreeTry = 500,
  mtryStart = 3,
  stepFactor = 1.5,
  improve = 0.01,
  trace = FALSE  # don't show real-time progress
)
```

We can see that the lowest OOB error is achieved by using 3 randomly chosen predictors at each split when building the trees.

Use the Final Model to Make Predictions and derive Residuals

```{r}
# Predict on training data
predicted_dm_rf <- predict(rf_model_dm, newdata=test_dm)

# Calculate residuals: observed - predicted
residuals_dm_rf <- temp_dm$daily_mean_temperature - predicted_dm_rf

plot(residuals_dm_rf)
```

--------------------------------------------------------------------------------





Histogram to check data distribution

```{r}
# Set up a 1-row, 3-column plotting area
par(mfrow = c(1, 3))

# Plot the histograms
hist(july_mean_temp$elevation, main = "Elevation", xlab = "Elevation (m)", col = "lightblue")
hist(july_mean_temp$tree_canopy_height, main = "Tree Canopy Height", xlab = "Height (m)", col = "lightgreen")
hist(july_mean_temp$building_height, main = "Building Height", xlab = "Height (m)", col = "lightcoral")
```

log-transformed the data to check possible changes to distribution

```{r}
# Set up a 1-row, 3-column plotting area
par(mfrow = c(1, 3))

# Plot the log-transformed histograms
hist(log(july_mean_temp$elevation + 1), 
     main = "Log(Elevation + 1)", 
     xlab = "log(Elevation)", 
     col = "lightblue")

hist(log(july_mean_temp$tree_canopy_height + 1), 
     main = "Log(Tree Canopy Height + 1)", 
     xlab = "log(Height)", 
     col = "lightgreen")

hist(log(july_mean_temp$building_height + 1), 
     main = "Log(Building Height + 1)", 
     xlab = "log(Height)", 
     col = "lightcoral")
```

!!Try regression models!!

Build a MLR

```{r}
model <- lm(mean_temperature ~ elevation + tree_canopy_height + building_height, data = july_mean_temp)

# Print model summary
summary(model)

plot(model)  # Look for non-random patterns in residuals
```

Stepwise MLR

```{r}
# stepwise variable selection
model.MLR.step <- step(model, direction="both")

# summary of the new model using stepwise covariates selection
summary(model.MLR.step)

install.packages("gtsummary")

model.MLR.step |> tbl_regression()
```

Interpretation of MLR performance

```{r}
# graphical diagnosis of the regression analysis
par(mfrow=c(2,2))
plot(model.MLR.step)
par(mfrow=c(1,1))
```

GLS model

```{r}
# Extract coordinates (assuming `geom` is the column with spatial data)
july_mean_temp$coords <- st_coordinates(july_mean_temp$geom)

# Separate the coordinates into `x` and `y` columns
july_mean_temp$x <- july_mean_temp$coords[, 1]
july_mean_temp$y <- july_mean_temp$coords[, 2]

gls_model <- gls(mean_temperature ~ elevation + tree_canopy_height + building_height,
                 data = july_mean_temp,
                 correlation = corExp(form = ~ x + y, nugget = TRUE))

summary(gls_model)
```

Test RF instead of regression model to model complex relationships and see if the performance is similar

Try RF

```{r}
# 2. Fit Random Forest model
rf_model <- randomForest(
  mean_temperature ~ elevation + tree_canopy_height + building_height,
  data = july_mean_temp,
  ntree = 500,
  importance = TRUE,
  na.action = na.omit
)

# View basic model summary
print(rf_model)

# View variable importance
importance(rf_model)
varImpPlot(rf_model)
```

```{r}
# 3. Predict and calculate residuals
july_mean_temp$rf_pred <- predict(rf_model)
july_mean_temp$residuals <- july_mean_temp$mean_temperature - july_mean_temp$rf_pred
```


```{r}
# 4. Fit variogram to residuals
fitted <- autofitVariogram(residuals ~ 1, input_data = july_mean_temp)
fitted_vgm <- fitted$var_model
plot(fitted$exp_var, fitted$var_model)
```

Create prediction locations for the kriging process including the covariates

```{r}
# Convert raster to points, including values, and remove NAs
predLocations <- terra::as.points(dem, values=TRUE, na.rm=TRUE)

# Reproject the elevation data to match correct CRS
predLocations <- terra::project(predLocations, y = crs(july_mean_temp))

# Check CRS of locations
crs(predLocations)

# Convert the points to a SpatialPointsDataFrame
predLocations <- as(predLocations, "Spatial")

# Convert sf object to sp object
july_mean_temp_sp <- as(july_mean_temp, "Spatial")

# Set CRS of predLocations_sp to match july_mean_temp
sp::proj4string(predLocations) == sp::proj4string(july_mean_temp_sp)

# Display the updated gridded points
class(predLocations)
```

```{r}
# 6. Interpolate residuals by kriging
kriged_resid <- krige(residuals ~ 1,
                      loc = july_mean_temp_sp,
                      newdata = predLocations,
                      model = fitted_vgm)
```

```{r}
# 10. Model performance metrics on training data
obs <- july_mean_temp$mean_temperature
pred_rf <- july_mean_temp$rf_pred
resid <- july_mean_temp$residuals

# R², RMSE, MAE
r2_rf <- 1 - sum((obs - pred_rf)^2) / sum((obs - mean(obs))^2)
rmse_rf <- sqrt(mean((obs - pred_rf)^2))
mae_rf <- mean(abs(obs - pred_rf))

cat("Random Forest Performance:\n")
cat("R² =", round(r2_rf, 3), "\n")
cat("RMSE =", round(rmse_rf, 3), "\n")
cat("MAE =", round(mae_rf, 3), "\n\n")

# 11. Moran's I on residuals (check spatial autocorrelation)
coords_mat <- coordinates(july_mean_temp_sp)
nb <- knn2nb(knearneigh(coords_mat, k = 4))
lw <- nb2listw(nb)
moran_result <- moran.test(resid, lw)
print(moran_result)

# 12. Cross-validation of kriging residuals
kr_cv <- krige.cv(residuals ~ 1, locations = july_mean_temp_sp, model = fitted_vgm)
cat("Kriging Cross-Validation:\n")
summary(kr_cv$residual)
```

