---
title: "Test_Regression_Kriging_Monthly_Mean_Temperature"
author: "Johannes Gruenewald"
date: "2025-04-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Spatial Data Handling & Processing
library(sf)
library(sp)
library(terra)
library(stars)
library(spdep)

# Spatial Analysis & Interpolation
library(gstat)
library(caTools)
library(randomForest)
library(nlme)
library(automap)
library(mgcv)
library(xgboost)
library(caret)

# Data Manipulation & Wrangling
library(tidyverse)
library(dplyr)
library(magrittr)

# Data Visualization
library(ggplot2)
library(scales)
library(gridExtra)

# Mapping & Interactive Visualization
library(mapview)
library(tmap)
```

--------------------------------------------------------------------------------

FOR TESTRUN WITH HOURLY MEANS

```{r Load and preprocess data}

# ================================================
# DATA LOADING, CLEANING, AND COVARIATE PREPARATION
# ================================================

# Read temperature data
temp <- sf::read_sf('../data/temp_HD/sensor_data_20230107_31days.gpkg')

# Removes rows where temp is NA
temp <- temp[!is.na(temp$temperature), ] 

# Reproject the data for Germany
temp <- st_transform(temp, crs="EPSG:32632")

# Ensure dateobserved is in the correct date-time format with German timezone
temp <- temp %>%
  mutate(dateobserved = as.POSIXct(dateobserved, format="%Y-%m-%d %H:%M:%S", tz = "Europe/Berlin"))

# Calculate hour count since epoch, rounded down to current hour
temp$chour <- floor(as.numeric(temp$dateobserved) / 3600)

# Test the results
print(format(as.POSIXct(temp$chour[66163] * 3600, origin = "1970-01-01", tz = "Europe/Berlin"), "%Y-%m-%d %H:%M:%S %Z"))

# Calculate mean temperature for each station and hour during July
temp_hm <- temp %>%
  filter(format(dateobserved, "%m") == "07") %>%  # Keep only July data
  group_by(stationname, chour) %>%                # Group by station and hour
  summarise(mh_temp = mean(temperature, na.rm = TRUE), .groups = "drop") %>%
  mutate(hour = as.POSIXct(chour * 3600, origin = "1970-01-01", tz = "Europe/Berlin"))

# Load elevation data
dem <- terra::rast("../data/DEM/hd_elevation_4326.tif")

# Reproject the elevation data to match correct CRS
dem <- terra::project(dem, y = crs(temp))

# Load Tree Canopy Height
tch <- terra::rast("../data/Canopy_height/clip_Forest_height_2019_NAFR.tif")

# Mask out all values which are assigned to water and have value 101
# Not part of this area. But needs to be masked as well: 102 Snow/ice; 103 No data
tch[tch == 101] <- NA

# Reproject the canopy height data to match correct CRS
tch <- terra::project(tch, y = crs(temp))

# Check if the two covariates have the same crs
crs(tch) == crs(dem)

# Load Building Height data
ghs_bh <- terra::rast("../data/building_height/clip_GHS_BUILT_H_AGBH_E2018_GLOBE_R2023A_54009_100_V1_0_R4_C19.tif")

# Reproject the building height data to match correct CRS
ghs_bh <- terra::project(ghs_bh, y = crs(temp))

# Check if CRS is correct
crs(ghs_bh) == crs(dem)

# Create a Raster Stack for all the covariate information

# Check resolution of each raster
terra::res(dem)
terra::res(tch)
terra::res(ghs_bh)

# Align TCH and building height to DEM resolution and extent
tch_aligned    <- terra::resample(tch, dem, method = "bilinear")
ghs_bh_aligned <- terra::resample(ghs_bh, dem, method = "near")

# Stack all layers
env_stack <- c(dem, tch_aligned, ghs_bh_aligned)

# Rename layers for clarity
names(env_stack) <- c("elev", "canopy", "bldg")

# Also add lat and lon to the raster stack
# Create coordinate rasters
lon <- terra::xFromCell(env_stack, 1:ncell(env_stack))
lat <- terra::yFromCell(env_stack, 1:ncell(env_stack))

# Convert to raster layers
lon_raster <- dem
lat_raster <- dem
values(lon_raster) <- lon
values(lat_raster) <- lat
names(lon_raster) <- "lon"
names(lat_raster) <- "lat"

# Stack the three raster layers
grids <- c(env_stack, lon_raster, lat_raster)

# Conversion when running the analysis without proximity to water
# Preparation of grid as a covariate for running RK

# Convert SpatRaster to RasterLayer (raster package)
grids_ras <- stack(grids)

# Now convert to SpatialGridDataFrame
grids_sp <- as(grids_ras, "SpatialGridDataFrame") 

grids_sp@data

# Extract coordinates of the measurement stations
# maybe use them as covariates

# Extract coordinates and data into a data frame
coords <- st_coordinates(temp)
locs <- cbind(temp, coords)
locs <- as.data.frame(locs)

# Keep only one row per station and select relevant columns
station_coords <- locs %>%
  distinct(stationname, .keep_all = TRUE) %>%
  select(stationname, X, Y)

coordinates(station_coords) <- ~ X + Y
proj4string(station_coords) <- CRS(proj4string(grids_sp))
```

```{r}
# Adding covariates to temperature data

# Convert Spatial object to a RasterBrick
r_brick <- brick(grids_sp)

# Convert the RasterBrick to a terra SpatRaster object
grid_terra <- rast(r_brick)

# Select, for each station, the record at its latest hour
sel_temp_hm <- temp_hm %>%
  group_by(stationname) %>%
  slice_max(order_by = chour, n = 1, with_ties = FALSE) %>%
  ungroup()

# Extract raster values at the point locations
extracted_vals <- terra::extract(grid_terra, sel_temp_hm)

# Remove the first column (ID)
extracted_vals <- extracted_vals[,-1]

# Combine the point data with the extracted raster values
temp_shm.ex <- cbind(as.data.frame(sel_temp_hm), extracted_vals)

str(temp_shm.ex)
```

--------------------------------------------------------------------------------

Load the temperature measurements as input data

```{r}
# Read temperature points
temp <- sf::read_sf('../data/temp_HD/sensor_data_20240107_31days.gpkg')

# Removes only rows where `temperature` is NA
temp <- temp[!is.na(temp$temperature), ] 

# Ensure dateobserved is in the correct date-time format
temp <- temp %>%
  mutate(dateobserved = as.POSIXct(dateobserved, format="%Y-%m-%d %H:%M:%S"))
```

Preprocessing of the temperature data
Calculate mean temperature for each measurement station for the month of July

```{r}
# Calculate the mean temperature for each station in July
temp_mm <- temp %>%
  filter(format(dateobserved, "%m") == "07") %>%
  group_by(stationname) %>%
  summarise(monthly_mean_temperature = mean(temperature, na.rm = TRUE), .groups = "drop")
```

Reproject the temperature data

```{r}
temp_mm <- st_transform(temp_mm, crs="EPSG:32632")
```

--------------------------------------------------------------------------------

Load and process environmental covariates

Load Elevation

```{r}
# Load the elevation data
dem <- terra::rast("../data/DEM/hd_elevation_4326.tif")

# Reproject the elevation data to match correct CRS
dem <- terra::project(dem, y = crs(temp_mm))
```

Load Tree Canopy Height

```{r}
# Load the elevation data
tch <- terra::rast("../data/Canopy_height/clip_Forest_height_2019_NAFR.tif")

# Mask out all values which are assigned to water and have value 101
# Not part of this area. But needs to be masked as well: 102 Snow/ice; 103 No data
tch[tch == 101] <- NA

# Reproject the elevation data to match correct CRS
tch <- terra::project(tch, y = crs(temp_mm))

# Check if the two covariates have the same crs
crs(tch) == crs(dem)
```

Load Building Height

```{r}
# Load the elevation data
ghs_bh <- terra::rast("../data/building_height/clip_GHS_BUILT_H_AGBH_E2018_GLOBE_R2023A_54009_100_V1_0_R4_C19.tif")

# Reproject the elevation data to match correct CRS
ghs_bh <- terra::project(ghs_bh, y = crs(temp_mm))

# Check if CRS is correct
crs(ghs_bh) == crs(dem)
```

--------------------------------------------------------------------------------

Adding covariates to temperature data

- Extract values from all the raster and add to temperature data at point locations

```{r}
# Extract raster values at the temp locations
elevation_values <- terra::extract(dem, temp_mm)

# Extract raster values at the temp locations
building_height_values <- terra::extract(ghs_bh, temp_mm)

# Extract tree canopy height values at the temp locations
canopy_heigth_values <- terra::extract(tch, temp_mm)

# Use dplyr to add the extracted values as a new column in the sf object
temp_mm <- temp_mm %>%
  mutate(elevation = elevation_values[[2]])

# Use dplyr to add the extracted values as a new column in the sf object
temp_mm <- temp_mm %>%
  mutate(building_height = building_height_values[[2]])

# Use dplyr to add the extracted values as a new column in the sf object
temp_mm <- temp_mm %>%
  mutate(tree_canopy_height = canopy_heigth_values[[2]])

# Check the updated sf object
print(temp_mm)
```

--------------------------------------------------------------------------------

Check data distribution of environmental covariates
Histogram to check data distribution

- log-transformation of the data doesn't make sense because we have 0 values for Forest and Building Height at certain point locations.

```{r}
# Set up a 1-row, 3-column plotting area
par(mfrow = c(1, 3))

# Plot the histograms
hist(temp_hm$elevation, main = "Elevation", xlab = "Elevation [m]", col = "lightblue")
hist(temp_hm$tree_canopy_height, main = "Global Forest Canopy Height", xlab = "Height [m]", col = "lightgreen")
hist(temp_hm$building_height, main = "Average of the Gross Building Height", xlab = "Height [m]", col = "lightcoral")
```

Visualize relationships between monthly mean temperature and environmental factors

```{r}
plot(temp_mm$tree_canopy_height, temp_mm$monthly_mean_temperature, 
     main = "Monthly Mean Temperature vs Global Forest Canopy Height", 
     xlab = "Forest Canopy Height [m]", 
     ylab = "Monthly Mean Temperature [°C]")

abline(lm(temp_mm$monthly_mean_temperature ~ temp_mm$tree_canopy_height), col = "red")

plot(temp_mm$building_height, temp_mm$monthly_mean_temperature, 
     main = "Monthly Mean Temperature vs Average of the Gross Building Height (AGBH)", 
     xlab = "AGBH [m]", 
     ylab = "Monthly Mean Temperature [°C]")

abline(lm(temp_mm$monthly_mean_temperature ~ temp_mm$building_height), col = "red")

plot(temp_mm$elevation, temp_mm$monthly_mean_temperature, 
     main = "Monthly Mean Temperature vs Elevation", 
     xlab = "Elevation [m]", 
     ylab = "Monthly Mean Temperature [°C]")

abline(lm(temp_mm$monthly_mean_temperature ~ temp_mm$elevation), col = "red")
```

Calculate Spearman
Explanation why I chose Spearman:
- Pearson’s correlation assumes both normality and linearity in the relationship between X and Y. 
- Spearman’s correlation has less stringent assumptions, assuming only that the relationship is monotonic. This means that the relationship has to be consistently increasing or decreasing, but that it does not have to do so in a linear manner (see left). 
- Spearman’s correlation also performs better than Pearson’s correlation in instances where there are outliers or very few data point -> this is the case for my data.

```{r}
cor.test(temp_mm$elevation, temp_mm$monthly_mean_temperature, method = "spearman")

cor.test(temp_mm$tree_canopy_height, temp_mm$monthly_mean_temperature, method = "spearman")

cor.test(temp_mm$building_height, temp_mm$monthly_mean_temperature, method = "spearman")
```

Spearman’s test is non-parametric and based on ranking the values. It assumes that all values are distinct so that ranks are unique. When ties occur (e.g., two identical temperatures or building heights) the exact p-value can't be computed. So for these results, p-value can't be fully trusted (elevation has a statistically significant relationship with temperature) be the correlation coefficient (rho) can still be trusted.
It is also an influencing factor, that I only work with n=31 data points

--------------------------------------------------------------------------------

GAM Model
Generalized Additive Models for each of the covariates

Approach based on: https://www.geeksforgeeks.org/generalized-additive-models-using-r/

Prep the data

```{r}
nrow(temp_mm)

# Splitting data in train and test data
split_gam_mm <- sample.split(temp_mm$monthly_mean_temperature, SplitRatio = 0.7)
length(split_gam_mm)

train_gam_mm <- subset(temp_mm, split_gam_mm == "TRUE")
test_gam_mm <- subset(temp_mm, split_gam_mm == "FALSE")
```

Models for each individual environmental covariates

```{r}
gam_elev <- gam(monthly_mean_temperature ~ s(elevation), data = train_gam_mm)
summary(gam_elev)
```

```{r}
gam_tch <- gam(monthly_mean_temperature ~ s(tree_canopy_height), data = temp_mm)
summary(gam_tch)
```

```{r}
gam_bh <- gam(monthly_mean_temperature ~ s(building_height), data = temp_mm)
summary(gam_bh)
```

GAM for all three covariates

Comment: A general rule of thumb is to use a high number of splines and use cross-validation of lambda (λ) values to find the model that generalises best. Remember we can have different splines and lambda values for every variable in our model.

```{r}
gam_all_mm <- gam(monthly_mean_temperature ~ 
                  elevation + 
                  s(tree_canopy_height) + 
                  building_height, 
                data = temp_mm)
summary(gam_all_mm)
```

Example: Visualize the results

```{r}
# Vary elevation across its range; fix the others at their median
predict_temp <- data.frame(
  elevation = seq(min(temp_mm$elevation), max(temp_mm$elevation), length.out = 20),
  tree_canopy_height = median(temp_mm$tree_canopy_height),
  building_height = median(temp_mm$building_height)
)

# Predict using the full GAM model
predictions <- predict(gam_all, newdata = predict_temp, type = "response", se.fit = TRUE)

# Plot the data and the GAM fit
ggplot() +
  geom_point(data = temp_mm, aes(x = elevation, y = monthly_mean_temperature)) +
  geom_line(data = data.frame(elevation = predict_temp$elevation, monthly_mean_temp = predictions$fit), 
            aes(x = elevation, y = monthly_mean_temp), color = "blue", size = 1) +
  geom_ribbon(data = data.frame(elevation = predict_temp$elevation, fit = predictions$fit, 
                                se = predictions$se.fit), aes(x = elevation, 
                                ymin = fit - 1.96 * se, 
                                ymax = fit + 1.96 * se), alpha = 0.3) +
  labs(title = "GAM Fit for Monthly Mean Temperature vs. Elevation", 
       x = "Elevation [m]", y = "Monthly Mean Temperature [°C]") +
  theme_minimal()
```

```{r}
# Vary elevation across its range; fix the others at their median
predict_temp <- data.frame(
  tree_canopy_height = seq(min(temp_mm$tree_canopy_height), max(temp_mm$tree_canopy_height), length.out = 20),
  elevation = median(temp_mm$elevation),
  building_height = median(temp_mm$building_height)
)

# Predict using the full GAM model
predictions <- predict(gam_all, newdata = predict_temp, type = "response", se.fit = TRUE)

# Plot the data and the GAM fit
ggplot() +
  geom_point(data = temp_mm, aes(x = tree_canopy_height, y = monthly_mean_temperature)) +
  geom_line(data = data.frame(tree_canopy_height = predict_temp$tree_canopy_height, monthly_mean_temp = predictions$fit), 
            aes(x = tree_canopy_height, y = monthly_mean_temp), color = "blue", size = 1) +
  geom_ribbon(data = data.frame(tree_canopy_height = predict_temp$tree_canopy_height, fit = predictions$fit, 
                                se = predictions$se.fit), aes(x = tree_canopy_height, 
                                ymin = fit - 1.96 * se, 
                                ymax = fit + 1.96 * se), alpha = 0.3) +
  labs(title = "GAM Fit for Monthly Mean Temperature vs. Forest Tree Canopy Height", 
       x = "Forest Tree Canopy Height [m]", y = "Monthly Mean Temperature [°C]") +
  theme_minimal()
```

Get the residuals from the fitted GAM model with the three environmental covariates

```{r}
gam_all_mm$residuals

temp_mm$gam_residuals <- gam_all_mm$residuals
```

Make predictions

```{r}
#make predictions
pred_gam_mm = predict(gam_all_mm, test_gam_mm)

# Mean Squared Error
mse_gam_mm = mean((test_gam_mm$monthly_mean_temperature - pred_gam_mm)^2)
print(paste("GAM MSE:", round(mse_gam_mm, 4)))

# Mean Absolute Error
mae_gam_mm = caret::MAE(test_gam_mm$monthly_mean_temperature, pred_gam_mm)
print(paste("GAM MAE:", round(mae_gam_mm, 4)))

# Root Mean Squared Error
rmse_gam_mm = caret::RMSE(test_gam_mm$monthly_mean_temperature, pred_gam_mm)
print(paste("GAM RMSE:", round(rmse_gam_mm, 4)))
```

Evaluate model performance

```{r}
#check the wiggliness of the model 
k.check(gam_all_mm)

gam.check(gam_all_mm)
```


--------------------------------------------------------------------------------

Test Random Forest (RF) instead of regression models to model complex relationships and see if the performance is similar to flexible regression models.

Approach based on: https://www.statology.org/random-forest-in-r/ and https://www.geeksforgeeks.org/random-forest-approach-in-r-programming/

Splitting the data in train and test data

```{r}
nrow(temp_mm)

# Splitting data in train and test data
split_rf_mm <- sample.split(temp_mm$monthly_mean_temperature, SplitRatio = 0.6)
length(split_rf_mm)

train_rf_mm <- subset(temp_mm, split_rf_mm == "TRUE")
test_rf_mm <- subset(temp_mm, split_rf_mm == "FALSE")
```

Fit the Random Forest Model

```{r}
#make this example reproducible
set.seed(0)

#fit the random forest model
rf_model_mm <- randomForest(
  formula = monthly_mean_temperature ~ elevation + tree_canopy_height + building_height,
  data = train_rf_mm,
  mtry = 3,
  importance = TRUE,
  na.action = na.omit
)

rf_model_mm
importance(rf_model_mm)

#find number of trees that produce lowest test MSE
which.min(rf_model_mm$mse)

#find RMSE of best model
sqrt(rf_model_mm$mse[which.min(rf_model_mm$mse)]) 

#plot the test MSE by number of trees
plot(rf_model_mm)

#produce variable importance plot
varImpPlot(rf_model_mm) 
```

Tune the model
This function produces the following plot, which displays the number of predictors used at each split when building the trees on the x-axis and the out-of-bag estimated error on the y-axis:

```{r}
model_tuned_mm <- tuneRF(
  x = data.frame(
    elevation = train_rf_mm$elevation,
    building_height = train_rf_mm$building_height,
    tree_canopy_height = train_rf_mm$tree_canopy_height
  ),
  y = train_rf_mm$monthly_mean_temperature,  # define response variable
  ntreeTry = 500,
  mtryStart = 3,
  stepFactor = 1.5,
  improve = 0.01,
  trace = FALSE  # don't show real-time progress
)
```

We can see that the lowest OOB error is achieved by using 2 randomly chosen predictors at each split when building the trees.

Use the Final Model to Make Predictions and derive Residuals

```{r}
# Predict on training data
predicted_rf_mm <- predict(rf_model_mm, newdata=test_rf_mm)

# Calculate residuals: observed - predicted
residuals_rf_mm <- temp_mm$monthly_mean_temperature - predicted_rf_mm

plot(residuals_rf_mm)
```

Calculate model performance

```{r}
# Calculate RMSE
rmse_rf_mm = caret::RMSE(test_rf_mm$monthly_mean_temperature, predicted_rf_mm)

#rmse_rf_mm <- sqrt(mean((actual_mm_rf - predicted_mm_rf)^2))
print(paste("Random Forest RMSE:", round(rmse_rf_mm, 4)))
```

--------------------------------------------------------------------------------

Testing XGBoost approach
short for extreme gradient boosting

Approach based on: https://www.statology.org/xgboost-in-r/

Prep the data

```{r}
#make this example reproducible
set.seed(0)

#split into training (80%) and testing set (20%)
parts_xg_mm = createDataPartition(temp_mm$monthly_mean_temperature, p = .7, list = F)
train_xg_mm = temp_mm[parts_xg_mm, ]
test_xg_mm = temp_mm[-parts_xg_mm, ]

#define predictor and response variables in training set
train_x_xg_mm = data.matrix(st_drop_geometry(train_xg_mm[, 4:6]))
train_y_xg_mm = train_xg_mm[[2]]

#define predictor and response variables in testing set
test_x_xg_mm = data.matrix(st_drop_geometry(test_xg_mm[, 4:6]))
test_y_xg_mm = test_xg_mm[[2]]

#define final training and testing sets
xgb_train_xg_mm = xgb.DMatrix(data = train_x_xg_mm, label = train_y_xg_mm)
xgb_test_xg_mm = xgb.DMatrix(data = test_x_xg_mm, label = test_y_xg_mm)
```

Fit the Model

```{r}
#define watchlist
watchlist_xg_mm = list(train=xgb_train_xg_mm, test=xgb_test_xg_mm)

#fit XGBoost model and display training and testing data at each round
model_xg_mm = xgb.train(
  data = xgb_train_xg_mm, 
  max.depth = 3, 
  watchlist=watchlist_xg_mm, 
  nrounds = 70,
  early_stopping_rounds = 10)
```

Check for best iteration and adapt in final model!

Define final model

```{r}
#define final model
final_xg_mm = xgboost(
  data = xgb_train_xg_mm, 
  max.depth = 3, 
  nrounds = 28, 
  verbose = 0)
```

Note: The argument verbose = 0 tells R not to display the training and testing error for each round.

Make Predictions and calculate accuracy measures:

```{r}
#make predictions
pred_y_xg_mm = predict(final_xg_mm, xgb_test_xg_mm)

# Mean Squared Error
mse_xg_mm = mean((test_y_xg_mm - pred_y_xg_mm)^2)
print(paste("XGBoost MSE:", round(mse_xg_mm, 4)))

# Mean Absolute Error
mae_xg_mm = caret::MAE(test_y_xg_mm, pred_y_xg_mm)
print(paste("XGBoost MAE:", round(mae_xg_mm, 4)))

# Root Mean Squared Error
rmse_xg_mm = caret::RMSE(test_y_xg_mm, pred_y_xg_mm)
print(paste("XGBoost RMSE:", round(rmse_xg_mm, 4)))
```

Implementation of 5-fold cross-validation

```{r}
set.seed(20)

# Prepare features and labels
features <- c("elevation", "tree_canopy_height", "building_height")
X <- data.matrix(st_drop_geometry(temp_mm[, features]))
y <- temp_mm$monthly_mean_temperature

# Create 5 folds
folds_xg_mm <- createFolds(y, k = 5, list = TRUE)

rmse_values <- numeric(length(folds_xg_mm))

for(i in seq_along(folds_xg_mm)) {
  
  test_idx <- folds_xg_mm[[i]]
  train_idx <- setdiff(seq_len(nrow(temp_mm)), test_idx)
  
  # Prepare DMatrix for XGBoost
  dtrain <- xgb.DMatrix(data = X[train_idx, ], label = y[train_idx])
  dtest <- xgb.DMatrix(data = X[test_idx, ], label = y[test_idx])
  
  # Define watchlist for early stopping
  watchlist <- list(train = dtrain, eval = dtest)
  
  # Train model with early stopping
  xgb_model <- xgb.train(
    data = dtrain,
    max_depth = 3,
    nrounds = 100,
    eta = 0.1,
    objective = "reg:squarederror",
    eval_metric = "rmse",
    watchlist = watchlist,
    early_stopping_rounds = 10,
    verbose = 0
  )
  
  # Predict on the test fold
  preds <- predict(xgb_model, dtest)
  
  # Calculate RMSE for this fold
  rmse_fold <- sqrt(mean((y[test_idx] - preds)^2))
  rmse_values[i] <- rmse_fold
  
  cat(sprintf("Fold %d RMSE: %.4f\n", i, rmse_fold))
}

cat(sprintf("Average RMSE across 5 folds: %.4f\n", mean(rmse_values)))

```



--------------------------------------------------------------------------------

Still some confusion here

Create prediction locations for the kriging process including the covariates

```{r}
# Convert raster to points, including values, and remove NAs
predLocations <- terra::as.points(dem, values=TRUE, na.rm=TRUE)

# Reproject the elevation data to match correct CRS
predLocations <- terra::project(predLocations, y = crs(temp_mm))

# Check CRS of locations
crs(predLocations)

# Convert the points to a SpatialPointsDataFrame
predLocations <- as(predLocations, "Spatial")

# Convert sf object to sp object
temp_mm_sp <- as(temp_mm, "Spatial")

# Set CRS of predLocations_sp to match july_mean_temp
sp::proj4string(predLocations) == sp::proj4string(temp_mm_sp)

# Display the updated gridded points
str(predLocations)

predLocations_sf <- st_as_sf(predLocations)
```

```{r}
# 4. Fit variogram to residuals
fitted <- autofitVariogram(gam_residuals ~ 1, input_data = temp_mm)
fitted_vgm <- fitted$var_model
plot(fitted$exp_var, fitted$var_model)
```

```{r}
# 6. Interpolate residuals by kriging
kriged_resid <- krige(gam_residuals ~ 1,
                      loc = temp_mm,
                      newdata = predLocations_sf,
                      model = fitted_vgm)
```

```{r}
str(kriged_resid)

# Convert to sf object
kriged_stars <- st_as_stars(kriged_resid)

# Plot kriged predicted residuals
tm_shape(kriged_stars) +
  tm_dots("var1.pred", 
          palette = "-RdBu",  # reversed palette to show negative vs positive residuals clearly
          title = "Kriged Residuals",
          size = 0.5) +
  tm_layout(main.title = "Kriged Residual Surface")

```



```{r}
cv_results <- krige.cv(gam_residuals ~ 1, 
                       locations = temp_mm_sp, 
                       model = fitted_vgm)

# Check error metrics
summary(cv_results$var1.pred - cv_results$observed)  # Mean error
sqrt(mean((cv_results$var1.pred - cv_results$observed)^2))  # RMSE

```

```{r}
# Step 1: Predict GAM component at kriging locations
gam_pred <- predict(gam_all, newdata = predLocations)

# Step 2: Get kriged residuals
kriged_resid_values <- kriged_resid$var1.pred

# Step 3: Combine both
final_pred <- gam_pred + kriged_resid_values
```



```{r}
predResidHDRas <- raster::brick(kriged_resid)
```

```{r}
#tmap_mode("plot")
tm_shape(subset(predTempHDRas, subset = "var1.pred")) + 
  tm_raster(col="var1.pred", title = "Prediction [°C]", n=9, 
            legend.reverse = TRUE, palette = "-RdBu") +
  tm_shape(july_mean_temp) + 
  tm_symbols(col="mean_temperature", title.col = "Observed [°C]", 
             legend.col.reverse = TRUE, palette = "-BrBG", 
             border.col = "black", size=.1, alpha=.9) +
  tm_scale_bar() + 
  tm_layout(title = "Temperature interpolation, HD", 
            legend.outside = TRUE, attr.outside = TRUE)
```

----------------------------------------------

```{r}
# 4. Fit variogram to residuals
fitted <- autofitVariogram(residuals ~ 1, input_data = july_mean_temp)
fitted_vgm <- fitted$var_model
plot(fitted$exp_var, fitted$var_model)
```

Create prediction locations for the kriging process including the covariates

```{r}
# Convert raster to points, including values, and remove NAs
predLocations <- terra::as.points(dem, values=TRUE, na.rm=TRUE)

# Reproject the elevation data to match correct CRS
predLocations <- terra::project(predLocations, y = crs(july_mean_temp))

# Check CRS of locations
crs(predLocations)

# Convert the points to a SpatialPointsDataFrame
predLocations <- as(predLocations, "Spatial")

# Convert sf object to sp object
july_mean_temp_sp <- as(july_mean_temp, "Spatial")

# Set CRS of predLocations_sp to match july_mean_temp
sp::proj4string(predLocations) == sp::proj4string(july_mean_temp_sp)

# Display the updated gridded points
class(predLocations)
```



```{r}
# 6. Interpolate residuals by kriging
kriged_resid <- krige(residuals ~ 1,
                      loc = july_mean_temp_sp,
                      newdata = predLocations,
                      model = fitted_vgm)
```

```{r}
# 10. Model performance metrics on training data
obs <- july_mean_temp$mean_temperature
pred_rf <- july_mean_temp$rf_pred
resid <- july_mean_temp$residuals

# R², RMSE, MAE
r2_rf <- 1 - sum((obs - pred_rf)^2) / sum((obs - mean(obs))^2)
rmse_rf <- sqrt(mean((obs - pred_rf)^2))
mae_rf <- mean(abs(obs - pred_rf))

cat("Random Forest Performance:\n")
cat("R² =", round(r2_rf, 3), "\n")
cat("RMSE =", round(rmse_rf, 3), "\n")
cat("MAE =", round(mae_rf, 3), "\n\n")

# 11. Moran's I on residuals (check spatial autocorrelation)
coords_mat <- coordinates(july_mean_temp_sp)
nb <- knn2nb(knearneigh(coords_mat, k = 4))
lw <- nb2listw(nb)
moran_result <- moran.test(resid, lw)
print(moran_result)

# 12. Cross-validation of kriging residuals
kr_cv <- krige.cv(residuals ~ 1, locations = july_mean_temp_sp, model = fitted_vgm)
cat("Kriging Cross-Validation:\n")
summary(kr_cv$residual)
```

--------------------------------------------------------------------------------

Linear regression models may be too simplistic and fail to accurately capture the correlation between temperature measurements and selected covariates. They were tested here to see if they might still produce meaningful results.

Multiple Linear Rregression (MLR)

```{r}
model <- lm(mean_temperature ~ elevation + tree_canopy_height + building_height, data = july_mean_temp)

# Print model summary
summary(model)

plot(model)  # Look for non-random patterns in residuals
```

Stepwise MLR

```{r}
# stepwise variable selection
model.MLR.step <- step(model, direction="both")

# summary of the new model using stepwise covariates selection
summary(model.MLR.step)
```

Interpretation of MLR performance

```{r}
# graphical diagnosis of the regression analysis
par(mfrow=c(2,2))
plot(model.MLR.step)
par(mfrow=c(1,1))
```

GLS model

```{r}
# Extract coordinates (assuming `geom` is the column with spatial data)
july_mean_temp$coords <- st_coordinates(july_mean_temp$geom)

# Separate the coordinates into `x` and `y` columns
july_mean_temp$x <- july_mean_temp$coords[, 1]
july_mean_temp$y <- july_mean_temp$coords[, 2]

gls_model <- gls(mean_temperature ~ elevation + tree_canopy_height + building_height,
                 data = july_mean_temp,
                 correlation = corExp(form = ~ x + y, nugget = TRUE))

summary(gls_model)
```

--------------------------------------------------------------------------------