---
title: "Explorative analysis of the temperature data"
author: "Johannes Gruenewald"
date: "2024-12-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Spatial Data Handling & Processing
library(sf)
library(sp)
library(terra)
library(stars)
library(spdep)

# Spatial Analysis & Interpolation
library(gstat)
library(caTools)
library(randomForest)
library(nlme)
library(automap)
library(mgcv)
library(xgboost)
library(caret)

# Data Manipulation & Wrangling
library(tidyverse)
library(dplyr)
library(tidyr)
library(magrittr)
library(lubridate)
library(slider)
library(stringr)

# Data Visualization
library(ggplot2)
library(scales)
library(gridExtra)

# Mapping & Interactive Visualization
library(mapview)
library(tmap)
```

--------------------------------------------------------------------------------

```{r Load temp data}

# Complete data import and pre-processing of the temperature (July 2023)

# Read temperature data for the entire year of 2023
temp <- sf::read_sf('../data/temp_HD/sensor_data_20230107_31days.gpkg')

# Removes rows where temp is NA
temp <- temp %>% filter(!is.na(temperature))

# sf-compatible CRS
CRS_UTM32 <- sf::st_crs(32632)

# sp-compatible CRS
CRS_UTM32_sp <- sp::CRS("+proj=utm +zone=32 +datum=WGS84 +units=m +no_defs")

# Reproject the data for Germany
temp <- st_transform(temp, crs=CRS_UTM32)

# Clean the data from outliers -> currently working with hard, manual breaks
# Currently nothing is removed by this step because no measurements are above 40°C for the year 2023
temp_clean <- temp %>% filter(temperature >= -30, temperature <= 45)

# Drop all measurement stations which didn't collect data the entire year around
temp_clean_station <- temp_clean %>%
  mutate(dateobserved = as.POSIXct(dateobserved, tz = "Europe/Berlin")) %>%
  filter(!entity_id %in% c(
    "hd:DE_Heidelberg_69120_12:WeatherObserved",
    "hd:DE_Heidelberg_69120_34:WeatherObserved",
    "hd:DE_Heidelberg_69123_41:WeatherObserved",
    "hd:DE_Gaiberg_69251_21:WeatherObserved",
    "hd:DE_Heidelberg_46:WeatherObserved"
  ))

# Check if exactly 5 stations were dropped
cat("Number of stations BEFORE cleaning: ",
    length(unique(na.omit(temp_clean$entity_id))), "\n")

cat("Number of stations AFTER cleaning:  ",
    length(unique(na.omit(temp_clean_station$entity_id))), "\n")

# Calculate hourly mean temp for cleaned data
temp_clean_station_hm <- temp_clean_station %>%
    mutate(hour = floor_date(dateobserved, unit = "hour")) %>%   # round down
    group_by(entity_id, hour) %>%
    summarise(hm_temp = mean(temperature, na.rm = TRUE), .groups = "drop")
```

```{r}

# Set time range
start_date <- as.POSIXct("2023-07-01 00:00:00", tz = "Europe/Berlin")
end_date   <- as.POSIXct("2023-07-12 23:00:00", tz = "Europe/Berlin")

# Filter temp data to first 12 days of July
temp_july_12 <- temp_clean_station_hm %>%
  filter(between(hour, start_date, end_date))

# Plot 12 day temperature
temp_time_series <- ggplot(temp_july_12, aes(x = hour, y = hm_temp)) +
  geom_line(alpha = 0.6) +
  labs(x = "Day", y = "Temperature [°C]") +
  scale_x_datetime(date_breaks = "1 day", date_labels = "%b %d") +
  scale_y_continuous(limits = c(10, 40), breaks = seq(10, 40, by = 5)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(
    axis.title.x = element_text(margin = ggplot2::margin(t = 25)),
    axis.title.y = element_text(margin = ggplot2::margin(r = 25))
  ) +
  theme_minimal()

temp_time_series

# Save plot
ggsave("../plots_report/temp_time_series_12days.png",
       plot = temp_time_series, width = 12, height = 8, dpi = 300)
```


--------------------------------------------------------------------------------

```{r Check temperature distribution across all measurement stations}

# Build daily dataset with rolling mean + ordering key from entity_id
daily <- temp_clean %>%
  st_drop_geometry() %>%
  filter(!is.na(dateobserved), !is.na(temperature), !is.na(entity_id)) %>%
  mutate(date = as.Date(dateobserved),
         year = lubridate::year(date)) %>%
  filter(year == 2023) %>%
  # pull the number after the last "_" and before ":" (e.g., "…_6:…")
  mutate(order_key = as.integer(str_extract(entity_id, "(?<=_)\\d+(?=:)"))) %>%
  group_by(entity_id, date) %>%
  summarise(tmean = mean(temperature), order_key = first(order_key), .groups = "drop") %>%
  arrange(order_key, date) %>%
  group_by(entity_id) %>%
  mutate(tmean7 = slider::slide_dbl(tmean, ~mean(.x, na.rm = TRUE),
                                    .before = 3, .after = 3)) %>%
  ungroup()

# Build facet order (levels) from the extracted order_key
levs <- daily %>%
  distinct(entity_id, order_key) %>%
  arrange(order_key, entity_id) %>%
  pull(entity_id)

# Apply facet order to entity_id
daily <- daily %>% mutate(entity_id = factor(entity_id, levels = levs))

p_temp <- ggplot(daily, aes(x = date)) +
  # map color to a label so we get a legend entry
  geom_line(aes(y = tmean,  color = "Daily mean"),  alpha = 0.35, linewidth = 0.3) +
  geom_line(aes(y = tmean7, color = "7-day mean"),              linewidth = 0.8) +
  facet_wrap(~ entity_id, ncol = 5, scales = "fixed") +
  scale_x_date(date_breaks = "3 months", date_labels = "%b",
               limits = as.Date(c("2023-01-01","2023-12-31"))) +
  scale_color_manual(
    name = NULL,
    values = c("Daily mean" = "black", "7-day mean" = "steelblue")
  ) +
  # make the legend lines match the thickness of the plotted lines
  guides(color = guide_legend(override.aes = list(linewidth = c(0.8, 0.8), alpha = 1))) +
  labs(
    title = "2023 Temperature by Station — ordered by sensor ID",
    x = NULL, y = "Temperature (°C)",
    caption = "Source: Pre-processed sensor data"
  ) +
  theme_minimal(base_size = 10) +
  theme(panel.grid.minor = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1),
        strip.text = element_text(face = "bold"),
        plot.title = element_text(size = 16, face = "bold"))

p_temp

# Save the figure
# ggsave(
#   filename = file.path("../plots", paste0("temperature_distribution_all_stations_2023", ".png")),
#   plot = p_temp, width = 10, height = 6, units = "in", dpi = 300
# )

str(temp)
length(unique(temp$entity_id))
```

--------------------------------------------------------------------------------

Check specific measurement station for outliers
To run this code successfully and really display the outliers the data needs to be imported again
```{r select example station to highlight outliers}

# Select one specific station by entity_id
station_data <- subset(temp, entity_id == "hd:DE_Heidelberg_69120_34:WeatherObserved")

# Build violin plot of monthly temperature distribution with summary stats
p_outlier <- station_data %>%
  filter(!is.na(dateobserved), !is.na(temperature)) %>%
  mutate(month = floor_date(dateobserved, "month")) %>%
  ggplot(aes(x = factor(month), y = temperature)) +
  geom_violin(fill = "#FFA500", color = "black", trim = FALSE,
              draw_quantiles = c(0.25, 0.5, 0.75)) +  # show Q1 / median / Q3
  stat_summary(
    fun.data = function(y) {
      y <- y[!is.na(y)]
      if (!length(y)) return(data.frame(y = NA, label = ""))
      n <- length(y); m <- mean(y); s <- sd(y)
      pad <- 0.03 * diff(range(y)) + 1e-9
      data.frame(y = max(y) + pad,
                 label = sprintf("Count = %d\nMean = %.2f\nSD = %.2f", n, m, s))
    },
    geom = "text", hjust = 0.5, vjust = -0.8,
    aes(label = after_stat(label)), size = 2.6
  ) +
  labs(title = "Monthly temperature distribution for station\nhd:DE_Heidelberg_69120_34:WeatherObserved (Inactive since May 8, 2023)",
       x = "Month", y = "Temperature (°C)",
       caption = "Source: Pre-processed sensor data") +
  scale_x_discrete(labels = function(x) format(as.Date(x), "%b %Y")) +
  scale_y_continuous(expand = expansion(mult = c(0.03, 0.18))) +
  theme_minimal(base_size = 12) +
  theme(panel.grid.minor = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(face = "bold"))

p_outlier

# Save plot
ggsave(
  filename = file.path("../plots", paste0("monthly_temperature_outlier_violin", ".png")),
  plot = p_outlier, width = 10, height = 6, units = "in", dpi = 300
)
```

This measurement station started to detect wrong temperature information starting in early May and stopped recording on the 8th of May 2023. Therefore, this measurement station will not be part of the analysis and excluded as it only provides information for the first 4 months of the year. The same happened to three other stations which will not be part of this analysis.

--------------------------------------------------------------------------------

```{r Further pre-processing to drop all inconsistent stations}

# Drop all measurement stations which didn't collect data the entire year around
temp_clean_station <- temp_clean %>%
  mutate(dateobserved = as.POSIXct(dateobserved, tz = "UTC")) %>%
  filter(!entity_id %in% c(
    "hd:DE_Heidelberg_69120_12:WeatherObserved",
    "hd:DE_Heidelberg_69120_34:WeatherObserved",
    "hd:DE_Heidelberg_69123_41:WeatherObserved",
    "hd:DE_Heidelberg_46:WeatherObserved"
  ))

# Check if exactly 4 stations were dropped
length(unique(na.omit(temp_clean$entity_id)))
length(unique(na.omit(temp_clean_station$entity_id)))
```

```{r}
temp_clean_station_dm <- temp_clean_station %>%
  group_by(entity_id, dateobserved) %>%                       # <- daily grouping
  summarise(dm_temp = mean(temperature, na.rm = TRUE),
            .groups = "drop")
```

```{r}
temp_clean_station_dm %>%
  filter(dateobserved == as.Date("2023-07-09")) %>%
  summarise(
    min_temp = min(dm_temp, na.rm = TRUE),
    max_temp = max(dm_temp, na.rm = TRUE),
    range_deg = max_temp - min_temp
  )

```

--------------------------------------------------------------------------------

```{r Check temperature distribution using bins of 5°C}

# Build global 5 °C breaks from the cleaned data (rounded outward)
tmin_clean <- floor(min(temp_clean_station$temperature, na.rm = TRUE) / 5) * 5
tmax_clean <- ceiling(max(temp_clean_station$temperature, na.rm = TRUE) / 5) * 5
breaks     <- seq(tmin_clean, tmax_clean, by = 5)

# Add clean labels
labs <- sprintf("%d to %d", breaks[-length(breaks)], breaks[-1])

# Bin + count, DROP GEOMETRY to avoid sfc column
counts_5C_clean <- temp_clean_station %>%
  sf::st_drop_geometry() %>%
  transmute(bin = cut(temperature,
                      breaks = breaks,
                      labels = labs,
                      right = FALSE,      
                      include_lowest = TRUE,
                      ordered_result = TRUE)) %>%
  filter(!is.na(bin)) %>%
  count(bin, name = "n") %>%
  tidyr::complete(bin = factor(labs, levels = labs, ordered = TRUE), fill = list(n = 0L)) %>% 
  mutate(pct = n / sum(n), cum_pct = cumsum(pct))

# Temperature distribution using 5°C bins for the year of 2023
p_dist <- ggplot(counts_5C_clean, aes(x = bin, y = n)) +
  geom_col(fill = "#FFA500", color = "black", alpha = 0.7) +
  scale_x_discrete(drop = FALSE) +
  scale_y_continuous(labels = scales::label_number(big.mark = ",", accuracy = 1)) +
  labs(title = "Temperature Distribution — Heidelberg (2023, 5°C bins)",
       x = "Temperature bin (°C)", y = "Count",
       caption = "Source: Pre-processed sensor data") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

p_dist

# Save PNG
ggsave(
  filename = file.path("../plots", paste0("temperature_distribution_Heidelberg_2023", ".png")),
  plot = p_dist, width = 10, height = 6, units = "in", dpi = 300
)
```

--------------------------------------------------------------------------------

```{r Calculate monthly mean temp}

temp_monthly_2023 <- temp_clean_station %>%
  # restrict to year 2023
  filter(year(dateobserved) == 2023) %>%
  # collapse to month
  mutate(month = floor_date(dateobserved, "month")) %>%
  group_by(month) %>%
  summarise(
    mean_temp = mean(temperature, na.rm = TRUE),
    sd_temp   = sd(temperature, na.rm = TRUE),
    n_obs     = n(),   # number of readings contributing
    .groups   = "drop"
  )

p_mean <- ggplot(temp_monthly_2023, aes(x = month, y = mean_temp)) +
  # line + points
  geom_line(color = "steelblue", linewidth = 1) +
  geom_point(aes(color = mean_temp), size = 4) +
  
  # color scale (blue for cold, red for hot)
  scale_color_gradient(low = "blue", high = "red") +
  
  # x-axis: month abbreviations
  scale_x_date(
    date_labels = "%b",
    breaks = "1 month"
  ) +
  
  # y-axis label
  labs(
    title = "Monthly Mean Temperatures (2023)",
    subtitle = "Across all weather stations in Heidelberg",
    x = "Month",
    y = "Mean Temperature (°C)",
    color = "Mean Temp"
  ) +
  
  # theme adjustments
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    plot.subtitle = element_text(size = 12, color = "grey30"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.minor = element_blank()
  )

# Save mean plot
ggsave("../plots/monthly_mean_temperatures_2023.png", p_mean, width = 8, height = 5, dpi = 300)

p_sd <- ggplot(temp_monthly_2023, aes(x = month, y = sd_temp)) +
  geom_line(color = "darkorange", linewidth = 1) +
  geom_point(aes(color = sd_temp), size = 4) +
  
  # color scale (low = stable, high = variable)
  scale_color_gradient(low = "lightblue", high = "darkred") +
  
  # x-axis formatting
  scale_x_date(
    date_labels = "%b",
    breaks = "1 month"
  ) +
  
  labs(
    title = "Monthly Temperature Variability (2023)",
    subtitle = "Standard deviation across all weather stations in Heidelberg",
    x = "Month",
    y = "Standard Deviation (°C)",
    color = "SD"
  ) +
  
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    plot.subtitle = element_text(size = 12, color = "grey30"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.minor = element_blank()
  )

# Save SD plot
ggsave("../plots/monthly_sd_temperatures_2023.png", p_sd, width = 8, height = 5, dpi = 300)

p_mean
p_sd
```


--------------------------------------------------------------------------------

```{r Calculate hourly mean temp}

# Calculate hourly mean temp for cleaned data
temp_clean_station_hm <- temp_clean_station %>%
    group_by(entity_id, chour) %>%
    summarise(hm_temp = mean(temperature, na.rm = TRUE), .groups = "drop") %>%
    mutate(hour = as.POSIXct(chour * 3600, origin = "1970-01-01", tz = "Europe/Berlin"))
```

--------------------------------------------------------------------------------

```{r Check temperature distribution of hourly mean temp using bins of 2°C}
# Build global 1 °C breaks from the hourly mean data (rounded outward)
tmin_hour <- floor(min(temp_clean_station_hm$hm_temp, na.rm = TRUE) / 1) * 1
tmax_hour <- ceiling(max(temp_clean_station_hm$hm_temp, na.rm = TRUE) / 1) * 1
breaks_1C <- seq(tmin_hour, tmax_hour, by = 1)

# Add clean labels
labs_1C <- sprintf("%d to %d", breaks_1C[-length(breaks_1C)], breaks_1C[-1])

# Bin + count
counts_1C_hourly <- temp_clean_station_hm %>%
  transmute(bin = cut(
    hm_temp,
    breaks = breaks_1C,
    labels = labs_1C,
    right = FALSE,
    include_lowest = TRUE,
    ordered_result = TRUE
  )) %>%
  filter(!is.na(bin)) %>%
  count(bin, name = "n") %>%
  tidyr::complete(bin = factor(labs_1C, levels = labs_1C, ordered = TRUE), fill = list(n = 0L)) %>%
  mutate(pct = n / sum(n), cum_pct = cumsum(pct))

# Plot: Hourly mean temperature distribution using 1°C bins
p_hourly_dist <- ggplot(counts_1C_hourly, aes(x = bin, y = n)) +
  geom_col(fill = "#1E90FF", color = "black", alpha = 0.7) +
  scale_x_discrete(drop = FALSE) +
  scale_y_continuous(labels = scales::label_number(big.mark = ",", accuracy = 1)) +
  labs(
    title = "Hourly Mean Temperature Distribution — Heidelberg (2023, 1°C bins)",
    x = "Temperature bin (°C)",
    y = "Count",
    caption = "Source: Pre-processed hourly mean sensor data"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Display plot
p_hourly_dist

# Save PNG
ggsave(
  filename = file.path("../plots", paste0("hourly_mean_temperature_distribution_Heidelberg_2023", ".png")),
  plot = p_hourly_dist,
  width = 10, height = 6, units = "in", dpi = 300
)
```

--------------------------------------------------------------------------------

Filter for hottest timestamp for the year of 2023 at 09-07-2023 at 14 pm

```{r}
# 09-07-2023 13pm
sel_469139 <- temp_clean_station_hm_cov %>%
  filter(chour == 469139)

# quick sanity checks
nrow(sel_469139)                  # how many rows (stations) at that hour
length(unique(sel_469139$entity_id))  # distinct stations
unique(sel_469139$hour)           # the POSIXct hour (should be length 1)

# 09-07-2023 14pm
sel_469140 <- temp_clean_station_hm_cov %>%
  filter(chour == 469140)

# quick sanity checks
nrow(sel_469140)                  # how many rows (stations) at that hour
length(unique(sel_469140$entity_id))  # distinct stations
unique(sel_469140$hour)           # the POSIXct hour (should be length 1)

# 09-07-2023 14pm
sel_469141 <- temp_clean_station_hm_cov %>%
  filter(chour == 469141)

# quick sanity checks
nrow(sel_469141)                  # how many rows (stations) at that hour
length(unique(sel_469141$entity_id))  # distinct stations
unique(sel_469141$hour)           # the POSIXct hour (should be length 1)
```



--------------------------------------------------------------------------------

Heatmap of temperature patterns

```{r}

df <- temp_clean_station_hm %>%
  filter(!is.na(hour), !is.na(hm_temp))

# Order stations by annual mean temperature (desc = warm → cool)
station_levels <- df %>%
  group_by(entity_id) %>%
  summarise(annual_mean = mean(hm_temp, na.rm = TRUE), .groups = "drop") %>%
  arrange(desc(annual_mean)) %>%
  pull(entity_id)

# Robust colour limits (1st–99th percentile)
clims <- quantile(df$hm_temp, c(0.01, 0.99), na.rm = TRUE)

# Month separators (optional)
month_lines <- seq(floor_date(min(df$hour), "month"),
                   floor_date(max(df$hour), "month"),
                   by = "1 month")

ggplot(df, aes(x = hour,
               y = factor(entity_id, levels = station_levels),
               fill = hm_temp)) +
  geom_raster() +  # faster than geom_tile for dense grids
  scale_fill_viridis_c(
    option = "plasma",
    limits = clims, oob = squish,   # clamp extremes
    na.value = "grey90",
    name = "Temperature (°C)"
  ) +
  scale_x_datetime(date_breaks = "1 month", date_labels = "%b %Y",
                   expand = c(0, 0)) +
  labs(title = "Hourly Mean Temperature by Station (2023)",
       x = "Date & Hour",
       y = "Station") +
  geom_vline(xintercept = month_lines, colour = "white",
             linewidth = 0.2, alpha = 0.3) +
  theme_minimal(base_size = 11) +
  theme(panel.grid = element_blank(),
        axis.text.y = element_text(size = 6))

```


--------------------------------------------------------------------------------

```{r}
temp_by_hour <- temp_clean_station_hm %>%
  mutate(hour_of_day = hour(hour)) %>%
  filter(!is.na(hour_of_day))

p_dirunal <- ggplot(temp_by_hour, aes(x = factor(hour_of_day), y = hm_temp, fill = hour_of_day)) +
  geom_boxplot(fill = "#FFA500", alpha = 0.7,  outlier.alpha = 0.3) +
  labs(
    title    = "Diurnal Cycle of Hourly Mean Temperature Across Stations",
    subtitle = "Distribution across 29 measurement stations for the year 2023",
    x        = "Hour of Day",
    y        = "Hourly Mean Temperature (°C)",
    fill     = "Hour of Day"
  ) +
  theme_minimal()

p_dirunal

# Save PNG
ggsave(
  filename = file.path("../plots", paste0("Diurnal_Cycle_Hourly_Mean_Temperature_Stations_2023", ".png")),
  plot = p_dirunal, width = 10, height = 6, units = "in", dpi = 300
)
```

hourly mean temps peak around 13-15 
Less between station spread in early morning and late day hours

--------------------------------------------------------------------------------

Analyse temperate data over the course of one month and compare them with each other.
Loop through all the months

```{r Loop through months to compute hourly mean temperatures and generate time series plots per station}
# Define your input folder
input_folder <- "../data/temp_HD"
output_folder <- "../temp_timeseries"
dir.create(output_folder, showWarnings = FALSE)

# List all .gpkg files
gpkg_files <- list.files(input_folder, pattern = "\\.gpkg$", full.names = TRUE)

# Create an empty list to store results
all_temp_hm <- list()

# Loop through each .gpkg file
for (file in gpkg_files) {
  cat("\nProcessing:", basename(file), "\n")

  # ---- Load and preprocess temp data ----
  temp <- sf::read_sf(file)
  temp <- temp[!is.na(temp$temperature), ]
  temp <- st_transform(temp, crs = "EPSG:32632")

  temp <- temp %>%
    mutate(dateobserved = as.POSIXct(dateobserved, format = "%Y-%m-%d %H:%M:%S", tz = "Europe/Berlin"))

  temp$chour <- floor(as.numeric(temp$dateobserved) / 3600)

  # ---- Calculate hourly means for July ----
  temp_hm <- temp %>%
    group_by(stationname, chour) %>%
    summarise(mh_temp = mean(temperature, na.rm = TRUE), .groups = "drop") %>%
    mutate(hour = as.POSIXct(chour * 3600, origin = "1970-01-01", tz = "Europe/Berlin"))

  # Store result in list
  all_temp_hm[[basename(file)]] <- temp_hm

  # ---- Create and save plot ----
  plot_name <- sub("\\.gpkg$", ".png", basename(file))
  plot_path <- file.path(output_folder, plot_name)

  p <- ggplot(temp_hm, aes(x = hour, y = mh_temp, color = stationname, group = stationname)) +
    geom_line() +
    geom_point(size = 1) +
    labs(
      title = paste("Hourly Mean Temperature -", basename(file)),
      x = "Timestamp", y = "Mean Temperature (°C)"
    ) +
    theme_minimal(base_size = 12) +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      legend.position = "none"
    )

  ggsave(plot_path, plot = p, width = 10, height = 6, dpi = 300)
}
```

--------------------------------------------------------------------------------

Calculate monthly mean temperature and standard deviation for the year of 2023

```{r monthly temp stats 2023}
# Define input and output folders
input_folder <- "../data/temp_HD"
output_folder <- "../monthly_summary_temp"
dir.create(output_folder, showWarnings = FALSE)

# List all .gpkg files
gpkg_files <- list.files(input_folder, pattern = "\\.gpkg$", full.names = TRUE)

# Create an empty list to store monthly summaries
monthly_stats_all <- list()

# Loop through each .gpkg file
for (file in gpkg_files) {
  cat("\nProcessing:", basename(file), "\n")

  # ---- Load and preprocess temp data ----
  temp <- sf::read_sf(file)
  temp <- temp[!is.na(temp$temperature), ]
  temp <- st_transform(temp, crs = "EPSG:32632")

  temp <- temp %>%
    mutate(dateobserved = as.POSIXct(dateobserved, format = "%Y-%m-%d %H:%M:%S", tz = "Europe/Berlin")) %>%
    filter(year(dateobserved) == 2023) %>%
    mutate(month = month(dateobserved, label = TRUE, abbr = TRUE))

  # ---- Monthly statistics: mean, SD, min, max ----
  monthly_stats <- temp %>%
    group_by(month) %>%
    summarise(
      mean_temp = mean(temperature, na.rm = TRUE),
      sd_temp   = sd(temperature, na.rm = TRUE),
      min_temp  = min(temperature, na.rm = TRUE),
      max_temp  = max(temperature, na.rm = TRUE),
      .groups = "drop"
    ) %>%
    mutate(source = basename(file))

  # Store result
  monthly_stats_all[[basename(file)]] <- monthly_stats

  # Save as CSV
  out_csv <- file.path(output_folder, paste0(tools::file_path_sans_ext(basename(file)), "_monthly_stats_2023.csv"))
  write.csv(monthly_stats, out_csv, row.names = FALSE)
}

# ---- Combine all into one summary table ----
combined_stats <- bind_rows(monthly_stats_all)

# Save combined table
write.csv(combined_stats, file.path(output_folder, "combined_monthly_stats_2023.csv"), row.names = FALSE)
```

```{r plot monthly mean temp and std}
ggplot(combined_stats, aes(x = month, y = mean_temp, group = source, color = source)) +
  geom_line() +
  geom_point(size = 3) +
  labs(
    title = "Monthly Mean Temperatures (2023)",
    x = "Month", y = "Mean Temperature (°C)"
  ) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "none")

ggplot(combined_stats, aes(x = month, y = sd_temp, group = source, color = source)) +
  geom_line() +
  geom_point(size = 3) +
  labs(
    title = "Monthly Temperature Standard Deviation (2023)",
    x = "Month", y = "Standard Deviation (°C)"
  ) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "none")

#ggsave(file.path(output_folder, "monthly_mean_temperatures_2023.png"), width = 10, height = 5, dpi = 300)
```

```{r}
combined_stats
```

--------------------------------------------------------------------------------

Calculate Spearman
Explanation why I chose Spearman:
- Pearson’s correlation assumes both normality and linearity in the relationship between X and Y. 
- Spearman’s correlation has less stringent assumptions, assuming only that the relationship is monotonic. This means that the relationship has to be consistently increasing or decreasing, but that it does not have to do so in a linear manner (see left). 
- Spearman’s correlation also performs better than Pearson’s correlation in instances where there are outliers or very few data point -> this is the case for my data.

```{r}
cor.test(temp_mm$elevation, temp_mm$monthly_mean_temperature, method = "spearman")

cor.test(temp_mm$tree_canopy_height, temp_mm$monthly_mean_temperature, method = "spearman")

cor.test(temp_mm$building_height, temp_mm$monthly_mean_temperature, method = "spearman")
```

Spearman’s test is non-parametric and based on ranking the values. It assumes that all values are distinct so that ranks are unique. When ties occur (e.g., two identical temperatures or building heights) the exact p-value can't be computed. So for these results, p-value can't be fully trusted (elevation has a statistically significant relationship with temperature) be the correlation coefficient (rho) can still be trusted.
It is also an influencing factor, that I only work with n=31 data points

--------------------------------------------------------------------------------

COULD MAYBE BE USED IN THE FUTURE TO COMBINE ALL THE TEMP MEASUREMENTS FROM ONE 
YEAR INTO ONE LAYER:

```{r}
# List all .gpkg files
gpkg_files <- list.files("../data/temp_HD/", pattern = "\\.gpkg$", full.names = TRUE)

# Read all files and bind them into one sf object
all_data <- gpkg_files %>%
  lapply(read_sf) %>%
  bind_rows()

# Write to a single layer in a new GeoPackage
st_write(all_data, "../data/combined_temp_data.gpkg", layer = "all_temp_data", driver = "GPKG")
```

```{r}
length(unique(all_data$entity_id))
```

