---
title: "RK_lm_different_timestamps"
author: "Johannes Gruenewald"
date: "2024-11-26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Load Required Libraries}

# Load Required Libraries

# Spatial Data Handling & Processing
library(sf)
library(sp)
library(terra)
library(raster)
library(stars)
library(spdep)
library(dplyr)

# Spatial Analysis & Interpolation
library(gstat)
library(randomForest)
library(nlme)
library(automap)
library(mgcv)

# Data Manipulation & Wrangling
library(tidyverse)

# Data Visualization
library(ggplot2)
library(ggspatial)
library(ggforce) 
library(scales)
library(gridExtra)
library(corrplot)
library(viridis)

# Mapping & Interactive Visualization
library(tmap)

# Additional
library(lattice)
library(car)
```

--------------------------------------------------------------------------------

```{r Load temp data}

# Complete data import and pre-processing

# Read temperature data for the entire year of 2023
temp <- sf::read_sf('../data/temp_HD/sensor_data_year_2023.geojson')

# Removes rows where temp is NA
temp <- temp %>% filter(!is.na(temperature))

# Reproject the data for Germany
temp <- st_transform(temp, crs="EPSG:32632")

# Clean the data from outliers -> currently working with hard, manual breaks
# Currently nothing is removed by this step because no measurements are above 40°C for the year 2023
temp_clean <- temp %>% filter(temperature >= -35, temperature <= 40)

# Calculate hour count since epoch, rounded down to current hour
temp_clean$chour <- floor(as.numeric(temp_clean$dateobserved) / 3600)

# Drop all measurement stations which didn't collect data the entire year around
temp_clean_station <- temp_clean %>%
  mutate(dateobserved = as.POSIXct(dateobserved, tz = "Europe/Berlin")) %>%
  filter(!entity_id %in% c(
    "hd:DE_Heidelberg_69120_12:WeatherObserved",
    "hd:DE_Heidelberg_69120_34:WeatherObserved",
    "hd:DE_Heidelberg_69123_41:WeatherObserved",
    "hd:DE_Heidelberg_46:WeatherObserved"
  ))

# Check if exactly 4 stations were dropped
length(unique(na.omit(temp_clean$entity_id)))
length(unique(na.omit(temp_clean_station$entity_id)))

# Calculate hourly mean temp for cleaned data
temp_clean_station_hm <- temp_clean_station %>%
    group_by(entity_id, chour) %>%
    summarise(hm_temp = mean(temperature, na.rm = TRUE), .groups = "drop") %>%
    mutate(hour = as.POSIXct(chour * 3600, origin = "1970-01-01", tz = "Europe/Berlin"))
```

--------------------------------------------------------------------------------

```{r Load and preprocess data}

# Import of pre-processed temp data and import and pre-processing of covariates 

# Import pre-processed hourly means, AOI and station coords
temp_clean_station_hm <- sf::read_sf('../data/temp_HD/temp_clean_station_hm.gpkg')
hd_bounds <- sf::read_sf('../data/AOI/OSM_boundaries_HD.geojson')
station_coords_sf <- sf::read_sf('../data/temp_HD/station_coords.gpkg')

# Drop Gaiberg station as it is outside of AOI
temp_clean_station_hm <- temp_clean_station_hm %>%
  filter(!entity_id %in% c(
    "hd:DE_Gaiberg_69251_21:WeatherObserved"
  ))

# Import the covariates
dem <- terra::rast("../data/DEM/hd_elevation_4326.tif")
tch <- terra::rast("../data/canopy_height/hd_canopy_height_4326.tif")
ghs_bh <- terra::rast("../data/building_height/hd_building_height_4326.tif")

# Mask out all values which are assigned to water and have value 101
# Not part of this area. But needs to be masked as well: 102 Snow/ice; 103 No data
tch[tch == 101] <- NA

# Reproject the data
temp_clean_station_hm <- st_transform(temp_clean_station_hm, crs="EPSG:32632")
hd_bounds <- st_transform(hd_bounds, crs="EPSG:32632")
station_coords_sf <- st_transform(station_coords_sf, crs="EPSG:32632")
dem <- terra::project(dem, y = crs(temp_clean_station_hm))
tch <- terra::project(tch, y = crs(temp_clean_station_hm))
ghs_bh <- terra::project(ghs_bh, y = crs(temp_clean_station_hm))

if (!identical(crs(tch), crs(dem))) {
  stop("CRS mismatch: Canopy height and DEM do not have the same coordinate reference system.")
}

if (!identical(crs(ghs_bh), crs(dem))) {
  stop("CRS mismatch: Building height and DEM do not have the same coordinate reference system.")
}

cat("All covariates share the same CRS.\n")

# Convert to SpatialPointsDataFrame
station_coords <- as(station_coords_sf, "Spatial")

# Create a Raster Stack for all covariate information

cat("Checking input raster resolutions (x by y):\n")
cat(sprintf("DEM:    %s\n", paste(terra::res(dem), collapse = " x ")))
cat(sprintf("TCH:    %s\n", paste(terra::res(tch), collapse = " x ")))
cat(sprintf("GHS_BH: %s\n\n", paste(terra::res(ghs_bh), collapse = " x ")))

# Align canopy height and building height to DEM resolution and extent
tch_aligned    <- terra::resample(tch, dem, method = "bilinear")
ghs_bh_aligned <- terra::resample(ghs_bh, dem, method = "near")

# Check alignment after resampling
if (!terra::compareGeom(dem, tch_aligned, ghs_bh_aligned, stopOnError = FALSE)) {
  stop("Covariate rasters are not perfectly aligned after resampling. Check extent/resolution/CRS.")
}

# Combine layers into a single SpatRaster stack
env_stack <- c(dem, tch_aligned, ghs_bh_aligned)

# Rename layers for clarity
names(env_stack) <- c("elevation", "canopy_height", "building_height")

# Code to include lat and lon information in raster stack

# Also add lat and lon to the raster stack
# Create coordinate rasters
lon <- terra::xFromCell(env_stack, 1:ncell(env_stack))
lat <- terra::yFromCell(env_stack, 1:ncell(env_stack))

# Convert to raster layers
lon_raster <- dem
lat_raster <- dem
values(lon_raster) <- lon
values(lat_raster) <- lat
names(lon_raster) <- "lon"
names(lat_raster) <- "lat"

# Stack the three raster layers
grids <- c(env_stack, lon_raster, lat_raster)

# Preparation of grid as a covariate for running RK
grids_ras <- stack(grids)

# Convert to SpatialGridDataFrame
grids_sp <- as(grids_ras, "SpatialGridDataFrame")

grids_sp@data
```

--------------------------------------------------------------------------------

```{r Create output folder}

# Create output dirs
output_dir <- "../RK_07_2023_12days"
if (!dir.exists(output_dir)) dir.create(output_dir)

variogram_dir <- file.path(output_dir, "variogram")
cv_dir        <- file.path(output_dir, "cross_validation")
rk_dir        <- file.path(output_dir, "rk_predictions")

dir.create(variogram_dir, showWarnings = FALSE)
dir.create(cv_dir, showWarnings = FALSE)
dir.create(rk_dir, showWarnings = FALSE)

# Datetime helpers
tz_berlin <- "Europe/Berlin"

ts_to_posix <- function(ch) {
  if (inherits(ch, "POSIXt")) return(ch)
  if (is.numeric(ch)) return(as.POSIXct(ch * 3600, origin = "1970-01-01", tz = tz_berlin))
  as.POSIXct(ch, format = "%Y-%m-%d %H:%M:%S", tz = tz_berlin)
}

nice_dt     <- function(x) format(x, "%Y-%m-%d %H:00 %Z")                                   
file_dt     <- function(x) format(x, "%Y-%m-%d_%H00")                                       
```

--------------------------------------------------------------------------------

```{r Main loop}

# Set timestamps and analysis period
timestamps_all <- sort(unique(temp_clean_station_hm$hour))
start_date <- as.POSIXct("2023-07-01 00:00:00", tz = "Europe/Berlin")
end_date   <- as.POSIXct("2023-07-12 23:00:00", tz = "Europe/Berlin")
timestamps <- timestamps_all[timestamps_all >= start_date & timestamps_all <= end_date]
cat("Looping from:", min(timestamps), "to", max(timestamps), "\n")

# Counters and progress bar
total     <- length(timestamps)
completed <- 0L
skipped   <- 0L
pb <- txtProgressBar(min = 0, max = total, style = 3)
cat(sprintf("Processing %d timestamps…\n", total))

# Init empty containers
total_metrics <- data.frame()
all_temp_data <- data.frame()

# Main loop over timestamps
for (i in seq_along(timestamps)) {
  ts <- timestamps[i]                                    
  dt_local <- ts_to_posix(ts)                     
  dt_lab   <- nice_dt(dt_local)                   
  dt_file  <- file_dt(dt_local)                   
  cat(sprintf("\n[%d/%d] Processing: chour=%s  (%s)\n", i, total, ts, dt_lab))  
  
  # ------------------------------------------------
  # STEP 1: Filter hourly data and visualize
  # ------------------------------------------------
  
  sel_temp_hm <- temp_clean_station_hm %>%
    filter(hour == ts)
  
  # Skip if fewer than 20 data points for current hour
  if (nrow(sel_temp_hm) < 20) {                                 
    skipped <- skipped + 1L                                       
    cat("  → skipped (n < 20)\n")                          
    next                                                         
  }
  
  # Append the selected temperature data to full dataset
  all_temp_data <- rbind(all_temp_data, sel_temp_hm)
  
  # plot hourly mean temperatures for all stations
  p_all <- ggplot(all_temp_data, aes(x = hour, y = hm_temp, color = entity_id, group = entity_id)) +  
    geom_line() +
    geom_point(size = 1) +
    labs(title = "Hourly Mean Temperature at Each Station",        
         x = "Date & Hour (Europe/Berlin)", y = "Mean Temperature (°C)", color = "Station") + 
    scale_x_datetime(date_labels = "%d.%m\n%H:%M", date_breaks = "12 hours") +              
    theme_minimal(base_size = 12) +
    theme(axis.text.x = element_text(angle = 0, hjust = 0.5))                                  
  
  # Save the temperature plot
  ggsave(file.path(output_dir, "all_stations_hourly_mean_temperature.png"),
         plot = p_all, width = 10, height = 6, dpi = 300)

  # ------------------------------------------------
  # STEP 2: Extract covariates from spatial grid
  # ------------------------------------------------
  
  # Convert Spatial object to RasterBrick
  r_brick <- brick(grids_sp)
  # Convert RasterBrick to terra SpatRaster object
  grid_terra <- rast(r_brick) 
  # Extract raster values at the point locations
  extracted_vals <- terra::extract(grid_terra, sel_temp_hm)
  # Remove the first column (ID)
  extracted_vals <- extracted_vals[,-1]
  # Combine the point data with the extracted raster values
  temp_shm.ex <- cbind(as.data.frame(sel_temp_hm), extracted_vals)
  
  # ------------------------------------------------
  # STEP 3: Fit multiple linear regression
  # ------------------------------------------------
  
  lm.temp_shm <- lm(hm_temp ~ elevation + canopy_height + building_height, data = temp_shm.ex)
  summary_lm <- summary(lm.temp_shm)
  
  # Calculate fitted values and residuals
  pred <- predict(lm.temp_shm, temp_shm.ex)
  resid <- temp_shm.ex$hm_temp - pred
  
  # Collect regression metrics
  model_metrics <- data.frame(
    timestamp      = ts,
    r_squared      = summary_lm$r.squared,
    adj_r_squared  = summary_lm$adj.r.squared,
    f_statistic    = summary_lm$fstatistic[1],
    f_pvalue       = pf(summary_lm$fstatistic[1],
                        summary_lm$fstatistic[2],
                        summary_lm$fstatistic[3],
                        lower.tail = FALSE),
    residual_se    = summary_lm$sigma,
    rmse           = sqrt(mean(resid^2)),
    mae            = mean(abs(resid)),
    AIC            = AIC(lm.temp_shm),
    BIC            = BIC(lm.temp_shm)
  )
  
  # Append results to cumulative LM table
  if (!exists("lm_metrics_all")) {
    lm_metrics_all <- model_metrics
  } else {
    lm_metrics_all <- rbind(lm_metrics_all, model_metrics)
  }

  # Attach residuals to dataset and convert to sf for spatial operations
  temp_shm.ex$residuals <- resid
  temp_shm.sf <- st_as_sf(temp_shm.ex) |> st_cast("POINT")
  st_crs(temp_shm.sf) <- st_crs(grids_sp)
  
  # ------------------------------------------------
  # STEP 4: Compute variogram and fit model
  # ------------------------------------------------
  
  # Compute empirical variogram map of model residuals
  varmap <- variogram(residuals ~ 1, data = temp_shm.sf, map = TRUE,
                    # cutoff is half the spatial extent (or diameter) of the study area
                    cutoff = sqrt(areaSpatialGrid(grids_sp)) / 2, 
                    # scaled relative to the resolution of the grid
                    width = 30 * grids_sp@grid@cellsize[1])

  # Store cutoff and bin width values for reuse and labeling
  cutoff_val <- sqrt(areaSpatialGrid(grids_sp)) / 2
  width_val  <- 30 * grids_sp@grid@cellsize[1]
  
  # Report how many rows are in variogram map
  cat("Variogram map has", nrow(as.data.frame(varmap)), "rows\n")
  
  # Define output filename for the variogram map plot
  file_varmap <- file.path(variogram_dir, paste0(dt_file, "_variogram_map.png"))  
  
  # Plot and save the directional variogram map
  png(file_varmap, width = 8, height = 6, units = "in", res = 300)
  print(
    plot(varmap,
         col.regions = grey(rev(seq(0, 1, 0.025))),
         main = sprintf("Directional Variogram Map (cutoff: %.1f km, width: %.1f km)\n%s",  
                        cutoff_val / 1000, width_val / 1000, dt_lab))
  )
  dev.off()
  
  # Compute directional variograms for North–South (0°) and East–West (90°)
  rv.temp <- variogram(residuals ~ 1, temp_shm.sf, alpha = c(0, 45, 90, 135))

  # Fit theoretical exponential variogram model to the directional variogram
  # Compute experimental variogram
  rv.temp <- variogram(residuals ~ 1, temp_shm.sf)
  
  # Try several initial models and pick the best fit
  init_models <- list(
    vgm("Sph"),
    vgm("Exp"),
    vgm("Gau")
  )
  
  fit_list <- lapply(init_models, function(m) fit.variogram(rv.temp, m))
  fit_errors <- sapply(fit_list, attr, "SSErr")
  rvgm.temp <- fit_list[[which.min(fit_errors)]]

  
  # Define output filename for directional variogram fit plot
  file_varfit <- file.path(variogram_dir, paste0(dt_file, "_directional_variogram_fit.png"))  
 
  # Plot the empirical and fitted directional variograms and save
  png(file_varfit, width = 8, height = 6, units = "in", res = 300)
  print(
    plot(rv.temp, rvgm.temp,
         main = paste0("Directional Variogram Fit (0°, 45°, 90°, 135°)\n", dt_lab),   
         plot.nu = FALSE, cex = 1.5, pch = "+", col = "black")
  )
  dev.off()
  
  # ------------------------------------------------
  # STEP 5a: Cross-validation of variogram model
  # ------------------------------------------------

  cv_resid <- krige.cv(residuals ~ 1,
                       locations = temp_shm.sf,
                       model = rvgm.temp,
                       nfold = nrow(temp_shm.sf),   # one fold per observation = LOOCV
                       nmax = 10)                   # max number of neighbors used for prediction
  
  # Compute CV performance metrics
  cv_rmse <- sqrt(mean(cv_resid$residual^2))
  cv_mae  <- mean(abs(cv_resid$residual))
  cv_me   <- mean(cv_resid$residual)
  cat("CV RMSE:", round(cv_rmse, 2), "MAE:", round(cv_mae, 2), "Bias:", round(cv_me, 2), "\n")
  
  # Store CV results  
  cv_metrics <- data.frame(
    timestamp = ts,
    cv_rmse   = cv_rmse,
    cv_mae    = cv_mae,
    cv_bias   = cv_me
  )
  
  if (!exists("cv_metrics_all")) {
    cv_metrics_all <- cv_metrics
  } else {
    cv_metrics_all <- rbind(cv_metrics_all, cv_metrics)
  }
  
  # Plot predicted vs residuals (CV)
  p_cv <- ggplot(cv_resid, aes(x = var1.pred, y = residual)) +
    geom_point() +
    geom_hline(yintercept = 0, linetype = "dashed") +
    labs(title = paste("Cross-validation Residuals\n", dt_lab), x = "Predicted", y = "Residual") +
    theme_minimal(base_size = 12)
  
  ggsave(filename = file.path(cv_dir, paste0(dt_file, "_cv_residuals_plot.png")),  
         plot = p_cv, width = 8, height = 6, dpi = 300)
  
  # ------------------------------------------------
  # STEP 5b: LOOCV for full Regression Kriging
  # ------------------------------------------------

  cv_rk <- krige.cv(
    formula   = hm_temp ~ elevation + canopy_height + building_height,
    locations = temp_shm.sf,
    model     = rvgm.temp,                 # variogram of residuals
    nfold     = nrow(temp_shm.sf),         # LOOCV
    nmax      = 10
  )

  # krige.cv returns: observed, var1.pred, residual (= observed - predicted)

  rk_cv_rmse <- sqrt(mean(cv_rk$residual^2))
  rk_cv_mae  <- mean(abs(cv_rk$residual))
  rk_cv_bias <- mean(cv_rk$residual)

  cat("RK LOOCV RMSE:", round(rk_cv_rmse, 2),
      "MAE:", round(rk_cv_mae, 2),
      "Bias:", round(rk_cv_bias, 2), "\n")

  # Store RK CV results
  rk_cv_metrics <- data.frame(
    timestamp   = ts,
    rk_cv_rmse  = rk_cv_rmse,
    rk_cv_mae   = rk_cv_mae,
    rk_cv_bias  = rk_cv_bias
  )

  if (!exists("rk_cv_metrics_all")) {
    rk_cv_metrics_all <- rk_cv_metrics
  } else {
    rk_cv_metrics_all <- rbind(rk_cv_metrics_all, rk_cv_metrics)
  }

  # Plot RK CV residuals
  p_rk_cv <- ggplot(cv_rk, aes(x = var1.pred, y = residual)) +
    geom_point() +
    geom_hline(yintercept = 0, linetype = "dashed") +
    labs(
      title = paste("RK LOOCV Residuals\n", dt_lab),
      x     = "RK predicted (°C)",
      y     = "Residual (Observed - Predicted)"
    ) +
    theme_minimal(base_size = 12)

  ggsave(
    filename = file.path(cv_dir, paste0(dt_file, "_rk_loocv_residuals_plot.png")),
    plot     = p_rk_cv,
    width    = 8, height = 6, dpi = 300
  )


  # ------------------------------------------------
  # STEP 6: Regression Kriging prediction
  # ------------------------------------------------
    
  # define kriging model using fitted variogram
  g.resid <- gstat(id = c("residuals"), formula = residuals ~ 1, data = temp_shm.sf,
                   nmax = 31, model = rvgm.temp)
  
  # Predict deterministic trend (LM) and stochastic residuals (OK)
  locTEMP.reg <- predict(lm.temp_shm, grids_sp)
  locTEMP <- predict(g.resid, grids_sp, beta=1, BLUE=FALSE)
  
  # Combine LM + kriged residuals
  rk.pred <- locTEMP.reg + locTEMP$residuals.pred
  
  # Add predictions to grid and export as GeoTIFF
  grid_out <- grids_sp
  grids_sp$rk_pred <- rk.pred
  rk_raster <- terra::rast(grids_sp["rk_pred"])
  rk_file_tif <- file.path(rk_dir, paste0(dt_file, "_rk_prediction_map.tif"))
  terra::writeRaster(
    rk_raster,
    filename = rk_file_tif,
    overwrite = TRUE,
    gdal = c("COMPRESS=LZW")  # optional compression
  )
  cat("RK prediction saved as GeoTIFF:", rk_file_tif, "\n")
  
  # Plot RK prediction map
  rk_file_img <- file.path(rk_dir, paste0(dt_file, "_rk_prediction_map", ".png"))   
  png(rk_file_img, width = 8, height = 6, units = "in", res = 300)
  print(
    spplot(grids_sp, "rk_pred",
           main = paste("Regression Kriging Prediction (°C)\n", dt_lab),   
           sp.layout = list("sp.points", station_coords, pch = 20, col = "white")))
  dev.off()
  
  # ------------------------------------------------
  # STEP 7: Evaluate RK predictions at station points
  # ------------------------------------------------
  
  # Match station coordinates with observed stations only
  obs_coords <- st_coordinates(temp_shm.sf)
  
  # Convert to SpatialPoints object with same CRS as prediction grid
  obs_stations <- SpatialPoints(coords = obs_coords, proj4string = CRS(proj4string(grids_sp)))
  
  # Extract only at observed station locations
  rk_at_obs <- sp::over(obs_stations, grids_sp)
  
  # Combine predictions with observations
  rk_eval <- cbind(rk_at_obs, obs = temp_shm.sf$hm_temp)

  # Check if predicted and observed data match in length
  cat("Predicted rows:", nrow(rk_at_obs), "\n")
  cat("Observed rows :", length(temp_shm.sf$hm_temp), "\n")
  
  # Calculate prediction error
  rk_eval$residual <- rk_eval$obs - rk_eval$rk_pred
  
  # Compute RK evaluation metrics
  rmse <- sqrt(mean(rk_eval$residual^2, na.rm = TRUE))
  mae  <- mean(abs(rk_eval$residual), na.rm = TRUE)
  bias <- mean(rk_eval$residual, na.rm = TRUE)
  cat("RK RMSE:", round(rmse, 2), "MAE:", round(mae, 2), "Bias:", round(bias, 2), "\n")
  
  # Append RK metrics
  rk_metrics <- data.frame(timestamp = ts, rk_rmse   = rmse, rk_mae    = mae, rk_bias   = bias)
  if (!exists("rk_metrics_all")) {
    rk_metrics_all <- rk_metrics
  } else {
    rk_metrics_all <- rbind(rk_metrics_all, rk_metrics)
  }
  
  # Plot RK residuals scatter
  rk_resid_plot <- ggplot(rk_eval, aes(x = rk_pred, y = residual)) +
    geom_point() +
    geom_hline(yintercept = 0, linetype = "dashed") +
    labs(title = paste("RK Prediction Residuals at Stations\n", dt_lab),   
         x = "RK Predicted (°C)", y = "Residual (Observed - Predicted)") +
    theme_minimal(base_size = 12)
  
  ggsave(filename = file.path(rk_dir, paste0(dt_file, "_rk_residuals_plot.png")),  
         plot = rk_resid_plot, width = 8, height = 6, dpi = 300)
  
  # Update progress bar
  completed <- completed + 1L 
  setTxtProgressBar(pb, completed)
  cat("\n")
  cat(sprintf("done. Completed: %d/%d (skipped: %d)\n",      
              completed, total, skipped))                        
}

# ------------------------------------------------
# Finalize and save results
# ------------------------------------------------

close(pb)                                                        
cat(sprintf("\nFinished. Completed: %d/%d | Skipped: %d\n",       
            completed, total, skipped))                         

# Save linear model metrics
metrics_file <- file.path(output_dir, "lm_model_performance_summary.csv")
if (exists("lm_metrics_all")) {                                     
  lm_metrics_all$datetime_local <- ts_to_posix(lm_metrics_all$timestamp)   
  lm_metrics_all <- lm_metrics_all %>% relocate(datetime_local, .after = timestamp)  
  write.csv(lm_metrics_all, metrics_file, row.names = FALSE)
}

# Save CV metrics
cv_file <- file.path(output_dir, "cross_validation_metrics.csv")
if (exists("cv_metrics_all")) {                                     
  cv_metrics_all$datetime_local <- ts_to_posix(cv_metrics_all$timestamp) 
  cv_metrics_all <- cv_metrics_all %>% relocate(datetime_local, .after = timestamp) 
  write.csv(cv_metrics_all, cv_file, row.names = FALSE)
}

# Save RK LOOCV metrics
rk_cv_file <- file.path(output_dir, "rk_loocv_metrics.csv")
if (exists("rk_cv_metrics_all")) {
  rk_cv_metrics_all$datetime_local <- ts_to_posix(rk_cv_metrics_all$timestamp)
  rk_cv_metrics_all <- rk_cv_metrics_all %>%
    relocate(datetime_local, .after = timestamp)
  write.csv(rk_cv_metrics_all, rk_cv_file, row.names = FALSE)
}

# Save RK metrics
rk_file <- file.path(output_dir, "rk_metrics.csv")
if (exists("rk_metrics_all")) {                                   
  rk_metrics_all$datetime_local <- ts_to_posix(rk_metrics_all$timestamp)
  rk_metrics_all <- rk_metrics_all %>% relocate(datetime_local, .after = timestamp) 
  write.csv(rk_metrics_all, rk_file, row.names = FALSE)
}
```

--------------------------------------------------------------------------------

Plots to analyse model performance metrices for entire month

```{r Analyse LM metrics}

# Read mlr metrics
lm_metrics <- read.csv("../rk_results_07-2023/lm_model_performance_summary.csv",
                       stringsAsFactors = FALSE)

# Parse datetime column correctly (same approach as CV & RK)
lm_metrics <- lm_metrics %>%
  mutate(
    datetime_calc = as.POSIXct(datetime_local, format = "%Y-%m-%d %H:%M:%S", tz = "Europe/Berlin"),
    date = as.Date(datetime_calc),
    hour = lubridate::hour(datetime_calc)
  )

# Reshape to long format
lm_metrics_long <- lm_metrics %>%
  pivot_longer(cols = c(r_squared, adj_r_squared),
               names_to = "metric",
               values_to = "value")

# Filter year 2023
lm_df_plot <- lm_metrics_long %>%
  filter(year(date) == 2023)

# Make sure hour is numeric
lm_df_plot$hour <- as.numeric(lm_df_plot$hour)

# Number of days per plot
days_per_page <- 12  

# Count total pages needed
n_pages <- ceiling(length(unique(lm_df_plot$date)) / days_per_page)

# Loop over pages and plot mlr metrics
for (i in seq_len(n_pages)) {
  p_lm <- ggplot(lm_df_plot, aes(x = hour, y = value, color = metric, group = metric)) +
    geom_line(linewidth = 1.2) +
    geom_hline(aes(yintercept = 1, linetype = "Perfect Fit (R² = 1)"), color = "grey60") +
    geom_point(size = 1.8) +
    ggforce::facet_wrap_paginate(~ date, ncol = 3, nrow = 4, scales = "fixed", page = i) +  
    scale_color_manual(
      values = c("r_squared" = "#6699CC", "adj_r_squared" = "#CC6677"),
      labels = c("r_squared" = expression(R^2), "adj_r_squared" = expression(Adjusted~R^2))
    ) +
    scale_linetype_manual(name = "", values = c("Perfect Fit (R² = 1)" = "dotted")) +
    labs(
      title    = sprintf("Daily Evaluation of Multiple Linear Regression Model Performance (Page %d/%d)", i, n_pages),
      subtitle = "Hourly R² and Adjusted R² values from models predicting hourly mean temperature",
      x = "Hour of the Day", y = "Metric Value", color = "Metrics"
    ) +
    scale_x_continuous(breaks = c(0, 6, 12, 18, 24)) +
    theme_minimal(base_size = 14) +
    theme(
      legend.position  = "top",
      legend.box       = "horizontal",
      panel.grid.minor = element_blank(),
      panel.spacing    = unit(0.8, "lines"),
      strip.background = element_rect(fill = "grey90", color = NA),
      strip.text       = element_text(face = "bold")
    )

  # Save plot to output directory
  ggsave(
    filename = file.path(output_dir, sprintf("lm_daily_eval_07_2023_page_%02d.png", i)),
    plot = p_lm,
    width = 12, height = 10, dpi = 300
  )
}
```

```{r Analyse CV metrics}

# Read cv metrics
cv_metrics <- read.csv("../RK_07_2023_12days/rk_loocv_metrics.csv",
                       stringsAsFactors = FALSE)

cv_metrics <- cv_metrics %>%
  mutate(
    timestamp = if_else(
      nchar(timestamp) == 10, paste0(timestamp, " 00:00:00"), timestamp
    ),
    timestamp = as.POSIXct(timestamp, format = "%Y-%m-%d %H:%M:%S", tz = "Europe/Berlin"),
    date = as.Date(format(timestamp, tz = "Europe/Berlin")),
    hour  = hour(timestamp),
    month = month(timestamp),
    year  = year(timestamp)
  ) %>%
  filter(year == 2023, month == 7)

# Reshape to long format
cv_metrics_long <- cv_metrics %>%
  pivot_longer(cols = c(rk_cv_rmse, rk_cv_mae),
               names_to = "metric",
               values_to = "value")

# Filter nur 2023
df_plot <- cv_metrics_long %>%
  filter(year(date) == 2023)

df_plot <- df_plot %>%
  filter(!is.na(date)) %>%                  
  mutate(date = as.Date(date)) 

df_plot$hour <- as.numeric(df_plot$hour)

# Loop over pages and plot cv metrics
for (i in seq_len(n_pages)) {
  p_cv <- ggplot(df_plot, aes(x = hour, y = value, color = metric, group = metric)) +
    geom_line(linewidth = 1.2) +
    geom_hline(aes(yintercept = 0, linetype = "Perfect Prediction (Error = 0)"), color = "grey60") +
    geom_point(size = 1.8) +
    ggforce::facet_wrap_paginate(~ date, ncol = 3, nrow = 4, scales = "fixed", page = i) +  # 12 days per plot
    scale_color_manual(
      values = c(
        "rk_cv_rmse" = "#6699CC",
        "rk_cv_mae"  = "#CC6677"
      ),
      labels = c(
        "rk_cv_rmse" = "Root Mean Square Error (RMSE)",
        "rk_cv_mae"  = "Mean Absolute Error (MAE)"
      )
    ) +
    scale_linetype_manual(
      name = "",
      values = c("Perfect Prediction (Error = 0)" = "dotted")
    ) +
    labs(
      title    = "Daily Evaluation of the Regression-Kriging Leave-One-Out Cross-Validation Metrics",
      subtitle = "Hourly RMSE and MAE values from LOOCV of models predicting hourly mean temperature",
      x        = "Hour of the Day",
      y        = "Error Metric [°C]",
      color    = "Metrics"
    ) +
    scale_x_continuous(breaks = c(0, 6, 12, 18, 24)) +
    theme_minimal(base_size = 14) +
    theme(
      legend.position  = "top",
      legend.box       = "horizontal",
      panel.grid.minor = element_blank(),
      panel.spacing    = unit(0.8, "lines"),
      strip.background = element_rect(fill = "grey90", color = NA),
      strip.text       = element_text(face = "bold")
    )

  # Save plot to output directory
  ggsave(
    filename = file.path(output_dir, sprintf("cv_metrics_07_2023_by_day_page_%02d.png", i)),
    plot = p_cv,
    width = 12, height = 8, dpi = 300
  )
}
```

```{r Analyse RK metrics}

# Read rk metrics
rk_metrics <- read.csv("../rk_results_07-2023/rk_metrics.csv",
                       stringsAsFactors = FALSE)

# Parse datetime correctly
rk_metrics <- rk_metrics %>%
  mutate(
    datetime_calc = as.POSIXct(datetime_local, format = "%Y-%m-%d %H:%M:%S", tz = "Europe/Berlin"),
    date = as.Date(datetime_calc),
    hour = lubridate::hour(datetime_calc)
  )

# Reshape to long format
rk_metrics_long <- rk_metrics %>%
  pivot_longer(cols = c(rk_rmse, rk_mae),
               names_to = "metric",
               values_to = "value")

# Filter year 2023
df_plot <- rk_metrics_long %>%
  filter(year(date) == 2023)

# Make sure hour is numeric
df_plot$hour <- as.numeric(df_plot$hour)

# Loop over pages and plot rk metrics
for (i in seq_len(n_pages)) {
  p_rk <- ggplot(df_plot, aes(x = hour, y = value, color = metric, group = metric)) +
    geom_line(linewidth = 1.2) +
    geom_hline(aes(yintercept = 0, linetype = "Perfect Prediction (Error = 0)"), color = "grey60") +
    geom_point(size = 1.8) +
    ggforce::facet_wrap_paginate(~ date, ncol = 3, nrow = 4, scales = "fixed", page = i) +  # 12 days/page
    scale_color_manual(
      values = c(
        "rk_rmse" = "#6699CC",
        "rk_mae"  = "#CC6677"
      ),
      labels = c(
        "rk_rmse" = "Root Mean Square Error (RMSE)",
        "rk_mae"  = "Mean Absolute Error (MAE)"
      )
    ) +
    scale_linetype_manual(
      name = "",
      values = c("Perfect Prediction (Error = 0)" = "dotted")
    ) +
    labs(
      title    = sprintf("Daily Evaluation of the Regression-Kriging Model Performance Metrics (Page %d/%d)", i, n_pages),
      subtitle = "Hourly RMSE and MAE values from models predicting hourly mean temperature",
      x        = "Hour of the Day",
      y        = "Error Metric [°C]",
      color    = "Metrics"
    ) +
    scale_x_continuous(breaks = c(0, 6, 12, 18, 24)) +
    theme_minimal(base_size = 14) +
    theme(
      legend.position  = "top",
      legend.box       = "horizontal",
      panel.grid.minor = element_blank(),
      panel.spacing    = unit(0.8, "lines"),
      strip.background = element_rect(fill = "grey90", color = NA),
      strip.text       = element_text(face = "bold")
    )

  # Save plot to output directory
  ggsave(
    filename = file.path(output_dir, sprintf("rk_metrics_07_2023_by_day_page_%02d.png", i)),
    plot = p_rk,
    width = 12, height = 8, dpi = 300
  )
}
```

--------------------------------------------------------------------------------

Plots to analyse model performance metrices for less than 13 days

```{r Analyse MLR metrics less 13}

# Daily Evaluation of Multiple Linear Regression Model Performance
p_lm <- ggplot(lm_df_plot, aes(x = hour, y = value, color = metric, group = metric)) +
  geom_line(linewidth = 1.2) +
  geom_hline(aes(yintercept = 1, linetype = "Perfect Fit (R² = 1)"),color = "grey60") +
  geom_point(size = 1.8) +
  facet_wrap(~ date, ncol = 3, scales = "fixed") +
  scale_color_manual(
    values = c(
      "r_squared"     = "#6699CC",
      "adj_r_squared" = "#CC6677"
    ),
    labels = c("r_squared"     = expression(R^2), "adj_r_squared" = expression(Adjusted~R^2))
  ) +
  scale_linetype_manual(name   = "", values = c("Perfect Fit (R² = 1)" = "dotted")) +
  labs(
    title    = "Daily Evaluation of Multiple Linear Regression Model Performance",
    subtitle = "Hourly R² and Adjusted R² values from models predicting hourly mean temperature",
    x        = "Hour of the Day",
    y        = "Metric Value",
    color    = "Metrics"
  ) +
  scale_x_continuous(breaks = c(0, 6, 12, 18, 24)) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position    = "top",
    legend.box         = "horizontal",
    panel.grid.minor   = element_blank(),
    panel.spacing      = unit(0.8, "lines"),
    strip.background   = element_rect(fill = "grey90", color = NA),
    strip.text         = element_text(face = "bold")
  )

# Show the plot
p_lm

# Save plot
# ggsave("../plots/lm_metrics_by_day_01-01-2023.png", plot = p_lm, width = 12, height = 8, dpi = 300)
```

```{r Analyse CV metrics less 13}

# Daily Evaluation of the Regression-Kriging Cross-Validation Metrics
p_cv <- ggplot(df_plot, aes(x = hour, y = value, color = metric, group = metric)) +
    geom_line(linewidth = 1.2) +
    geom_hline(aes(yintercept = 0, linetype = "Perfect Prediction (Error = 0)"), color = "grey60") +
    geom_point(size = 1.8) +
    ggforce::facet_wrap_paginate(~ date, ncol = 3, nrow = 4, scales = "fixed") +  
    scale_color_manual(
      values = c(
        "rk_cv_rmse" = "#6699CC",
        "rk_cv_mae"  = "#CC6677"
      ),
      labels = c(
        "rk_cv_rmse" = "Root Mean Square Error (RMSE)",
        "rk_cv_mae"  = "Mean Absolute Error (MAE)"
      )
    ) +
    scale_linetype_manual(
      name = "",
      values = c("Perfect Prediction (Error = 0)" = "dotted")
    ) +
    labs(
      title    = "Daily Evaluation of the Regression-Kriging Leave-One-Out Cross-Validation Metrics",
      subtitle = "Hourly RMSE and MAE values from LOOCV of models predicting hourly mean temperature",
      x        = "Hour of the Day",
      y        = "Error Metric [°C]",
      color    = "Metrics"
    ) +
    scale_x_continuous(breaks = c(0, 6, 12, 18, 24)) +
    theme_minimal(base_size = 14) +
    theme(
      legend.position  = "top",
      legend.box       = "horizontal",
      panel.grid.minor = element_blank(),
      panel.spacing    = unit(0.8, "lines"),
      strip.background = element_rect(fill = "grey90", color = NA),
      strip.text       = element_text(face = "bold")
    )

p_cv

# Save plot to output directory
# ggsave(
#   filename = file.path(output_dir, "cv_metrics_07_2023_by_day_page.png"),
#   plot = p_cv,
#   width = 12, height = 8, dpi = 300
# )
```

```{r Analyse RK metrics less 13}

# Daily Evaluation of the Regression-Kriging Model Performance Metrics
p_rk <- ggplot(df_plot, aes(x = hour, y = value, color = metric, group = metric)) +
  geom_line(linewidth = 1.2) +
  geom_hline(aes(yintercept = 0, linetype = "Perfect Prediction (Error = 0)"),color = "grey60") +
  geom_point(size = 1.8) +
  facet_wrap(~ date, ncol = 3, scales = "fixed") +   # gleiche y-Achse
  scale_color_manual(
    values = c(
      "rk_rmse" = "#6699CC",
      "rk_mae"  = "#CC6677"
    ),
    labels = c(
      "rk_rmse" = "Root Mean Square Error (RMSE)",
      "rk_mae"  = "Mean Absolute Error (MAE)"
    )
  ) +
  scale_linetype_manual(name   = "", values = c("Perfect Prediction (Error = 0)" = "dotted")) +
  labs(
    title    = "Daily Evaluation of the Regression-Kriging Model Performance Metrics",
    subtitle = "Hourly RMSE and MAE values from models predicting hourly mean temperature",
    x        = "Hour of the Day",
    y        = "Error Metric [°C]",
    color    = "Metrics"
  ) +
  scale_x_continuous(breaks = c(0, 6, 12, 18, 24)) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position   = "top",
    panel.grid.minor  = element_blank(),
    strip.background  = element_rect(fill = "grey90", color = NA),
    strip.text        = element_text(face = "bold")
  )

# Show the plot
p_rk

# Save plot
# ggsave("../plots/rk_metrics_week-01-2023.png", plot = p_rk, width = 12, height = 8, dpi = 300)
```

--------------------------------------------------------------------------------

Impact of weather conditions on model performance

```{r}

# Prep weather condition data
weather_df <- sf::read_sf('../data/temp_HD/sensor_data_year_2023.geojson')

# Drop all measurement stations which didn't collect data the entire year around
weather_df <- weather_df %>%
  filter(!entity_id %in% c(
    "hd:DE_Heidelberg_69120_12:WeatherObserved",
    "hd:DE_Heidelberg_69120_34:WeatherObserved",
    "hd:DE_Heidelberg_69123_41:WeatherObserved",
    "hd:DE_Gaiberg_69251_21:WeatherObserved",
    "hd:DE_Heidelberg_46:WeatherObserved"
  ))

weather_df_subset <- weather_df %>%
  mutate(dateobserved = as.POSIXct(dateobserved, tz = "Europe/Berlin")) %>%
  # Filter to July 1 to July 12
  filter(
      dateobserved >= ymd_hms("2023-07-01 00:00:00", tz = "Europe/Berlin"),
      dateobserved <= ymd_hms("2023-07-12 23:00:00", tz = "Europe/Berlin")
  )  %>%
  select(-stationname, -description, -location, -path, -layer)

df_env <- weather_df_subset %>% 
  st_drop_geometry()

df_env <- df_env %>%
  mutate(hour = floor_date(dateobserved, "hour"))

df_hourly_means <- df_env %>%
  group_by(hour) %>%
  summarise(
    temperature_mean       = mean(temperature, na.rm = TRUE),
    relativehumidity_mean  = mean(relativehumidity, na.rm = TRUE),
    dewpoint_mean          = mean(dewpoint, na.rm = TRUE),
    pressure_mean          = mean(atmosphericpressure, na.rm = TRUE),
    solarradiation_mean    = mean(solarradiation, na.rm = TRUE),
    precipitation_mean     = mean(precipitation, na.rm = TRUE),
    windspeed_mean         = mean(windspeed, na.rm = TRUE),
    
    n_stations = n_distinct(entity_id),
    .groups = "drop"
  )
```


```{r}

# Add mean temperature across all weather stations for each hour
df_hourly_mean <- temp_clean_station_hm %>%
  
  # Filter to July 1 to July 12
  filter(
    hour >= ymd_hms("2023-07-01 00:00:00", tz = "Europe/Berlin"),
    hour <= ymd_hms("2023-07-12 23:00:00", tz = "Europe/Berlin")
  ) %>%
  
  # Calculate hourly mean temperature across stations
  group_by(hour) %>%
  summarise(
    temp_mean = mean(hm_temp, na.rm = TRUE),
    n_stations = n_distinct(entity_id),
    .groups = "drop"
  )

str(df_hourly_mean)

df_hourly_temp <- df_hourly_mean %>%
  mutate(
    date = as.Date(hour),
    hour_num = lubridate::hour(hour)
  )

temp_min <- min(df_hourly_temp$temp_mean, na.rm = TRUE)
temp_max <- max(df_hourly_temp$temp_mean, na.rm = TRUE)

# We want to map temp range onto error range (0–3.5)
error_min <- 0
error_max <- 3.5

df_hourly_temp <- df_hourly_temp %>%
  mutate(
    temp_scaled = scales::rescale(
      temp_mean,
      to   = c(error_min, error_max),
      from = c(temp_min, temp_max)
    )
  )

```

```{r}
p_cv <- ggplot(df_plot, aes(x = hour, y = value, color = metric, group = metric)) +
    # Temperature as histogram-like bars in the background
  geom_col(
    data = df_hourly_temp,
    aes(
      x = hour_num,
      y = temp_scaled,
      fill = temp_mean
    ),
    inherit.aes = FALSE,
    alpha = 0.45,
    width = 0.9
  ) +
  
  # Temperature color ramp
  scale_fill_gradientn(
    colours = viridis::viridis(100, option = "C"),
    name = "Mean Temp (°C)"
  ) +
  
    geom_line(linewidth = 1.2) +
    geom_hline(aes(yintercept = 0, linetype = "Perfect Prediction (Error = 0)"), color = "grey60") +
    geom_point(size = 1.8) +
    ggforce::facet_wrap_paginate(~ date, ncol = 3, nrow = 4, scales = "fixed") +  
    scale_color_manual(
      values = c(
        "rk_cv_rmse" = "#6699CC",
        "rk_cv_mae"  = "#CC6677"
      ),
      labels = c(
        "rk_cv_rmse" = "Root Mean Square Error (RMSE)",
        "rk_cv_mae"  = "Mean Absolute Error (MAE)"
      )
    ) +
    scale_linetype_manual(
      name = "",
      values = c("Perfect Prediction (Error = 0)" = "dotted")
    ) +
    labs(
      title    = "Daily Evaluation of the Regression-Kriging Leave-One-Out Cross-Validation Metrics",
      subtitle = "Hourly RMSE and MAE values from LOOCV of models predicting hourly mean temperature",
      x        = "Hour of the Day",
      y        = "Error Metric [°C]",
      color    = "Metrics"
    ) +
    scale_x_continuous(breaks = c(0, 6, 12, 18, 24)) +
    theme_minimal(base_size = 14) +
    theme(
      legend.position  = "top",
      legend.box       = "horizontal",
      panel.grid.minor = element_blank(),
      panel.spacing    = unit(0.8, "lines"),
      strip.background = element_rect(fill = "grey90", color = NA),
      strip.text       = element_text(face = "bold")
    )

p_cv

```


--------------------------------------------------------------------------------

Comparison of the Urban Climate Analysis of HD (2023) with the 
RK temperature prediction surface

```{r Compare the Stadtklima-Analyse with my RK prediction surface}

# Load rasters and vector data
mod  <- rast("../rk_results_07-09-2023/rk_predictions/2023-07-09_1400_rk_prediction_map.tif")
ref5 <- rast("../data/reference/2_22_074_ist_t14_rev01.tif")
river <- st_read("../data/waterways/waterways_HD.gpkg")                 
landuse <- st_read("../data/DE522L1_HEIDELBERG_UA2018_v013/Data/DE522L1_HEIDELBERG_UA2018_v013.gpkg")

# Check if model has CRS
if (is.na(crs(mod))) stop("`mod` has no CRS. Set it first.")

# Define CRS and project reference raster to model grid
crs_mod <- crs(mod, proj = TRUE)
crs(ref5) <- "EPSG:32632"
ref30 <- project(ref5, mod, method = "bilinear")

# Reproject vector layers to project CRS
crs_mod <- crs(mod, proj = TRUE)
hd_bounds <- st_transform(hd_bounds, crs = crs_mod)
river     <- st_transform(river,     crs = crs_mod)
landuse   <- st_transform(landuse,   crs = crs_mod)

# Crop and mask rasters by AOI
mod    <- mask(crop(mod,    vect(hd_bounds)), vect(hd_bounds))
ref30  <- mask(crop(ref30,  vect(hd_bounds)), vect(hd_bounds))

# Clip vector layers to AOI
river <- st_intersection(river, hd_bounds)
landuse <- st_intersection(landuse, hd_bounds)

# Verify alignment between datasets
stopifnot(terra::same.crs(ref30, mod))
print(st_crs(hd_bounds)$input)
stopifnot(isTRUE(all.equal(res(mod),  res(ref30))))
stopifnot(isTRUE(all.equal(ext(mod),  ext(ref30))))
```

```{r Model–Reference Temperature Comparison and Metrics}

# Select single-hour rasters
mod_h   <- mod    
ref30_h <- ref30       

# Apply common NA mask
both_mask <- !is.na(values(mod_h)) & !is.na(values(ref30_h))
mod_h[!both_mask]   <- NA
ref30_h[!both_mask] <- NA

# Extract cell values from both rasters
vr <- values(c(mod_h, ref30_h))           
colnames(vr) <- c("rk_temp", "ref_temp")  # rename columns
comparison_df <- as.data.frame(vr) |> drop_na() 

# Calculate comparison metrics
pearson_r <- cor(comparison_df$rk_temp, comparison_df$ref_temp, method = "pearson")
spearman_r<- cor(comparison_df$rk_temp, comparison_df$ref_temp, method = "spearman")
bias_C    <- mean(comparison_df$rk_temp - comparison_df$ref_temp)
mae_C     <- mean(abs(comparison_df$rk_temp - comparison_df$ref_temp))
rmse_C    <- sqrt(mean((comparison_df$rk_temp - comparison_df$ref_temp)^2))

# Combine metrics for output
metrics <- c(
  pearson_r = pearson_r,
  spearman_r = spearman_r,
  bias_C = bias_C,
  MAE_C = mae_C,
  RMSE_C = rmse_C
)
print(metrics)

# Sample data if very large (for faster plotting)
set.seed(1)
comparison_plot_df <- if (nrow(comparison_df) > 1e6) {
  comparison_df[sample.int(nrow(comparison_df), 1e6), ]
} else comparison_df

# Plot model vs. reference scatter with 1:1 line
ggplot(comparison_plot_df, aes(x = ref_temp, y = rk_temp)) +
  geom_hex(bins = 40) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  coord_equal() +
  labs(
    x = "Reference (5 m downsampled to 30 m mean) [°C]",
    y = "RK model (30 m) [°C]",
    title = sprintf("Agreement at 14:00 — r = %.2f, RMSE = %.2f °C", pearson_r, rmse_C)
  )
```

Short interpretation of the plot:
The RK model reproduces the relative spatial pattern of heat well (r ≈ 0.88) but is systematically too warm by about 8–9 °C across the city at 14:00.

```{r RK and Reference Temperature Map Visualization}

# Convert rasters to data frames for plotting  
mod_df  <- as.data.frame(mod_h,  xy = TRUE, na.rm = TRUE)
ref_df  <- as.data.frame(ref30_h, xy = TRUE, na.rm = TRUE)
names(mod_df)[3] <- "value"
names(ref_df)[3] <- "value"

# Compute shared color scale range  
mm_mod <- terra::minmax(mod_h)
mm_ref <- terra::minmax(ref30_h)
rng <- range(c(mm_mod, mm_ref), na.rm = TRUE)

# Convert vector layers to sf  
river_sf     <- st_as_sf(river)
hd_bounds_sf <- st_as_sf(hd_bounds)

# Plot RK model map  
p_rk_map <- ggplot() +
  geom_raster(data = mod_df, aes(x = x, y = y, fill = value)) +
  geom_sf(data = river_sf, color = "lightblue", linewidth = 0.4) +
  geom_sf(data = hd_bounds_sf, fill = NA, color = "black", linewidth = 0.4) +
  scale_fill_viridis_c(name = "Temperature [°C]", limits = rng) +
  coord_sf() + 
  annotation_scale(location = "br", width_hint = 0.3) +    
  annotation_north_arrow(location = "tl", which_north = "true", width = unit(0.9, "cm"),
                         style = north_arrow_orienteering) +
  labs(title = "RK model",
      subtitle = sprintf("Temperature value range: %.1f – %.1f °C", 
                         min(mod_df$value, na.rm = TRUE), max(mod_df$value, na.rm = TRUE))) +
  theme_minimal(base_size = 13) +
  theme(
    axis.title = element_blank(),
    axis.text  = element_blank(),
    panel.grid = element_blank(),
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 9, face = "plain", 
                                 margin = margin(t = 1, b = 5))
  )

# Plot reference map  
p_ref_map <- ggplot() +
  geom_raster(data = ref_df, aes(x = x, y = y, fill = value)) +
  geom_sf(data = river_sf, color = "lightblue", linewidth = 0.4) +
  geom_sf(data = hd_bounds_sf, fill = NA, color = "black", linewidth = 0.4) +
  scale_fill_viridis_c(name = "Temperature [°C]", limits = rng) +
  coord_sf() +
  annotation_scale(location = "br", width_hint = 0.3) +
  annotation_north_arrow(location = "tl", which_north = "true", width = unit(0.9, "cm"),
                         style = north_arrow_orienteering) +
  labs(title = "Reference model",
       subtitle = sprintf("Temperature value range: %.1f – %.1f °C", 
                          min(ref_df$value, na.rm = TRUE), max(ref_df$value, na.rm = TRUE))) +
  theme_minimal(base_size = 13) +
  theme(
    axis.title = element_blank(),
    axis.text  = element_blank(),
    panel.grid = element_blank(),
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 9, face = "plain", 
                                 margin = margin(t = 1, b = 5))
  )

# Combine both plots with shared legend  
p_rk_ref <- (p_rk_map + p_ref_map + 
                    plot_layout(guides = "collect") & 
                    theme(legend.position = "right")) +
  plot_annotation(
    title = "Comparison of RK and Reference Temperature Maps — Heidelberg, 09-07-2023 14:00",
    theme = theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5,
        margin = margin(b = 10)))
  )

# Display combined plot
p_rk_ref

# Save as PNG
ggsave("../plots/rk_vs_ref_14-00.png", p_rk_ref, width = 10, height = 5, dpi = 300)
```

Short explanation of the following chunk:
The following chunk is a quality-control checkpoint. It doesn’t change the analysis output, but it ensures that what I compare is valid (same units, same grid, no hidden bias).
Keep it as a safety check but might omit it from the final report.

```{r Residual diagnostics and consistency checks}

# Residual summary (model-reference)
resid <- comparison_df$rk_temp - comparison_df$ref_temp
summary(resid)
mean(resid)        # mean bias
median(resid)      # median bias
mean(resid < 0)    # share of negative residuals
mean(resid > 0)    # share of positive residuals

# Unit consistency check
summary(comparison_df$rk_temp)
summary(comparison_df$ref_temp)
# Values ~280–320 → Kelvin; ~10–45 → Celsius
# A ~273.15 offset would indicate mixed units

# Spatial alignment check (CRS, resolution, extent)
stopifnot(terra::same.crs(ref30, mod))
stopifnot(isTRUE(all.equal(terra::res(ref30),  terra::res(mod))))
stopifnot(isTRUE(all.equal(terra::ext(ref30),  terra::ext(mod))))
```

Short interpretation of the residual summary metrics:
The RK model systematically overestimates temperatures by about +8.7 °C across all grid cells, with no negative residuals. Both datasets are in °C and perfectly aligned spatially.

```{r Spatial Residual Map and Histogram}

# Residual histogram
p_resid_hist <- ggplot(data.frame(resid), aes(x = resid)) +
  geom_histogram(bins = 60, fill = "steelblue", color = "white", alpha = 0.8) +
  geom_vline(xintercept = 0, color = "black", linetype = "dashed", linewidth = 0.6) +
  labs(
    title = "Residual Distribution (RK Model – Reference, 09-07-2023 14:00)",
    subtitle = sprintf("Mean bias = %.2f °C", mean(resid)),
    x = "Residual (Model − Reference) [°C]",
    y = "Number of cells"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 10, margin = margin(b = 5)),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.major.y = element_line(color = "grey85"),
    axis.title = element_text(face = "bold")
  )

# Recalculate spatial residuals (per grid cell) to retain geographic structure 
resid_raster <- mod_h - ref30_h

# Convert residual raster to data frame for ggplot visualization
resid_map_df <- as.data.frame(resid_raster, xy = TRUE, na.rm = TRUE)
names(resid_map_df)[3] <- "residual"

# Define symmetric color scale limits around zero
rmax <- ceiling(max(abs(terra::global(resid_raster, "min", na.rm = TRUE)$min),
                    abs(terra::global(resid_raster, "max", na.rm = TRUE)$max)))
# round to nearest multiple of 5 for clean legend breaks
rmax <- ceiling(rmax / 5) * 5  

# Plot spatial residual distribution
p_resid_map <- ggplot() +
  geom_raster(data = resid_map_df, aes(x = x, y = y, fill = residual)) +
  geom_sf(data = river_sf, color = "lightblue", linewidth = 0.4) +
  geom_sf(data = hd_bounds_sf, fill = NA, color = "black", linewidth = 0.5) +
  scale_fill_gradient2(
    name = "Residual (°C)",
    low = "#2166ac", mid = "white", high = "#b2182b",
    midpoint = 0,
    limits = c(-rmax, rmax),
    breaks = seq(-15, 15, 5),
    labels = seq(-15, 15, 5)
  ) +
  coord_sf() +
  annotation_scale(location = "br", width_hint = 0.3) +    
  annotation_north_arrow(location = "tl", which_north = "true", width = unit(0.9, "cm"),
                         style = north_arrow_orienteering) +
  labs(
    title = "Spatial Distribution of Residuals (Model − Reference) on 09-07-2023 at 14:00 [°C]",
    subtitle = "Blue: Model colder than reference | Red: Model warmer than reference"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 9, margin = margin(b = 8)),
    legend.position = "right",
    legend.title = element_text(face = "bold"),
    legend.text = element_text(size = 9),
    axis.text = element_blank(),
    axis.title = element_blank(),
    panel.grid = element_blank()
  )

# Display the plots
p_resid_hist
p_resid_map

# Save as PNG
# ggsave("../plots/residual_histogram_rk_vs_reference_2023-07-09_14-00.png", p_resid_hist, width = 10, height = 5, dpi = 300)

# ggsave("../plots/residual_spatial_map_rk_vs_reference_2023-07-09_14-00.png", p_resid_map, width = 10, height = 5, dpi = 300)
```

```{r Robust Performance Metrics and Regression Fit}

# Robust central tendency
median_bias <- median(resid, na.rm = TRUE)

# Linear regression between reference and model temperatures
lm_rk_ref_temp <- lm(ref_temp ~ rk_temp, data = comparison_df)
summary(lm_rk_ref_temp)$coefficients 
summary(lm_rk_ref_temp)$r.squared    

# Print summary results
c(median_bias = median_bias, R2 = summary(lm_rk_ref_temp)$r.squared)
```

Short interpretation of the linear regression model results:
The analysis shows that the RK model reproduces the general spatial temperature pattern well (R² = 0.78) but consistently predicts higher absolute temperatures than the reference. With a median bias of about +8.7 °C (in this case the same as mean_bias) and a regression slope of 2.7, the results indicate that the RK model is overall too warm and exhibits a narrower temperature range, underrepresenting the spatial variability present in the reference data.

```{r Mean Temperature and Correlation by Landuse Class}

# Align raster CRS to land use polygons
mod_h   <- project(mod_h, landuse)
ref30_h <- project(ref30_h, landuse)

# Extract mean temperature per land use polygon
mod_vals <- terra::extract(mod_h, landuse, fun = mean, na.rm = TRUE)
ref_vals <- terra::extract(ref30_h, landuse, fun = mean, na.rm = TRUE)

# Add mean values to land use data
landuse$rk_mean  <- mod_vals[, 2]
landuse$ref_mean <- ref_vals[, 2]

# Calculate model–reference difference
landuse <- landuse %>%
  mutate(diff = rk_mean - ref_mean)

# Summarize by land use class
landuse_summary <- landuse %>%
  st_drop_geometry() %>%
  group_by(class_2018) %>%
  summarise(
    rk_mean   = mean(rk_mean, na.rm = TRUE),
    ref_mean  = mean(ref_mean, na.rm = TRUE),
    diff_mean = mean(diff, na.rm = TRUE),
    corr      = cor(rk_mean, ref_mean, use = "complete.obs")
  )

print(landuse_summary)
```

```{r Color Palette for CORINE Landcover}

# Define color palette for land cover classes based on CORINE
landcover_colors <- c(
  "Continuous Urban fabric (S.L. > 80%)" = "#a40000",
  "Discontinuous Dense Urban Fabric (S.L.: 50% - 80%)" = "#d73027",
  "Discontinuous Medium Density Urban Fabric (S.L.: 30% - 50%)" = "#f46d43",
  "Discontinuous Low Density Urban Fabric (S.L.: 10% - 30%)" = "#fdae61",
  "Discontinuous very Low Density Urban Fabric (S.L.: < 10%)" = "#fee090",
  "Isolated Structures" = "#ffffbf",
  "Industrial, commercial, public, military and private units" = "#bdbdbd",
  "Fast transit roads and associated land" = "#969696",
  "Other roads and associated land" = "#636363",
  "Railways and associated land" = "#252525",
  "Port areas" = "#878787",
  "Airports" = "#cccccc",
  "Mineral extraction and dump sites" = "#ffcc99",
  "Construction sites" = "#ffeda0",
  "Land without current use" = "#f0f0f0",
  "Green urban areas" = "#78c679",
  "Sports and leisure facilities" = "#41ab5d",
  "Arable land (annual crops)" = "#e6ab02",
  "Permanent crops" = "#fee391",
  "Pastures" = "#a1d99b",
  "Complex and mixed cultivation patterns" = "#c2e699",
  "Orchards" = "#addd8e",
  "Forests" = "#006837",
  "Herbaceous vegetation associations" = "#31a354",
  "Open spaces with little or no vegetation" = "#d9f0a3",
  "Wetlands" = "#80cdc1",
  "Water" = "#67a9cf"
)
```

```{r Correlation by land cover class}

# Summarise model–reference correlation by land cover class
cor_summary <- landuse %>%
  st_drop_geometry() %>%
  group_by(class_2018) %>%
  summarise(
    n = n(),
    corr = cor(rk_mean, ref_mean, use = "complete.obs"),
    rk_mean = mean(rk_mean, na.rm = TRUE),
    ref_mean = mean(ref_mean, na.rm = TRUE),
    diff_mean = mean(rk_mean - ref_mean, na.rm = TRUE)
  ) %>%
  arrange(desc(corr))

# remove classes without valid correlations
cor_summary <- cor_summary %>% filter(!is.na(corr))

# Plot correlation by land cover class
p_landcover_corr <- ggplot(cor_summary, aes(
  x = reorder(class_2018, corr), 
  y = corr,                      
  fill = class_2018               
)) +
  geom_col(width = 0.6, color = "grey30") +                         
  geom_text(aes(label = sprintf("%.2f", corr)),         
            hjust = -0.1, size = 3, color = "black") +
  scale_fill_manual(values = landcover_colors) +                
  coord_flip() +                                                    
  geom_hline(yintercept = 0, color = "grey50", linetype = "dashed") + 
  labs(
    title = "Temperature Correlation by Land Cover Class (09 July 2023, 14:00)",
    subtitle = "Pearson’s r between RK model and reference temperatures",
    caption = "Data: RK model output, Urban Climate Analysis Heidelberg (2023),\nUrban Atlas Land Cover/Land Use (2018), Europe, 6-yearly",
    x = NULL,
    y = "Pearson Correlation (r)"
  ) +
  theme_bw(base_size = 13) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 14), 
    plot.subtitle = element_text(hjust = 0.5, size = 10, margin = margin(b = 8)), 
    legend.position = "none",                                          
    axis.text.y = element_text(size = 9),
    panel.grid.minor = element_blank(),
    panel.grid.major.y = element_blank()
  )

# Display the plots
p_landcover_corr

# Save as PNG
# ggsave("../plots/landcover_correlation_2023-07-09_14-00.png", p_landcover_corr, width = 10, height = 5, dpi = 300)
```

--------------------------------------------------------------------------------

Comparison of the monthly mean temperature surface with the 
RK temperature prediction surface for hourly means

```{r Temporal variability analysis per pixel}

# Load monthly mean RK surface
mm_july_rk  <- terra::rast("../rk_monthly_mean/rk_predictions/2023_07_rk_prediction_map.tif")

# Directory with hourly RK predictions
rk_dir <- "../rk_results_07-09-2023/rk_predictions"

# List all hourly RK raster files for July 2023
rk_files <- list.files(rk_dir, pattern = "2023-07.*_rk_prediction_map\\.tif$", full.names = TRUE)

# Stack all hourly rasters
rk_stack <- rast(rk_files)

# Pixel-wise mean temperature (hourly average over July)
rk_mean_july <- app(rk_stack, fun = mean, na.rm = TRUE)

# Pixel-wise SD (temporal variability)
rk_sd_july <- app(rk_stack, fun = sd, na.rm = TRUE)

# Coefficient of variation (relative variability)
rk_cv_july <- rk_sd_july / rk_mean_july

# Plot SD map
plot(rk_sd_july, col = viridis(20),
     main = "Hourly RK Temperature Variability (SD, July 2023)",
     xlab = "", ylab = "")

# Plot Coefficient of variation map
plot(rk_cv_july, col = viridis(20),
     main = "Relative RK Temperature Variability (CV, July 2023)",
     xlab = "", ylab = "")

# Compare monthly mean, hourly mean, and hourly SD
plot(c(mm_july_rk, rk_mean_july, rk_sd_july),
     col = viridis(20),
     main = c("Monthly Mean RK (from monthly model)",
              "Hourly Mean RK (average of all hours)",
              "Hourly Variability (SD across hours)"))
```

```{r Residual variability relative to the monthly mean RK}

# Residuals = hourly RK minus monthly mean RK
res_stack <- rk_stack - mm_july_rk

# Pixel-wise SD of residuals (temporal variability)
res_var <- app(res_stack, fun = sd, na.rm = TRUE)

# Convert to data frame for plotting
res_df <- as.data.frame(res_var, xy = TRUE, na.rm = TRUE)
names(res_df)[3] <- "residual_sd"

# Plot spatial SD map
ggplot(res_df, aes(x, y, fill = residual_sd)) +
  geom_raster() +
  scale_fill_viridis_c(name = "SD (°C)") +
  coord_equal() +
  theme_minimal() +
  labs(title = "Residual Variability (Hourly RK vs Monthly Mean RK, July 2023)")

# Extract mean SD per landcover polygon
res_var_by_lc <- terra::extract(res_var, landuse, fun = mean, na.rm = TRUE)

# Rename value column
names(res_var_by_lc)[2] <- "mean_sd"

# Add landcover class names
res_var_by_lc$class_2018 <- landuse$class_2018

# Aggregate by class
res_var_summary <- res_var_by_lc %>%
  group_by(class_2018) %>%
  summarise(mean_sd = mean(mean_sd, na.rm = TRUE)) %>%
  arrange(desc(mean_sd))

# Plot mean SD by landcover class
ggplot(res_var_summary, aes(x = reorder(class_2018, mean_sd), y = mean_sd, fill = mean_sd)) +
  geom_col() +
  coord_flip() +
  scale_fill_viridis_c(name = "SD (°C)") +
  labs(
    title = "Residual Variability by Landcover Class",
    subtitle = "Standard deviation of hourly RK deviations from monthly RK mean (July 2023)",
    x = "Landcover Class (class_2018)",
    y = "Mean SD (°C)"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    axis.text.y = element_text(size = 9),
    legend.position = "right"
  )
```
