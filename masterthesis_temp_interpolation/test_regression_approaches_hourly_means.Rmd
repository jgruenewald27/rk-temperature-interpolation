---
title: "test_regression_approaches_hourly_means"
author: "Johannes Gruenewald"
date: "2025-04-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r Load Required Libraries}

# Spatial Data Handling & Processing
library(sf)
library(sp)
library(terra)
library(raster)
library(stars)
library(spdep)

# Spatial Analysis & Interpolation
library(gstat)
library(randomForest)
library(nlme)
library(automap)
library(mgcv)
library(lmtest)
library(sandwich)
library(broom)
library(relaimpo)

# Data Manipulation & Wrangling
library(tidyverse)

# Data Visualization
library(ggplot2)
library(scales)
library(gridExtra)
library(corrplot)

# Mapping & Interactive Visualization
library(tmap)

# Additional
library(lattice)
library(car)
```

--------------------------------------------------------------------------------

```{r Load temp data}

# Complete data import and pre-processing

# Read temperature data for the entire year of 2023
temp <- sf::read_sf('../data/temp_HD/sensor_data_year_2023.geojson')

# Removes rows where temp is NA
temp <- temp %>% filter(!is.na(temperature))

# Reproject the data for Germany
temp <- st_transform(temp, crs="EPSG:32632")

# Clean the data from outliers -> currently working with hard, manual breaks
# Currently nothing is removed by this step because no measurements are above 40°C for the year 2023
temp_clean <- temp %>% filter(temperature >= -35, temperature <= 40)

# Calculate hour count since epoch, rounded down to current hour
temp_clean$chour <- floor(as.numeric(temp_clean$dateobserved) / 3600)

# Drop all measurement stations which didn't collect data the entire year around
temp_clean_station <- temp_clean %>%
  mutate(dateobserved = as.POSIXct(dateobserved, tz = "UTC")) %>%
  filter(!entity_id %in% c(
    "hd:DE_Heidelberg_69120_12:WeatherObserved",
    "hd:DE_Heidelberg_69120_34:WeatherObserved",
    "hd:DE_Heidelberg_69123_41:WeatherObserved",
    "hd:DE_Heidelberg_46:WeatherObserved"
  ))

# Check if exactly 4 stations were dropped
length(unique(na.omit(temp_clean$entity_id)))
length(unique(na.omit(temp_clean_station$entity_id)))

# Calculate hourly mean temp for cleaned data
temp_clean_station_hm <- temp_clean_station %>%
    group_by(entity_id, chour) %>%
    summarise(hm_temp = mean(temperature, na.rm = TRUE), .groups = "drop") %>%
    mutate(hour = as.POSIXct(chour * 3600, origin = "1970-01-01", tz = "Europe/Berlin"))
```

--------------------------------------------------------------------------------

```{r Load and preprocess data}

# Import of pre-processed temp data and import and pre-processing of covariates 

# Import pre-processed hourly means, AOI and station coords
temp_clean_station_hm <- sf::read_sf('../data/temp_HD/temp_clean_station_hm.gpkg')
hd_bounds <- sf::read_sf('../data/AOI/OSM_boundaries_HD.geojson')
station_coords_sf <- sf::read_sf('../data/temp_HD/station_coords.gpkg')

# Drop Gaiberg station as it is outside of AOI
temp_clean_station_hm <- temp_clean_station_hm %>%
  filter(!entity_id %in% c(
    "hd:DE_Gaiberg_69251_21:WeatherObserved"
  ))

# Import the covariates
dem <- terra::rast("../data/DEM/hd_elevation_4326.tif")
tch <- terra::rast("../data/canopy_height/hd_canopy_height_4326.tif")
ghs_bh <- terra::rast("../data/building_height/hd_building_height_4326.tif")

# Mask out all values which are assigned to water and have value 101
# Not part of this area. But needs to be masked as well: 102 Snow/ice; 103 No data
tch[tch == 101] <- NA

# Reproject the data
temp_clean_station_hm <- st_transform(temp_clean_station_hm, crs="EPSG:32632")
hd_bounds <- st_transform(hd_bounds, crs="EPSG:32632")
station_coords_sf <- st_transform(station_coords_sf, crs="EPSG:32632")
dem <- terra::project(dem, y = crs(temp_clean_station_hm))
tch <- terra::project(tch, y = crs(temp_clean_station_hm))
ghs_bh <- terra::project(ghs_bh, y = crs(temp_clean_station_hm))

if (!identical(crs(tch), crs(dem))) {
  stop("CRS mismatch: Canopy height and DEM do not have the same coordinate reference system.")
}

if (!identical(crs(ghs_bh), crs(dem))) {
  stop("CRS mismatch: Building height and DEM do not have the same coordinate reference system.")
}

cat("All covariates share the same CRS.\n")

# Convert to SpatialPointsDataFrame
station_coords <- as(station_coords_sf, "Spatial")

# Create a Raster Stack for all covariate information

cat("Checking input raster resolutions (x by y):\n")
cat(sprintf("DEM:    %s\n", paste(terra::res(dem), collapse = " x ")))
cat(sprintf("TCH:    %s\n", paste(terra::res(tch), collapse = " x ")))
cat(sprintf("GHS_BH: %s\n\n", paste(terra::res(ghs_bh), collapse = " x ")))

# Align canopy height and building height to DEM resolution and extent
tch_aligned    <- terra::resample(tch, dem, method = "bilinear")
ghs_bh_aligned <- terra::resample(ghs_bh, dem, method = "near")

# Check alignment after resampling
if (!terra::compareGeom(dem, tch_aligned, ghs_bh_aligned, stopOnError = FALSE)) {
  stop("Covariate rasters are not perfectly aligned after resampling. Check extent/resolution/CRS.")
}

# Combine layers into a single SpatRaster stack
env_stack <- c(dem, tch_aligned, ghs_bh_aligned)

# Rename layers for clarity
names(env_stack) <- c("elevation", "canopy_height", "building_height")

# Code to include lat and lon information in raster stack

# Also add lat and lon to the raster stack
# Create coordinate rasters
lon <- terra::xFromCell(env_stack, 1:ncell(env_stack))
lat <- terra::yFromCell(env_stack, 1:ncell(env_stack))

# Convert to raster layers
lon_raster <- dem
lat_raster <- dem
values(lon_raster) <- lon
values(lat_raster) <- lat
names(lon_raster) <- "lon"
names(lat_raster) <- "lat"

# Stack the three raster layers
grids <- c(env_stack, lon_raster, lat_raster)

# Preparation of grid as a covariate for running RK
grids_ras <- stack(grids)

# Convert to SpatialGridDataFrame
grids_sp <- as(grids_ras, "SpatialGridDataFrame")

grids_sp@data
```

--------------------------------------------------------------------------------

```{r create_output_folder}

output_dir <- "../MLR_07_2023_12days"
if (!dir.exists(output_dir)) dir.create(output_dir)

# Define subdirectories for specific output types
lm_dir        <- file.path(output_dir, "lm_metrics")
lm_dir_z      <- file.path(output_dir, "lm_metrics_z")
variogram_dir <- file.path(output_dir, "variogram")

# Create subdirectories if they don't exist
dir.create(lm_dir, showWarnings = FALSE)
dir.create(lm_dir_z, showWarnings = FALSE)
dir.create(variogram_dir, showWarnings = FALSE)

# ================================================
# Datetime helpers (labels & filenames)
# ================================================
ts_to_posix <- function(ch) {
  if (inherits(ch, "POSIXt")) {
    # already a datetime — just return it
    return(ch)
  } else {
    # numeric hours → convert to POSIXct
    return(as.POSIXct(ch * 3600, origin = "1970-01-01", tz = tz_berlin))
  }
}
nice_dt     <- function(x) format(x, "%Y-%m-%d %H:00 %Z")                                   
file_dt     <- function(x) format(x, "%Y-%m-%d_%H00")    

# Define output file and remove any old version
metrics_file <- file.path(lm_dir, "lm_model_performance_summary.csv")
if (file.exists(metrics_file)) {
  file.remove(metrics_file)
  cat("Old metrics file removed — starting fresh.\n")
}

metrics_file_z <- file.path(lm_dir_z, "lm_z_model_performance_summary.csv")
if (file.exists(metrics_file_z)) {
  file.remove(metrics_file_z)
  cat("Old metrics file removed — starting fresh.\n")
}

# where to store per-covariate results from the z-MLR
coef_file_z <- file.path(lm_dir_z, "lm_z_coefficients_by_timestamp.csv")
```

--------------------------------------------------------------------------------

```{r Main loop}

# Set timestamps and analysis period
timestamps_all <- sort(unique(temp_clean_station_hm$hour))
start_date <- as.POSIXct("2023-07-01 00:00:00", tz = "Europe/Berlin")
end_date   <- as.POSIXct("2023-07-12 23:00:00", tz = "Europe/Berlin")
timestamps <- timestamps_all[timestamps_all >= start_date & timestamps_all <= end_date]
cat("Looping from:", min(timestamps), "to", max(timestamps), "\n")

# Counters and progress bar
total     <- length(timestamps)
completed <- 0L
skipped   <- 0L
pb <- txtProgressBar(min = 0, max = total, style = 3)
cat(sprintf("Processing %d timestamps…\n", total))

# Init empty containers
total_metrics <- data.frame()
all_temp_data <- data.frame()
mlr_residuals_all <- data.frame()

# Main loop over timestamps
for (i in seq_along(timestamps)) {
  ts <- timestamps[i]                                    
  dt_local <- ts_to_posix(ts)                     
  dt_lab   <- nice_dt(dt_local)                   
  dt_file  <- file_dt(dt_local)                   
  cat(sprintf("\n[%d/%d] Processing: hour=%s  (%s)\n", i, total, ts, dt_lab))  

  # ------------------------------------------------
  # STEP 1: Filter hourly data and plot temperature
  # ------------------------------------------------
  
  sel_temp_hm <- temp_clean_station_hm %>%
    filter(hour == ts)
  
  # Skip if fewer than 20 data points for current hour
  if (nrow(sel_temp_hm) < 20) {                                 
    skipped <- skipped + 1L                                       
    cat("  → skipped (n < 20)\n")                          
    next                                                         
  }
  
  # Append the selected temperature data to full dataset
  all_temp_data <- rbind(all_temp_data, sel_temp_hm)
  
  # Generate line plot showing hourly mean temperature for each station
  # across the time range covered so far in the loop
  p_all <- ggplot(all_temp_data, aes(x = hour, y = hm_temp, color = entity_id, group = entity_id)) +  
    geom_line() +
    geom_point(size = 1) +
    labs(title = "Hourly Mean Temperature at Each Station",        
         x = "Date & Hour (Europe/Berlin)", y = "Mean Temperature (°C)", color = "Station") + 
    scale_x_datetime(date_labels = "%d.%m\n%H:%M", date_breaks = "12 hours") +              
    theme_minimal(base_size = 12) +
    theme(axis.text.x = element_text(angle = 0, hjust = 0.5))                                  
  
  # Save the temperature plot
  ggsave(file.path(output_dir, "all_stations_hourly_mean_temperature.png"),
         plot = p_all, width = 10, height = 6, dpi = 300)

  # ------------------------------------------------
  # STEP 2: Add covariates (elev, canopy, bldg) from grid
  # ------------------------------------------------
  
  # Convert Spatial object to RasterBrick
  r_brick <- brick(grids_sp)
  
  # Convert RasterBrick to terra SpatRaster object
  grid_terra <- rast(r_brick)
  
  # Extract raster values at the point locations
  extracted_vals <- terra::extract(grid_terra, sel_temp_hm)
  
  # Remove the first column (ID)
  extracted_vals <- extracted_vals[,-1]
  
  # Combine the point data with the extracted raster values
  temp_shm.ex <- cbind(as.data.frame(sel_temp_hm), extracted_vals)
  
  # Choose which covariates to use (must match layer names from your raster)
  covars <- c("elevation", "canopy_height", "building_height")
  covars <- intersect(covars, names(temp_shm.ex))
  
  # Safe z-scaling within THIS timestamp (across stations present now)
  safe_z <- function(x) {
    m <- mean(x, na.rm = TRUE)
    s <- sd(x,   na.rm = TRUE)
    if (is.na(s) || s == 0) return(rep(0, length(x)))  
    (x - m) / s
  }
  
  # Add *_z columns
  for (nm in covars) {
    temp_shm.ex[[paste0(nm, "_z")]] <- safe_z(temp_shm.ex[[nm]])
  }
  
  # Keep the z parameters used THIS HOUR
  z_params_this <- data.frame(
    timestamp = ts,
    covariate = covars,
    mean      = sapply(temp_shm.ex[covars], function(v) mean(v, na.rm = TRUE)),
    sd        = sapply(temp_shm.ex[covars], function(v) sd(v,   na.rm = TRUE))
  )
  # Append to a csv if you like:
  write.table(z_params_this, file = file.path(lm_dir_z, "z_params_by_timestamp.csv"),
              sep = ",", row.names = FALSE,
              col.names = !file.exists(file.path(lm_dir_z,"z_params_by_timestamp.csv")),
              append = TRUE)
  
  # ------------------------------------------------
  # STEP 3: Fit linear model and prepare spatial data
  # ------------------------------------------------
  
  # Fit multiple linear regression model
  reg_model <- lm(hm_temp ~ elevation + canopy_height + building_height, data = temp_shm.ex)
  summary_lm <- summary(reg_model)
  
  # Compute residuals and predictions
  pred <- predict(reg_model, temp_shm.ex)
  resid <- temp_shm.ex$hm_temp - pred
  
  # Extract per-timestamp residuals
  mlr_residuals_df <- temp_shm.ex %>%
    dplyr::transmute(
      timestamp   = ts,
      entity_id   = entity_id,      # station id; adjust if your column is named differently
      hm_temp_obs = hm_temp,        # observed temperature
      hm_temp_pred = pred,          # predicted by regular MLR
      residual    = resid,          # obs - pred
      elevation   = elevation,
      canopy_height = canopy_height,
      building_height = building_height
    )
  
  # Accumulate
  if (!exists("mlr_residuals_all") || nrow(mlr_residuals_all) == 0) {
    mlr_residuals_all <- mlr_residuals_df
  } else {
    mlr_residuals_all <- dplyr::bind_rows(mlr_residuals_all, mlr_residuals_df)
  }
    
  # Extract metrics
  model_metrics <- data.frame(
    timestamp      = ts,
    r_squared      = summary_lm$r.squared,
    adj_r_squared  = summary_lm$adj.r.squared,
    f_statistic    = summary_lm$fstatistic[1],
    f_pvalue       = pf(summary_lm$fstatistic[1],
                        summary_lm$fstatistic[2],
                        summary_lm$fstatistic[3],
                        lower.tail = FALSE),
    residual_se    = summary_lm$sigma,
    rmse           = sqrt(mean(resid^2)),
    mae            = mean(abs(resid)),
    AIC            = AIC(reg_model),
    BIC            = BIC(reg_model)
  )
  
  write.table(
    model_metrics,
    file = metrics_file,
    sep = ",",
    row.names = FALSE,
    col.names = !file.exists(metrics_file),
    append = TRUE
  )

  # Save model's residuals back into original df as new column
  temp_shm.ex$residuals <- resid
  
  # Convert df into spatial object using existing geometry and
  # Convert MULTIPOINT geometries to POINT
  temp_shm.sf <- st_as_sf(temp_shm.ex) |> st_cast("POINT")
  
  # z-MLR: fit with standardized predictors
  reg_model_z  <- lm(hm_temp ~ elevation_z + canopy_height_z + building_height_z,
                     data = temp_shm.ex)
  summary_lm_z <- summary(reg_model_z)
  
  # predictions & residuals for z-MLR (distinct names to avoid clobbering)
  pred_z  <- predict(reg_model_z, temp_shm.ex)
  resid_z <- temp_shm.ex$hm_temp - pred_z
  
  # Model-level metrics for z-MLR
  model_metrics_z <- data.frame(
    timestamp      = ts,
    r_squared      = summary_lm_z$r.squared,
    adj_r_squared  = summary_lm_z$adj.r.squared,
    f_statistic    = summary_lm_z$fstatistic[1],
    f_pvalue       = pf(summary_lm_z$fstatistic[1],
                        summary_lm_z$fstatistic[2],
                        summary_lm_z$fstatistic[3],
                        lower.tail = FALSE),
    residual_se    = summary_lm_z$sigma,
    rmse           = sqrt(mean(resid_z^2)),
    mae            = mean(abs(resid_z)),
    AIC            = AIC(reg_model_z),
    BIC            = BIC(reg_model_z)
  )
  
  write.table(
    model_metrics_z,
    file = metrics_file_z,
    sep = ",",
    row.names = FALSE,
    col.names = !file.exists(metrics_file_z),
    append = TRUE
  )
  
  # ---- Your requested additions: robust SEs, β (unitless), relative importance ----
  
  # robust SEs (HC3) for safer p-values
  rob <- coeftest(reg_model_z, vcov = vcovHC(reg_model_z, type = "HC3"))
  
  coef_df <- broom::tidy(rob) |>
    dplyr::filter(term != "(Intercept)") |>
    dplyr::transmute(
      timestamp     = ts,
      term,
      estimate_semi = estimate,      # °C per 1 SD of predictor (since X are z)
      se_robust     = std.error,
      t_robust      = statistic,
      p_robust      = p.value
    )
  
  # unitless β for cross-time comparability
  y_sd <- sd(temp_shm.ex$hm_temp, na.rm = TRUE)
  coef_df$beta_std <- coef_df$estimate_semi / y_sd
  
  # relative importance (LMG): who explains most variance at this hour?
  imp <- calc.relimp(reg_model_z, type = "lmg", rela = TRUE)
  coef_df$lmg <- imp$lmg[match(coef_df$term, names(imp$lmg))]  # sums to 1 per timestamp
  
  # append to tidy CSV
  write.table(
    coef_df,
    file = coef_file_z,
    sep = ",",
    row.names = FALSE,
    col.names = !file.exists(coef_file_z),
    append = TRUE
  )

  # ------------------------------------------------
  # STEP 4: Fit GAM model and prepare spatial data
  # ------------------------------------------------
  
  # CONTINUE WORKING HERE
  
  
  # ------------------------------------------------
  # STEP 5: Compute anisotropy & fit directional variogram
  # ------------------------------------------------
  
  # Compute empirical variogram map of model residuals
  # This helps visualize spatial autocorrelation in all directions
  varmap <- variogram(residuals ~ 1, data = temp_shm.sf, map = TRUE,
                    # cutoff is half the spatial extent (or diameter) of the study area
                    cutoff = sqrt(areaSpatialGrid(grids_sp)) / 2, 
                    # scaled relative to the resolution of the grid
                    width = 30 * grids_sp@grid@cellsize[1])

  # Store cutoff and bin width values for reuse and labeling
  cutoff_val <- sqrt(areaSpatialGrid(grids_sp)) / 2
  width_val  <- 30 * grids_sp@grid@cellsize[1]
  
  # Report how many rows are in variogram map
  cat("Variogram map has", nrow(as.data.frame(varmap)), "rows\n")
  
  # Define output filename for the variogram map plot
  file_varmap <- file.path(variogram_dir, paste0(dt_file, "_variogram_map.png"))  
  
  # Plot and save the directional variogram map
  png(file_varmap, width = 8, height = 6, units = "in", res = 300)
  print(
    plot(varmap,
         col.regions = grey(rev(seq(0, 1, 0.025))),
         main = sprintf("Directional Variogram Map (cutoff: %.1f km, width: %.1f km)\n%s",  
                        cutoff_val / 1000, width_val / 1000, dt_lab))
  )
  dev.off()
  
  # Compute directional variograms for North–South (0°) and East–West (90°)
  rv.temp <- variogram(residuals ~ 1, temp_shm.sf, alpha = c(0, 45, 90, 135))

  # Fit theoretical exponential variogram model to the directional variogram
  rvgm.temp <- fit.variogram(rv.temp,
      vgm(psill=var(temp_shm.sf$residuals),
          "Exp", nugget=0, anis= c(p=90, s=0.5))) # p = direction, s = anisotropy ratio
  
  # Define output filename for directional variogram fit plot
  file_varfit <- file.path(variogram_dir, paste0(dt_file, "_directional_variogram_fit.png"))  
 
  # Plot the empirical and fitted directional variograms and save
  png(file_varfit, width = 8, height = 6, units = "in", res = 300)
  print(
    plot(rv.temp, rvgm.temp,
         main = paste0("Directional Variogram Fit (0°, 45°, 90°, 135°)\n", dt_lab),   
         plot.nu = FALSE, cex = 1.5, pch = "+", col = "black")
  )
  dev.off()
  
  # ------------------------------------------------
  # STEP 6: Progress bar
  # ------------------------------------------------  
  
  completed <- completed + 1L 
  setTxtProgressBar(pb, completed)
  cat("\n")
  cat(sprintf("  → done. Completed: %d/%d (skipped: %d)\n",      
              completed, total, skipped))                        
}

# Close progress bar + final summary
close(pb)                                                        
cat(sprintf("\nFinished. Completed: %d/%d | Skipped: %d\n",       
            completed, total, skipped))                         

mlr_residuals_file <- file.path(output_dir, "mlr_residuals_all.csv")
if (exists("mlr_residuals_all") && nrow(mlr_residuals_all) > 0) {
  mlr_residuals_all$datetime_local <- ts_to_posix(mlr_residuals_all$timestamp)
  mlr_residuals_all <- dplyr::relocate(mlr_residuals_all, datetime_local, .after = timestamp)
  write.csv(mlr_residuals_all, mlr_residuals_file, row.names = FALSE)
}
```

--------------------------------------------------------------------------------

Plot the outputs

```{r Analyse LM metrics}

# Read CSV with lm_metrics
lm_metrics <- read.csv("../regression_tests/lm_metrics/lm_model_performance_summary.csv")

lm_metrics <- lm_metrics %>%
  mutate(
    datetime_calc = as.POSIXct(timestamp * 3600, origin = "1970-01-01", tz = "Europe/Berlin"),
    date = as.Date(datetime_calc),
    hour = hour(datetime_calc)
  )

# Reshape to long format
lm_metrics_long <- lm_metrics %>%
  pivot_longer(cols = c(r_squared, adj_r_squared),
               names_to = "metric",
               values_to = "value")

# Filter year 2023
lm_df_plot <- lm_metrics_long %>%
  filter(year(date) == 2023)

# Plot: Daily Evaluation of Multiple Linear Regression Model Performance
p_lm <- ggplot(lm_df_plot, aes(x = hour, y = value, color = metric, group = metric)) +
  # Lines and points for model metrics
  geom_line(linewidth = 1.2) +
  geom_hline(
    aes(yintercept = 1, linetype = "Perfect Fit (R² = 1)"),
    color = "grey60"
  ) +
  geom_point(size = 1.8) +
  # Facet by date
  facet_wrap(~ date, ncol = 3, scales = "fixed") +
  # Color and linetype scales
  scale_color_manual(
    values = c(
      "r_squared"     = "#6699CC",
      "adj_r_squared" = "#CC6677"
    ),
    labels = c(
      "r_squared"     = expression(R^2),
      "adj_r_squared" = expression(Adjusted~R^2)
    )
  ) +
  scale_linetype_manual(
    name   = "",
    values = c("Perfect Fit (R² = 1)" = "dotted")
  ) +
  # Labels
  labs(
    title    = "Daily Evaluation of Multiple Linear Regression Model Performance",
    subtitle = "Hourly R² and Adjusted R² values from models predicting hourly mean temperature",
    x        = "Hour of the Day",
    y        = "Metric Value",
    color    = "Metrics"
  ) +
  # X-axis ticks
  scale_x_continuous(breaks = c(0, 6, 12, 18, 24)) +
  # Theme
  theme_minimal(base_size = 14) +
  theme(
    legend.position    = "top",
    legend.box         = "horizontal",
    panel.grid.minor   = element_blank(),
    panel.spacing      = unit(0.8, "lines"),
    strip.background   = element_rect(fill = "grey90", color = NA),
    strip.text         = element_text(face = "bold")
  )

# Show plot
p_lm

# Save plot
#ggsave("../regression_tests/lm_metrics_r2_adjr2_by_day_04_2023.png", plot = p_lm, width = 12, height = 8, dpi = 300)
```


```{r}
# Read CSV with lm_metrics
lm_metrics <- read.csv("../regression_tests/lm_metrics/lm_model_performance_summary.csv")

lm_metrics <- lm_metrics %>%
  mutate(
    datetime_calc = as.POSIXct(timestamp * 3600, origin = "1970-01-01", tz = "Europe/Berlin"),
    date = as.Date(datetime_calc),
    hour = hour(datetime_calc)
  )

# Reshape to long format for RMSE and MAE
lm_metrics_long <- lm_metrics %>%
  pivot_longer(
    cols = c(rmse, mae),
    names_to = "metric",
    values_to = "value"
  )

# Filter for year 2023
lm_df_plot <- lm_metrics_long %>%
  filter(year(date) == 2023)

# Plot: Daily Evaluation of Multiple Linear Regression Model Performance (RMSE & MAE)
p_lm <- ggplot(lm_df_plot, aes(x = hour, y = value, color = metric, group = metric)) +
  geom_line(linewidth = 1.2) +
  geom_point(size = 1.8) +
  # Facet by date
  facet_wrap(~ date, ncol = 3, scales = "fixed") +
  # Color scale for RMSE and MAE
  scale_color_manual(
    values = c(
      "rmse" = "#6699CC",
      "mae"  = "#CC6677"
    ),
    labels = c(
      "rmse" = "RMSE",
      "mae"  = "MAE"
    )
  ) +
  # Labels
  labs(
    title    = "Daily Evaluation of Multiple Linear Regression Model Performance",
    subtitle = "Hourly RMSE and MAE values from models predicting hourly mean temperature",
    x        = "Hour of the Day",
    y        = "Metric Value",
    color    = "Metrics"
  ) +
  # X-axis ticks
  scale_x_continuous(breaks = c(0, 6, 12, 18, 24)) +
  # Theme
  theme_minimal(base_size = 14) +
  theme(
    legend.position    = "top",
    legend.box         = "horizontal",
    panel.grid.minor   = element_blank(),
    panel.spacing      = unit(0.8, "lines"),
    strip.background   = element_rect(fill = "grey90", color = NA),
    strip.text         = element_text(face = "bold")
  )

# Show plot
p_lm

# Save plot (optional)
#ggsave("../regression_tests/lm_metrics_rmse_mae_by_day_04_2023.png", plot = p_lm, width = 12, height = 8, dpi = 300)
```

--------------------------------------------------------------------------------

How well do the environmental covariates describe hourly mean temperature

Following plot can answer this research question:
- [ ] Wie gut beschreiben die covariates die hourly mean temperature zu bestimmten timestamps 
- [ ] Was sind mögliche Erklärungen für diese patterns? Welche Prozesse können zu diesen patterns führen?

```{r}
# Read and prepare data
coef_all <- read.csv(coef_file_z)
fit <- read.csv(metrics_file_z)

fit <- fit %>%
  mutate(
    timestamp = if_else(
      nchar(timestamp) == 10, paste0(timestamp, " 00:00:00"), timestamp
    ),
    timestamp = as.POSIXct(timestamp, format = "%Y-%m-%d %H:%M:%S", tz = "Europe/Berlin"),
    month = month(timestamp),
    year = year(timestamp)
  ) %>%
  filter(year == 2023, month == 7)

coef_all <- coef_all %>%
  mutate(
    timestamp = if_else(
      nchar(timestamp) == 10,
      paste0(timestamp, " 00:00:00"),
      timestamp
    ),
    timestamp = as.POSIXct(timestamp, format = "%Y-%m-%d %H:%M:%S", tz = "Europe/Berlin"),
    date = as.Date(format(timestamp, tz = "Europe/Berlin")),
    hour  = hour(timestamp),
    month = month(timestamp),
    year  = year(timestamp)
  ) %>%
  filter(month == 7 & year == 2023) %>%
  left_join(fit %>% select(timestamp, r_squared), by = "timestamp")

# Determine pagination structure
days_per_page <- 12
n_pages <- ceiling(length(unique(coef_all$date)) / days_per_page)

# Output directory
output_plots <- "../plots/figures_covariate_importance"
if (!dir.exists(output_dir)) dir.create(output_dir)

# Paginated plotting loop
for (i in seq_len(n_pages)) {
  p_cov <- ggplot(coef_all, aes(x = hour)) +
    geom_area(aes(y = lmg, fill = term), position = "fill", alpha = 0.85) +
    geom_line(aes(y = r_squared, color = "Model R²"), linewidth = 0.9) +
    ggforce::facet_wrap_paginate(~ date, ncol = 3, nrow = 4, page = i) +
    scale_x_continuous(breaks = seq(0, 24, 6), limits = c(0, 24)) +
    scale_y_continuous(
      breaks = c(0, 0.25, 0.50, 0.75, 1.00),
      labels = scales::percent_format(accuracy = 1),
      sec.axis = sec_axis(~ ., name = "Model R²")
    ) +
    theme(
      panel.grid = element_blank(),
      panel.grid.major.y = element_line(color = "grey85")
    ) +
    scale_fill_manual(
      name = "Covariates (z-standardised)",
      values = c(
        "building_height_z" = "darkorange3",
        "canopy_height_z"   = "darkolivegreen",
        "elevation_z"       = "burlywood2"
      ),
      labels = c(
        "building_height_z" = "Building Height",
        "canopy_height_z"   = "Canopy Height",
        "elevation_z"       = "Elevation"
      )
    ) +
    scale_color_manual(
      name = "",  # we'll merge this with the fill legend
      values = c("Model R²" = "black"),
      guide = guide_legend(override.aes = list(linetype = 1, fill = NA))
    ) +
    guides(
      fill = guide_legend(order = 1),
      color = guide_legend(order = 2)
    ) +
    labs(
      title    = sprintf("Daily Variation of LMG-Based Covariate Importance\nin Hourly Multiple Linear Regression Models"),
      caption = "LMG = Lindeman–Merenda–Gold relative importance measure.  
Each hour was modelled using a separate multiple linear regression with z-standardised covariates. 
Areas show the proportional contribution of each covariate to the total model R² for that hour.",
      x = "Hour of the Day",
      y = "Share of R² (LMG, Sum = 1)"
    ) +
    theme_minimal(base_size = 14) +
    theme(
      legend.position  = "bottom",
      legend.title     = element_text(face = "bold"),
      legend.key.width = unit(2, "lines"),
      strip.text       = element_text(face = "bold", size = 12),
      panel.spacing    = unit(0.8, "lines"),
      plot.title       = element_text(face = "bold", size = 20, hjust = 0.5, 
                                      margin = margin(t = 10, b = 10)),
      plot.subtitle    = element_text(size = 12, hjust = 0.5)
    ) +
    theme(
      plot.caption.position = "plot", 
      plot.caption = element_text(hjust = 0, margin = margin(t = 20))
    ) +
    theme(
      panel.grid.major.x = element_line(color = "grey90"),  
      panel.grid.major.y = element_line(color = "grey90"), 
      panel.grid.minor = element_blank()                   
    )

  # Save each page
  ggsave(
    filename = file.path(output_plots, sprintf("covariate_importance.png")),
    plot = p_cov,
    width = 12, height = 10, dpi = 300
  )
}
```

The figure illustrates how the relative influence of topography, vegetation, and urban structure on hourly mean temperature varies over the course of each day.

Overall Canopy height has the smallest impact of the three covariates on the hourly mean temperature predictions. Across the entire day, elevation has the biggest impact with building height having a strong influence on hourly mean temperature during nighttime.

The following figure will answer this research questions:
- [ ] Bei welchen timestamps sind die covariates besonders signifikant? Gibt es da bestimmte patterns bei bestimmten klimatischen Bedingungen oder täglich widerkehrende Regelmäßigkeiten?

```{r}
# Output directory for saving figures
output_pval <- "../plots/figures_pvalues"
if (!dir.exists(output_dir)) dir.create(output_dir)

# Categorize p-values before plotting
coef_all <- coef_all %>%
  mutate(
    p_category = case_when(
      p_robust < 0.001 ~ "< 0.001",
      p_robust < 0.01  ~ "< 0.01",
      p_robust < 0.05  ~ "< 0.05",
      TRUE             ~ "≥ 0.05 (n.s.)"
    )
  )

coef_all <- coef_all %>%
  left_join(moran_summary, by = c("date", "hour"))

# One row per (date, hour), keeping only hours with significant Moran's I
moran_band <- coef_all %>%
  dplyr::distinct(date, hour, moran_sig.y) %>%
  dplyr::filter(moran_sig.y)   # TRUE = significant spatial autocorrelation


for (i in seq_len(n_pages)) {
  p_pval <- ggplot(coef_all, aes(x = hour, y = term, fill = p_category)) +
    # PLOT CONTENT
    geom_tile(color = "grey85") +
    geom_rect(
      data = moran_band,
      aes(
        xmin = hour - 0.5, xmax = hour + 0.5,
        ymin = -Inf, ymax = Inf,
        color = "Moran’s I (p < 0.05)"
      ),
      inherit.aes = FALSE,
      fill = NA,
      linewidth = 0.4,
      alpha = 0.4
    ) +
    ggforce::facet_wrap_paginate(~ date, ncol = 3, nrow = 4, page = i) +
    scale_y_discrete(labels = c(
      "building_height_z" = "Building Height",
      "canopy_height_z"   = "Canopy Height",
      "elevation_z"       = "Elevation"
    )) +
    scale_x_continuous(breaks = seq(0, 24, 6), limits = c(0, 24)) +
    scale_fill_manual(
      name = "Significance (robust p-value)",
      values = c(
        "< 0.001" = "#440154",
        "< 0.01"  = "#31688e",
        "< 0.05"  = "#35b779",
        "≥ 0.05 (n.s.)" = "grey85"
      )
    ) +
    scale_color_manual(
      name = "",
      values = c("Moran’s I (p < 0.05)" = "firebrick3")
    ) +
    labs(
      x = "Hour of the Day",
      y = "Covariates",
      title = "Hourly Robust Significance of Model Covariates",
      caption = "Robust p-values are based on hourly multiple linear regression models with z-standardised covariates.
Darker colors represent higher statistical significance. 
Red outlines highlight hours where regression residuals show significant spatial autocorrelation (Moran’s I, p < 0.05)."
    ) +
    theme_minimal(base_size = 14) +
    theme(
      plot.title = element_text(
        face = "bold", size = 20, hjust = 0.5,
        margin = margin(t = 10, b = 10)
      ),
      plot.subtitle = element_text(size = 12, hjust = 0.5),
      plot.caption.position = "plot",
      plot.caption = element_text(hjust = 0, margin = margin(t = 20)),
      strip.text = element_text(face = "bold", size = 12),
      legend.position = "bottom",
      legend.title = element_text(face = "bold"),
      legend.key.width = unit(2, "lines"),
      legend.text = element_text(size = 11),
      legend.box = "horizontal",
      panel.spacing = unit(0.8, "lines"),
      panel.grid = element_blank(),
      panel.grid.major.x = element_line(color = "grey90"),
      panel.grid.major.y = element_line(color = "grey90"),
      panel.grid.minor = element_blank()
    ) +
    guides(
      fill = guide_legend(order = 1),
      color = guide_legend(order = 2,
                           override.aes = list(fill = NA, linewidth = 0.9, size = 5))
    )


  # Save each page
  ggsave(
    filename = file.path(output_pval, sprintf("covariate_pvalues.png")),
    plot = p_pval,
    width = 12, height = 10, dpi = 300
  )
}
```

--------------------------------------------------------------------------------

Adding covariates to temperature data

```{r}
# Extract raster values at the point locations
extracted_vals <- terra::extract(grids, temp_hm)

# Remove the first column (ID)
extracted_vals <- extracted_vals[,-1]

# Combine the point data with the extracted raster values
temp_hm.ex <- cbind(as.data.frame(temp_hm), extracted_vals)

temp_hm.ex
```

Old version: Extract values from all the raster and add to temperature data at point locations

```{r}
# Extract raster values at the temp locations
elevation_values <- terra::extract(dem, temp_dm)

# Extract raster values at the temp locations
building_height_values <- terra::extract(ghs_bh, temp_dm)

# Extract tree canopy height values at the temp locations
canopy_heigth_values <- terra::extract(tch, temp_dm)

# Use dplyr to add the extracted values as a new column in the sf object
temp_dm <- temp_dm %>%
  mutate(elevation = elevation_values[[2]])

# Use dplyr to add the extracted values as a new column in the sf object
temp_dm <- temp_dm %>%
  mutate(building_height = building_height_values[[2]])

# Use dplyr to add the extracted values as a new column in the sf object
temp_dm <- temp_dm %>%
  mutate(tree_canopy_height = canopy_heigth_values[[2]])

# Check the updated sf object
print(temp_dm)
```

--------------------------------------------------------------------------------

Descriptive statistics for each station

```{r}
# Compute descriptive statistics for each station
station_stats <- temp_hm.ex %>%
  group_by(stationname) %>%
  summarise(
    n_obs        = n(),
    temp_mean    = mean(mh_temp, na.rm = TRUE),
    temp_sd      = sd(mh_temp, na.rm = TRUE),
    temp_min     = min(mh_temp, na.rm = TRUE),
    temp_max     = max(mh_temp, na.rm = TRUE),
    chour_min    = min(chour, na.rm = TRUE),
    chour_max    = max(chour, na.rm = TRUE)
  )

# View first few rows
print(station_stats)
```

Stations that didn't measure the entire time or had missing measurements:

Likely missing values:
- 69115_Czernyring_22/11-12	726
- 69117_Königstuhl_10	724
- 69117_Rodelweg	708
- 69118_Am_Büchsenackerhang_39	714
- 69118_B37	711
- 69118_Wendestelle_Hbw	711
- 69123_Gewannn_Die_Äußeren_Rauschen	724
- 69123_Grenzhöfer_Weg	711	
- 69251_In_den_Weinäckern	703

Started later:
- 69115_Poststraße_15	448	
- 69123_Richard-Drach-Straße_7	87

--------------------------------------------------------------------------------

Fit Linear Regression model as a test

CONTINUE UNDERSTANDING THIS PART AND FURTHER INVESTIGATE HOW HOUR OF THE DAY INFO CAN BE INCLUDED AS A COVARIATE
IF NOTHING WORK -> CONTINUE WITH HENGL 2009 AND FURTHER APPLY HIS PROCESS AND TRY TO UNDERSTAND THE CONNECTION BETWEEN 
THE REGRESSION PART AND THE KRIGING OF THE RESIDUALS

```{r}
lm.temp_hm <- lm(mh_temp ~ elev + canopy + bldg + lon + lat + chour,
                data = temp_hm.ex)

summary(lm.temp_hm)
```

--------------------------------------------------------------------------------

Test Random Forest (RF) instead of regression models to model complex relationships and see if the performance is similar to flexible regression models.

Splitting the data in train and test data

```{r}
nrow(temp_dm)

# Splitting data in train and test data
split_dm <- sample.split(temp_dm$daily_mean_temperature, SplitRatio = 0.7)
length(split_dm)

train_dm <- subset(temp_dm, split_dm == "TRUE")
test_dm <- subset(temp_dm, split_dm == "FALSE")
```

Fit the Random Forest Model

```{r}
#make this example reproducible
set.seed(10)

#fit the random forest model
rf_model_dm <- randomForest(
  formula = daily_mean_temperature ~ elevation + tree_canopy_height + building_height,
  data = train_dm,
  mtry = 3,
  importance = TRUE,
  na.action = na.omit
)

rf_model_dm
importance(rf_model_dm)

#find number of trees that produce lowest test MSE
which.min(rf_model_dm$mse)

#find RMSE of best model
sqrt(rf_model_dm$mse[which.min(rf_model_dm$mse)]) 

#plot the test MSE by number of trees
plot(rf_model_dm)

#Importance plot
importance(rf_model_dm)

#produce variable importance plot
varImpPlot(rf_model_dm) 
```

Tune the model
This function produces the following plot, which displays the number of predictors used at each split when building the trees on the x-axis and the out-of-bag estimated error on the y-axis:

```{r}
model_tuned_dm <- tuneRF(
  x = data.frame(
    elevation = train_dm$elevation,
    building_height = train_dm$building_height,
    tree_canopy_height = train_dm$tree_canopy_height
  ),
  y = train_dm$daily_mean_temperature,  # define response variable
  ntreeTry = 500,
  mtryStart = 3,
  stepFactor = 1.5,
  improve = 0.01,
  trace = FALSE  # don't show real-time progress
)
```

We can see that the lowest OOB error is achieved by using 3 randomly chosen predictors at each split when building the trees.

Use the Final Model to Make Predictions and derive Residuals

```{r}
# Predict on training data
predicted_dm_rf <- predict(rf_model_dm, newdata=test_dm)

# Calculate residuals: observed - predicted
residuals_dm_rf <- temp_dm$daily_mean_temperature - predicted_dm_rf

plot(residuals_dm_rf)
```

--------------------------------------------------------------------------------





Histogram to check data distribution

```{r}
# Set up a 1-row, 3-column plotting area
par(mfrow = c(1, 3))

# Plot the histograms
hist(july_mean_temp$elevation, main = "Elevation", xlab = "Elevation (m)", col = "lightblue")
hist(july_mean_temp$tree_canopy_height, main = "Tree Canopy Height", xlab = "Height (m)", col = "lightgreen")
hist(july_mean_temp$building_height, main = "Building Height", xlab = "Height (m)", col = "lightcoral")
```

log-transformed the data to check possible changes to distribution

```{r}
# Set up a 1-row, 3-column plotting area
par(mfrow = c(1, 3))

# Plot the log-transformed histograms
hist(log(july_mean_temp$elevation + 1), 
     main = "Log(Elevation + 1)", 
     xlab = "log(Elevation)", 
     col = "lightblue")

hist(log(july_mean_temp$tree_canopy_height + 1), 
     main = "Log(Tree Canopy Height + 1)", 
     xlab = "log(Height)", 
     col = "lightgreen")

hist(log(july_mean_temp$building_height + 1), 
     main = "Log(Building Height + 1)", 
     xlab = "log(Height)", 
     col = "lightcoral")
```

!!Try regression models!!

Stepwise MLR

```{r}
# stepwise variable selection
model.MLR.step <- step(model, direction="both")

# summary of the new model using stepwise covariates selection
summary(model.MLR.step)

install.packages("gtsummary")

model.MLR.step |> tbl_regression()
```

Interpretation of MLR performance

```{r}
# graphical diagnosis of the regression analysis
par(mfrow=c(2,2))
plot(model.MLR.step)
par(mfrow=c(1,1))
```

GLS model

```{r}
# Extract coordinates (assuming `geom` is the column with spatial data)
july_mean_temp$coords <- st_coordinates(july_mean_temp$geom)

# Separate the coordinates into `x` and `y` columns
july_mean_temp$x <- july_mean_temp$coords[, 1]
july_mean_temp$y <- july_mean_temp$coords[, 2]

gls_model <- gls(mean_temperature ~ elevation + tree_canopy_height + building_height,
                 data = july_mean_temp,
                 correlation = corExp(form = ~ x + y, nugget = TRUE))

summary(gls_model)
```

Test RF instead of regression model to model complex relationships and see if the performance is similar

Try RF

```{r}
# 2. Fit Random Forest model
rf_model <- randomForest(
  mean_temperature ~ elevation + tree_canopy_height + building_height,
  data = july_mean_temp,
  ntree = 500,
  importance = TRUE,
  na.action = na.omit
)

# View basic model summary
print(rf_model)

# View variable importance
importance(rf_model)
varImpPlot(rf_model)
```

```{r}
# 3. Predict and calculate residuals
july_mean_temp$rf_pred <- predict(rf_model)
july_mean_temp$residuals <- july_mean_temp$mean_temperature - july_mean_temp$rf_pred
```


```{r}
# 4. Fit variogram to residuals
fitted <- autofitVariogram(residuals ~ 1, input_data = july_mean_temp)
fitted_vgm <- fitted$var_model
plot(fitted$exp_var, fitted$var_model)
```

Create prediction locations for the kriging process including the covariates

```{r}
# Convert raster to points, including values, and remove NAs
predLocations <- terra::as.points(dem, values=TRUE, na.rm=TRUE)

# Reproject the elevation data to match correct CRS
predLocations <- terra::project(predLocations, y = crs(july_mean_temp))

# Check CRS of locations
crs(predLocations)

# Convert the points to a SpatialPointsDataFrame
predLocations <- as(predLocations, "Spatial")

# Convert sf object to sp object
july_mean_temp_sp <- as(july_mean_temp, "Spatial")

# Set CRS of predLocations_sp to match july_mean_temp
sp::proj4string(predLocations) == sp::proj4string(july_mean_temp_sp)

# Display the updated gridded points
class(predLocations)
```

```{r}
# 6. Interpolate residuals by kriging
kriged_resid <- krige(residuals ~ 1,
                      loc = july_mean_temp_sp,
                      newdata = predLocations,
                      model = fitted_vgm)
```

```{r}
# 10. Model performance metrics on training data
obs <- july_mean_temp$mean_temperature
pred_rf <- july_mean_temp$rf_pred
resid <- july_mean_temp$residuals

# R², RMSE, MAE
r2_rf <- 1 - sum((obs - pred_rf)^2) / sum((obs - mean(obs))^2)
rmse_rf <- sqrt(mean((obs - pred_rf)^2))
mae_rf <- mean(abs(obs - pred_rf))

cat("Random Forest Performance:\n")
cat("R² =", round(r2_rf, 3), "\n")
cat("RMSE =", round(rmse_rf, 3), "\n")
cat("MAE =", round(mae_rf, 3), "\n\n")

# 11. Moran's I on residuals (check spatial autocorrelation)
coords_mat <- coordinates(july_mean_temp_sp)
nb <- knn2nb(knearneigh(coords_mat, k = 4))
lw <- nb2listw(nb)
moran_result <- moran.test(resid, lw)
print(moran_result)

# 12. Cross-validation of kriging residuals
kr_cv <- krige.cv(residuals ~ 1, locations = july_mean_temp_sp, model = fitted_vgm)
cat("Kriging Cross-Validation:\n")
summary(kr_cv$residual)
```

