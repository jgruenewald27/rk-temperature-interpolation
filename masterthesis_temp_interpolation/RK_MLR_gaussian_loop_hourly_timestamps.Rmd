---
title: "RK_MLR_gaussian_loop_hourly_timestamps"
output: html_document
date: "2025-12-01"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Load Required Libraries}

# Spatial Data Handling & Processing
library(sf)
library(sp)
library(terra)
library(raster)
library(stars)
library(spdep)

# Spatial Analysis & Interpolation
library(gstat)
library(randomForest)
library(nlme)
library(automap)
library(mgcv)
library(lmtest)
library(sandwich)
library(broom)
library(relaimpo)

# Data Manipulation & Wrangling
library(tidyverse)

# Data Visualization
library(ggplot2)
library(scales)
library(gridExtra)
library(corrplot)

# Mapping & Interactive Visualization
library(tmap)

# Additional
library(lattice)
library(car)
```

--------------------------------------------------------------------------------

```{r Load temp data}

# Complete data import and pre-processing of the temperature (July 2023)

# Read temperature data for the entire year of 2023
temp <- sf::read_sf('../data/temp_HD/sensor_data_20230107_31days.gpkg')

# Removes rows where temp is NA
temp <- temp %>% filter(!is.na(temperature))

# Reproject the data for Germany
temp <- st_transform(temp, crs="EPSG:32632")

# Clean the data from outliers -> currently working with hard, manual breaks
# Currently nothing is removed by this step because no measurements are above 
# 40°C for the year 2023
temp_clean <- temp %>% filter(temperature >= -35, temperature <= 40)

# Drop all measurement stations which didn't collect data the entire year around
temp_clean_station <- temp_clean %>%
  mutate(dateobserved = as.POSIXct(dateobserved, tz = "Europe/Berlin")) %>%
  filter(!entity_id %in% c(
    "hd:DE_Heidelberg_69120_12:WeatherObserved",
    "hd:DE_Heidelberg_69120_34:WeatherObserved",
    "hd:DE_Heidelberg_69123_41:WeatherObserved",
    "hd:DE_Gaiberg_69251_21:WeatherObserved",
    "hd:DE_Heidelberg_46:WeatherObserved"
  ))

# Check if exactly 5 stations were dropped
cat("Number of stations BEFORE cleaning: ",
    length(unique(na.omit(temp_clean$entity_id))), "\n")

cat("Number of stations AFTER cleaning:  ",
    length(unique(na.omit(temp_clean_station$entity_id))), "\n")

# Calculate hourly mean temp for cleaned data
temp_clean_station_hm <- temp_clean_station %>%
    mutate(hour = floor_date(dateobserved, unit = "hour")) %>%   # round down
    group_by(entity_id, hour) %>%
    summarise(hm_temp = mean(temperature, na.rm = TRUE), .groups = "drop")
```

```{r Load and preprocess covariates}

# Import of pre-processed temp data and import and pre-processing of covariates 

#Shortcut -> import pre-processed hourly means
temp_clean_station_hm <- sf::read_sf('../data/temp_HD/others/temp_clean_station_hm.gpkg')

# Reproject pre-processed temp data
temp_clean_station_hm <- st_transform(temp_clean_station_hm, crs="EPSG:32632")

# Drop Gaiberg station as it is outside of AOI
temp_clean_station_hm <- temp_clean_station_hm %>%
  filter(!entity_id %in% c(
    "hd:DE_Gaiberg_69251_21:WeatherObserved"
  ))

# Import AOI and station coords
hd_bounds <- sf::read_sf('../data/AOI/OSM_boundaries_HD.geojson')
station_coords_sf <- sf::read_sf('../data/temp_HD/others/station_coords.gpkg')

# Import the covariates
dem <- terra::rast("../data/DEM/hd_elevation_4326.tif")
tch <- terra::rast("../data/canopy_height/hd_canopy_height_4326.tif")
ghs_bh <- terra::rast("../data/building_height/hd_ANBH_4326.tif")

# Mask out all values which are assigned to water and have value 101
# Not part of this area. But needs to be masked as well: 102 Snow/ice; 103 No data
tch[tch == 101] <- NA

# Reproject the data
hd_bounds <- st_transform(hd_bounds, crs = st_crs(temp_clean_station_hm))
station_coords_sf <- st_transform(station_coords_sf, crs = st_crs(temp_clean_station_hm))
dem <- terra::project(dem, y = crs(temp_clean_station_hm))
tch <- terra::project(tch, y = crs(temp_clean_station_hm))
ghs_bh <- terra::project(ghs_bh, y = crs(temp_clean_station_hm))

# Check if any CRS mismatches
if (!identical(crs(tch), crs(dem))) {
  stop("CRS mismatch: Tree canopy height and DEM don't have the same CRS.")
}
if (!identical(crs(ghs_bh), crs(dem))) {
  stop("CRS mismatch: Building height and DEM don't have the same CRS.")
}
cat("All covariates share the same CRS.\n")

# Convert to SpatialPointsDataFrame
station_coords <- as(station_coords_sf, "Spatial")

# Create a Raster Stack for all covariate information
cat("Checking input raster resolutions in metres:\n")
cat(sprintf("DEM:    %.2f x %.2f\n", terra::res(dem)[1], terra::res(dem)[2]))
cat(sprintf("TCH:    %.2f x %.2f\n", terra::res(tch)[1], terra::res(tch)[2]))
cat(sprintf("GHS_BH: %.2f x %.2f\n\n", terra::res(ghs_bh)[1], terra::res(ghs_bh)[2]))

# Align canopy height and building height to DEM resolution and extent
tch_aligned    <- terra::resample(tch, dem, method = "bilinear")
ghs_bh_aligned <- terra::resample(ghs_bh, dem, method = "near")

# Check alignment after resampling
if (!terra::compareGeom(dem, tch_aligned, ghs_bh_aligned, stopOnError = FALSE)) {
  stop("Covariate rasters are not perfectly aligned after resampling. Check extent/resolution/CRS.")
}

# Set a Gaussian filter

# Gaussian filter with radius 60 m and default sigma (≈ radius/3)
w <- focalWeight(tch, d = 22.86647, type = "Gauss")

# Apply Gaussian smoothing to the ALIGNED canopy height raster
tch_cooling <- focal(tch_aligned, w = w, fun = sum, na.rm = TRUE)

# Ensure cooling raster matches DEM resolution and extent
tch_cooling_aligned <- terra::resample(tch_cooling, dem, method = "bilinear")

names(tch_cooling_aligned) <- "canopy_cooling"

# Combine layers into a single SpatRaster stack
env_stack <- c(dem, tch_aligned, tch_cooling_aligned, ghs_bh_aligned)

# Rename layers for clarity
names(env_stack) <- c("elevation", "canopy_height", "canopy_cooling", "building_height")

# Code to include lat and lon information in raster stack

# Also add lat and lon to the raster stack
# Create coordinate rasters
lon <- terra::xFromCell(env_stack, 1:ncell(env_stack))
lat <- terra::yFromCell(env_stack, 1:ncell(env_stack))

# Convert to raster layers
lon_raster <- dem
lat_raster <- dem
values(lon_raster) <- lon
values(lat_raster) <- lat
names(lon_raster) <- "lon"
names(lat_raster) <- "lat"

# Stack the three raster layers
grids <- c(env_stack, lon_raster, lat_raster)

# Preparation of grid as a covariate for running RK
grids_ras <- stack(grids)

# Convert to SpatialGridDataFrame
grids_sp <- as(grids_ras, "SpatialGridDataFrame")
```

--------------------------------------------------------------------------------

```{r Adding covariates to temperature data}

# Convert prediction grid once
r_brick    <- raster::brick(grids_sp)
grid_terra <- terra::rast(r_brick)
terra::crs(grid_terra) <- "EPSG:32632"

# Extract raster values at point features
ex <- terra::extract(grid_terra, temp_clean_station_hm)

# Aggregate duplicates to a single row per original feature
# Use mean
ex_agg <- aggregate(. ~ ID, data = ex, FUN = mean, na.rm = TRUE)

# Reorder aggregated rows to match original feature order
ex_agg <- ex_agg[match(seq_len(nrow(temp_clean_station_hm)), ex_agg$ID), -1]

# Bind attributes + extracted covariates
temp_clean_station_hm_cov <- cbind(as.data.frame(temp_clean_station_hm), ex_agg)

str(temp_clean_station_hm_cov)
```

```{r Calculate correlation between the three covariates and the dependent variable temperature}

# Pick relevant columns
df <- temp_clean_station_hm_cov %>%
  st_drop_geometry() %>%   # remove geometry column
  dplyr::select(entity_id, hm_temp, elevation, canopy_cooling, building_height)

# Aggregate temperature per station (mean across time)
station_df <- df %>%
  group_by(entity_id, elevation, canopy_cooling, building_height) %>%
  summarise(mean_temp = mean(hm_temp, na.rm = TRUE), .groups = "drop")

# Compute correlation matrix (Pearson + Spearman)
vars <- c("mean_temp", "elevation", "canopy_cooling", "building_height")

corr_p <- cor(station_df[vars], use = "pairwise.complete.obs", method = "pearson")
corr_s <- cor(station_df[vars], use = "pairwise.complete.obs", method = "spearman")

print("Pearson correlation:")
print(corr_p)

print("Spearman correlation:")
print(corr_s)

# reshape to long format
station_long <- station_df %>%
  pivot_longer(cols = c(elevation, canopy_cooling, building_height),
               names_to = "covariate", values_to = "value")

# 2. Compute Pearson correlation per covariate
cors <- station_long %>%
  group_by(covariate) %>%
  summarise(
    r = cor(value, mean_temp, method = "pearson", use = "pairwise.complete.obs")
  ) %>%
  mutate(
    label = sprintf("italic(r) == %.2f", r)   # format for plotmath
  )

p_corr_temp_cov <- ggplot(station_long, aes(x = value, y = mean_temp)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = TRUE, color = "darkred") +
  #geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs"), color = "blue", linetype = "dashed")
  facet_wrap(~ covariate, scales = "free_x", nrow = 1,
             labeller = labeller(
               covariate = c(elevation = "Elevation [m]",
                             canopy_cooling = "Gaussian-smoothed Canopy Height [m]",
                             building_height = "Building Height [m]")
             )) +
  geom_text(
    data = cors,
    aes(label = label),
    x = -Inf, y = Inf,
    hjust = -0.2, vjust = 2,
    parse = TRUE,
    size = 5,
    fontface = "bold",
    inherit.aes = FALSE
  ) +
  scale_y_continuous(limits = c(9, 15)) +
  scale_x_continuous(expand = expansion(mult = c(0.02, 0.02))) +
  labs(title = "Linear Relationships Between Yearly Mean Temperature and Environmental Covariates",
       x = NULL, y = "Yearly Mean Temperature [°C]") +
  theme_bw() +
  theme(
    strip.text = element_text(face = "bold", size = 14),
    plot.title = element_text(
        face = "bold", size = 20, hjust = 0.5,
        margin = margin(t = 10, b = 10)),
    axis.title.y = element_text(size = 14),
    axis.text = element_text(size = 12)
  )

p_corr_temp_cov

# Save output to file
#ggsave("../plots/correlation_temperature_vs_covariates.png", p_corr_temp_cov, width = 12, height = 6, dpi = 300)
```

```{r Check for multicollineraity}
# Model 1: raw canopy height
mlr_vif_raw <- lm(hm_temp ~ elevation + canopy_height + building_height,
                  data = temp_clean_station_hm_cov)

# Model 2: gaussian-filtered canopy height
mlr_vif_gauss <- lm(hm_temp ~ elevation + canopy_cooling + building_height,
                    data = temp_clean_station_hm_cov)

# Helper: extract VIF into tidy format
get_vif_df <- function(model, model_label) {
  car::vif(model) %>%
    as.data.frame() %>%
    rownames_to_column("covariate") %>%
    rename(VIF = 2) %>%
    mutate(model = model_label)
}

vif_df <- bind_rows(
  get_vif_df(mlr_vif_raw,   "No filter"),
  get_vif_df(mlr_vif_gauss, "Gaussian filter")
) %>%
  mutate(
    covariate = dplyr::case_when(
      covariate %in% c("canopy_height", "canopy_cooling") ~ "canopy",
      TRUE ~ covariate
    ),
    covariate = factor(
      covariate,
      levels = c("elevation", "canopy", "building_height"),
      labels = c("Elevation", "Canopy height", "Building height")
    ),
    model = factor(model, levels = c("No filter", "Gaussian filter"))
  )

4

# Save output to file
ggsave("../plots_report/multicollinearity.png", p_vif_comp, width = 8, height = 5, dpi = 300)
```


```{r Create output folder}

output_dir <- "../gaussian_MLR_07_2023_12days"
if (!dir.exists(output_dir)) dir.create(output_dir)

# Define subdirectories for specific output types
lm_dir        <- file.path(output_dir, "lm_metrics")
lm_dir_z      <- file.path(output_dir, "lm_metrics_z")
variogram_dir <- file.path(output_dir, "variogram")

# Create subdirectories if they don't exist
dir.create(lm_dir, showWarnings = FALSE)
dir.create(lm_dir_z, showWarnings = FALSE)
dir.create(variogram_dir, showWarnings = FALSE)

# ================================================
# Datetime helpers (labels & filenames)
# ================================================
ts_to_posix <- function(ch) {
  if (inherits(ch, "POSIXt")) {
    # already a datetime — just return it
    return(ch)
  } else {
    # numeric hours → convert to POSIXct
    return(as.POSIXct(ch * 3600, origin = "1970-01-01", tz = tz_berlin))
  }
}
nice_dt     <- function(x) format(x, "%Y-%m-%d %H:00 %Z")                                   
file_dt     <- function(x) format(x, "%Y-%m-%d_%H00")    

# Define output file and remove any old version
metrics_file <- file.path(lm_dir, "lm_model_performance_summary.csv")
if (file.exists(metrics_file)) {
  file.remove(metrics_file)
  cat("Old metrics file removed — starting fresh.\n")
}

metrics_file_z <- file.path(lm_dir_z, "lm_z_model_performance_summary.csv")
if (file.exists(metrics_file_z)) {
  file.remove(metrics_file_z)
  cat("Old metrics file removed — starting fresh.\n")
}

# where to store per-covariate results from the z-MLR
coef_file_z <- file.path(lm_dir_z, "lm_z_coefficients_by_timestamp.csv")
```

```{r Main loop}

# Set timestamps and analysis period
timestamps_all <- sort(unique(temp_clean_station_hm$hour))
start_date <- as.POSIXct("2023-07-01 00:00:00", tz = "Europe/Berlin")
end_date   <- as.POSIXct("2023-07-12 23:00:00", tz = "Europe/Berlin")
timestamps <- timestamps_all[timestamps_all >= start_date & timestamps_all <= end_date]
cat("Looping from:", min(timestamps), "to", max(timestamps), "\n")

# Counters and progress bar
total     <- length(timestamps)
completed <- 0L
skipped   <- 0L
pb <- txtProgressBar(min = 0, max = total, style = 3)
cat(sprintf("Processing %d timestamps…\n", total))

# Init empty containers
total_metrics <- data.frame()
all_temp_data <- data.frame()
mlr_residuals_all <- data.frame()

# Main loop over timestamps
for (i in seq_along(timestamps)) {
  ts <- timestamps[i]                                    
  dt_local <- ts_to_posix(ts)                     
  dt_lab   <- nice_dt(dt_local)                   
  dt_file  <- file_dt(dt_local)                   
  cat(sprintf("\n[%d/%d] Processing: hour=%s  (%s)\n", i, total, ts, dt_lab))  

  # ------------------------------------------------
  # STEP 1: Filter hourly data and plot temperature
  # ------------------------------------------------
  
  sel_temp_hm <- temp_clean_station_hm %>%
    filter(hour == ts)
  
  # Skip if fewer than 20 data points for current hour
  if (nrow(sel_temp_hm) < 20) {                                 
    skipped <- skipped + 1L                                       
    cat("  → skipped (n < 20)\n")                          
    next                                                         
  }
  
  # Append the selected temperature data to full dataset
  all_temp_data <- rbind(all_temp_data, sel_temp_hm)
  
  # Generate line plot showing hourly mean temperature for each station
  # across the time range covered so far in the loop
  p_all <- ggplot(all_temp_data, aes(x = hour, y = hm_temp, color = entity_id, group = entity_id)) +  
    geom_line() +
    geom_point(size = 1) +
    labs(title = "Hourly Mean Temperature at Each Station",        
         x = "Date & Hour (Europe/Berlin)", y = "Mean Temperature (°C)", color = "Station") + 
    scale_x_datetime(date_labels = "%d.%m\n%H:%M", date_breaks = "12 hours") +              
    theme_minimal(base_size = 12) +
    theme(axis.text.x = element_text(angle = 0, hjust = 0.5))                                  
  
  # Save the temperature plot
  ggsave(file.path(output_dir, "all_stations_hourly_mean_temperature.png"),
         plot = p_all, width = 10, height = 6, dpi = 300)

  # ------------------------------------------------
  # STEP 2: Add covariates (elev, canopy, bldg) from grid
  # ------------------------------------------------
  
  # Convert Spatial object to RasterBrick
  r_brick <- brick(grids_sp)
  
  # Convert RasterBrick to terra SpatRaster object
  grid_terra <- rast(r_brick)
  
  # Extract raster values at the point locations
  extracted_vals <- terra::extract(grid_terra, sel_temp_hm)
  
  # Remove the first column (ID)
  extracted_vals <- extracted_vals[,-1]
  
  # Combine the point data with the extracted raster values
  temp_shm.ex <- cbind(as.data.frame(sel_temp_hm), extracted_vals)
  
  # Choose which covariates to use (must match layer names from your raster)
  covars <- c("elevation", "canopy_cooling", "building_height")
  covars <- intersect(covars, names(temp_shm.ex))
  
  # Safe z-scaling within THIS timestamp (across stations present now)
  safe_z <- function(x) {
    m <- mean(x, na.rm = TRUE)
    s <- sd(x,   na.rm = TRUE)
    if (is.na(s) || s == 0) return(rep(0, length(x)))  
    (x - m) / s
  }
  
  # Add *_z columns
  for (nm in covars) {
    temp_shm.ex[[paste0(nm, "_z")]] <- safe_z(temp_shm.ex[[nm]])
  }
  
  # Keep the z parameters used THIS HOUR
  z_params_this <- data.frame(
    timestamp = ts,
    covariate = covars,
    mean      = sapply(temp_shm.ex[covars], function(v) mean(v, na.rm = TRUE)),
    sd        = sapply(temp_shm.ex[covars], function(v) sd(v,   na.rm = TRUE))
  )
  # Append to a csv if you like:
  write.table(z_params_this, file = file.path(lm_dir_z, "z_params_by_timestamp.csv"),
              sep = ",", row.names = FALSE,
              col.names = !file.exists(file.path(lm_dir_z,"z_params_by_timestamp.csv")),
              append = TRUE)
  
  # ------------------------------------------------
  # STEP 3: Fit linear model and prepare spatial data
  # ------------------------------------------------
  
  # Fit multiple linear regression model
  reg_model <- lm(hm_temp ~ elevation + canopy_cooling + building_height, data = temp_shm.ex)
  summary_lm <- summary(reg_model)
  
  # Compute residuals and predictions
  pred <- predict(reg_model, temp_shm.ex)
  resid <- temp_shm.ex$hm_temp - pred
  
  # Extract per-timestamp residuals
  mlr_residuals_df <- temp_shm.ex %>%
    dplyr::transmute(
      timestamp   = ts,
      entity_id   = entity_id,      # station id; adjust if your column is named differently
      hm_temp_obs = hm_temp,        # observed temperature
      hm_temp_pred = pred,          # predicted by regular MLR
      residual    = resid,          # obs - pred
      elevation   = elevation,
      canopy_cooling = canopy_cooling,
      building_height = building_height
    )
  
  # Accumulate
  if (!exists("mlr_residuals_all") || nrow(mlr_residuals_all) == 0) {
    mlr_residuals_all <- mlr_residuals_df
  } else {
    mlr_residuals_all <- dplyr::bind_rows(mlr_residuals_all, mlr_residuals_df)
  }
    
  # Extract metrics
  model_metrics <- data.frame(
    timestamp      = ts,
    r_squared      = summary_lm$r.squared,
    adj_r_squared  = summary_lm$adj.r.squared,
    f_statistic    = summary_lm$fstatistic[1],
    f_pvalue       = pf(summary_lm$fstatistic[1],
                        summary_lm$fstatistic[2],
                        summary_lm$fstatistic[3],
                        lower.tail = FALSE),
    residual_se    = summary_lm$sigma,
    rmse           = sqrt(mean(resid^2)),
    mae            = mean(abs(resid)),
    AIC            = AIC(reg_model),
    BIC            = BIC(reg_model)
  )
  
  write.table(
    model_metrics,
    file = metrics_file,
    sep = ",",
    row.names = FALSE,
    col.names = !file.exists(metrics_file),
    append = TRUE
  )

  # Save model's residuals back into original df as new column
  temp_shm.ex$residuals <- resid
  
  # Convert df into spatial object using existing geometry and
  # Convert MULTIPOINT geometries to POINT
  temp_shm.sf <- st_as_sf(temp_shm.ex) |> st_cast("POINT")
  
  # z-MLR: fit with standardized predictors
  reg_model_z  <- lm(hm_temp ~ elevation_z + canopy_cooling_z + building_height_z,
                     data = temp_shm.ex)
  summary_lm_z <- summary(reg_model_z)
  
  # predictions & residuals for z-MLR (distinct names to avoid clobbering)
  pred_z  <- predict(reg_model_z, temp_shm.ex)
  resid_z <- temp_shm.ex$hm_temp - pred_z
  
  # Model-level metrics for z-MLR
  model_metrics_z <- data.frame(
    timestamp      = ts,
    r_squared      = summary_lm_z$r.squared,
    adj_r_squared  = summary_lm_z$adj.r.squared,
    f_statistic    = summary_lm_z$fstatistic[1],
    f_pvalue       = pf(summary_lm_z$fstatistic[1],
                        summary_lm_z$fstatistic[2],
                        summary_lm_z$fstatistic[3],
                        lower.tail = FALSE),
    residual_se    = summary_lm_z$sigma,
    rmse           = sqrt(mean(resid_z^2)),
    mae            = mean(abs(resid_z)),
    AIC            = AIC(reg_model_z),
    BIC            = BIC(reg_model_z)
  )
  
  write.table(
    model_metrics_z,
    file = metrics_file_z,
    sep = ",",
    row.names = FALSE,
    col.names = !file.exists(metrics_file_z),
    append = TRUE
  )
  
  # ---- Your requested additions: robust SEs, β (unitless), relative importance ----
  
  # robust SEs (HC3) for safer p-values
  rob <- coeftest(reg_model_z, vcov = vcovHC(reg_model_z, type = "HC3"))
  
  coef_df <- broom::tidy(rob) |>
    dplyr::filter(term != "(Intercept)") |>
    dplyr::transmute(
      timestamp     = ts,
      term,
      estimate_semi = estimate,      # °C per 1 SD of predictor (since X are z)
      se_robust     = std.error,
      t_robust      = statistic,
      p_robust      = p.value
    )
  
  # unitless β for cross-time comparability
  y_sd <- sd(temp_shm.ex$hm_temp, na.rm = TRUE)
  coef_df$beta_std <- coef_df$estimate_semi / y_sd
  
  # relative importance (LMG): who explains most variance at this hour?
  imp <- calc.relimp(reg_model_z, type = "lmg", rela = TRUE)
  coef_df$lmg <- imp$lmg[match(coef_df$term, names(imp$lmg))]  # sums to 1 per timestamp
  
  # append to tidy CSV
  write.table(
    coef_df,
    file = coef_file_z,
    sep = ",",
    row.names = FALSE,
    col.names = !file.exists(coef_file_z),
    append = TRUE
  )

  # ------------------------------------------------
  # STEP 4: Fit GAM model and prepare spatial data
  # ------------------------------------------------
  
  # CONTINUE WORKING HERE
  
  
  # ------------------------------------------------
  # STEP 5: Compute anisotropy & fit directional variogram
  # ------------------------------------------------
  
  # Compute empirical variogram map of model residuals
  # This helps visualize spatial autocorrelation in all directions
  varmap <- variogram(residuals ~ 1, data = temp_shm.sf, map = TRUE,
                    # cutoff is half the spatial extent (or diameter) of the study area
                    cutoff = sqrt(areaSpatialGrid(grids_sp)) / 2, 
                    # scaled relative to the resolution of the grid
                    width = 30 * grids_sp@grid@cellsize[1])

  # Store cutoff and bin width values for reuse and labeling
  cutoff_val <- sqrt(areaSpatialGrid(grids_sp)) / 2
  width_val  <- 30 * grids_sp@grid@cellsize[1]
  
  # Report how many rows are in variogram map
  cat("Variogram map has", nrow(as.data.frame(varmap)), "rows\n")
  
  # Define output filename for the variogram map plot
  file_varmap <- file.path(variogram_dir, paste0(dt_file, "_variogram_map.png"))  
  
  # Plot and save the directional variogram map
  png(file_varmap, width = 8, height = 6, units = "in", res = 300)
  print(
    plot(varmap,
         col.regions = grey(rev(seq(0, 1, 0.025))),
         main = sprintf("Directional Variogram Map (cutoff: %.1f km, width: %.1f km)\n%s",  
                        cutoff_val / 1000, width_val / 1000, dt_lab))
  )
  dev.off()
  
  # Compute directional variograms for North–South (0°) and East–West (90°)
  rv.temp <- variogram(residuals ~ 1, temp_shm.sf, alpha = c(0, 45, 90, 135))

  # Fit theoretical exponential variogram model to the directional variogram
  rvgm.temp <- fit.variogram(rv.temp,
      vgm(psill=var(temp_shm.sf$residuals),
          "Exp", nugget=0, anis= c(p=90, s=0.5))) # p = direction, s = anisotropy ratio
  
  # Define output filename for directional variogram fit plot
  file_varfit <- file.path(variogram_dir, paste0(dt_file, "_directional_variogram_fit.png"))  
 
  # Plot the empirical and fitted directional variograms and save
  png(file_varfit, width = 8, height = 6, units = "in", res = 300)
  print(
    plot(rv.temp, rvgm.temp,
         main = paste0("Directional Variogram Fit (0°, 45°, 90°, 135°)\n", dt_lab),   
         plot.nu = FALSE, cex = 1.5, pch = "+", col = "black")
  )
  dev.off()
  
  # ------------------------------------------------
  # STEP 6: Progress bar
  # ------------------------------------------------  
  
  completed <- completed + 1L 
  setTxtProgressBar(pb, completed)
  cat("\n")
  cat(sprintf("  → done. Completed: %d/%d (skipped: %d)\n",      
              completed, total, skipped))                        
}

# Close progress bar + final summary
close(pb)                                                        
cat(sprintf("\nFinished. Completed: %d/%d | Skipped: %d\n",       
            completed, total, skipped))                         

mlr_residuals_file <- file.path(output_dir, "mlr_residuals_all.csv")
if (exists("mlr_residuals_all") && nrow(mlr_residuals_all) > 0) {
  mlr_residuals_all$datetime_local <- ts_to_posix(mlr_residuals_all$timestamp)
  mlr_residuals_all <- dplyr::relocate(mlr_residuals_all, datetime_local, .after = timestamp)
  write.csv(mlr_residuals_all, mlr_residuals_file, row.names = FALSE)
}
```

```{r Analyse MLR metrics}

# Read CSV with lm_metrics
lm_metrics <- read.csv("../gaussian_MLR_07_2023_12days/lm_metrics/lm_model_performance_summary.csv")

class(lm_metrics$timestamp)

lm_metrics <- lm_metrics %>%
  mutate(
    timestamp = if_else(
      nchar(timestamp) == 10, paste0(timestamp, " 00:00:00"), timestamp
    ),
    timestamp = as.POSIXct(timestamp, format = "%Y-%m-%d %H:%M:%S", tz = "Europe/Berlin"),
    date = as.Date(format(timestamp, tz = "Europe/Berlin")),
    hour = hour(timestamp)
  )

# Reshape to long format
lm_metrics_long <- lm_metrics %>%
  pivot_longer(cols = c(r_squared, adj_r_squared),
               names_to = "metric",
               values_to = "value")

# Filter year 2023
lm_df_plot <- lm_metrics_long %>%
  filter(year(date) == 2023)

# Plot: Daily Evaluation of Multiple Linear Regression Model Performance
p_lm <- ggplot(lm_df_plot, aes(x = hour, y = value, color = metric, group = metric)) +
  # Lines and points for model metrics
  geom_line(linewidth = 1.2) +
  geom_hline(
    aes(yintercept = 1, linetype = "Perfect Fit (R² = 1)"),
    color = "grey60"
  ) +
  geom_point(size = 1.8) +
  # Facet by date
  facet_wrap(~ date, ncol = 3, scales = "fixed") +
  # Color and linetype scales
  scale_color_manual(
    values = c(
      "r_squared"     = "#6699CC",
      "adj_r_squared" = "#CC6677"
    ),
    labels = c(
      "r_squared"     = expression(R^2),
      "adj_r_squared" = expression(Adjusted~R^2)
    )
  ) +
  scale_linetype_manual(
    name   = "",
    values = c("Perfect Fit (R² = 1)" = "dotted")
  ) +
  # Labels
  labs(
    title    = "Daily Evaluation of Multiple Linear Regression Model Performance",
    subtitle = "Hourly R² and Adjusted R² values from models predicting hourly mean temperature",
    x        = "Hour of the Day",
    y        = "Metric Value",
    color    = "Metrics"
  ) +
  # X-axis ticks
  scale_x_continuous(breaks = c(0, 6, 12, 18, 24)) +
  # Theme
  theme_minimal(base_size = 14) +
  theme(
    legend.position    = "top",
    legend.box         = "horizontal",
    panel.grid.minor   = element_blank(),
    panel.spacing      = unit(0.8, "lines"),
    strip.background   = element_rect(fill = "grey90", color = NA),
    strip.text         = element_text(face = "bold")
  )

# Show plot
p_lm

# Save plot
#ggsave("../regression_tests/lm_metrics_r2_adjr2_by_day_04_2023.png", plot = p_lm, width = 12, height = 8, dpi = 300)
```

```{r Daily Variation of LMG-Based Covariate Importance}
# Read and prepare data
coef_all <- read.csv(coef_file_z)
fit <- read.csv(metrics_file_z)

class(fit)

fit <- fit %>%
  mutate(
    timestamp = if_else(
      nchar(timestamp) == 10, paste0(timestamp, " 00:00:00"), timestamp
    ),
    timestamp = as.POSIXct(timestamp, format = "%Y-%m-%d %H:%M:%S", tz = "Europe/Berlin"),
    month = month(timestamp),
    year = year(timestamp)
  ) %>%
  filter(year == 2023, month == 7)

coef_all <- coef_all %>%
  mutate(
    timestamp = if_else(
      nchar(timestamp) == 10,
      paste0(timestamp, " 00:00:00"),
      timestamp
    ),
    timestamp = as.POSIXct(timestamp, format = "%Y-%m-%d %H:%M:%S", tz = "Europe/Berlin"),
    date = as.Date(format(timestamp, tz = "Europe/Berlin")),
    hour  = hour(timestamp),
    month = month(timestamp),
    year  = year(timestamp)
  ) %>%
  filter(month == 7 & year == 2023) %>%
  left_join(dplyr::select(fit, timestamp, r_squared), by = "timestamp")

# Determine pagination structure
days_per_page <- 12
n_pages <- ceiling(length(unique(coef_all$date)) / days_per_page)

# Output directory
output_plots <- "../plots/figures_covariate_importance"
if (!dir.exists(output_dir)) dir.create(output_dir)

# Paginated plotting loop
for (i in seq_len(n_pages)) {
  p_cov <- ggplot(coef_all, aes(x = hour)) +
    geom_area(aes(y = lmg, fill = term), position = "fill", alpha = 0.85) +
    geom_line(aes(y = r_squared, color = "Model R²"), linewidth = 0.9) +
    ggforce::facet_wrap_paginate(~ date, ncol = 3, nrow = 4, page = i) +
    scale_x_continuous(breaks = seq(0, 24, 6), limits = c(0, 24)) +
    scale_y_continuous(
      breaks = c(0, 0.25, 0.50, 0.75, 1.00),
      labels = scales::percent_format(accuracy = 1),
      sec.axis = sec_axis(~ ., name = "Model R²")
    ) +
    theme(
      panel.grid = element_blank(),
      panel.grid.major.y = element_line(color = "grey85")
    ) +
    scale_fill_manual(
      name = "Covariates (z-standardised)",
      values = c(
        "building_height_z" = "darkorange3",
        "canopy_cooling_z"   = "darkolivegreen",
        "elevation_z"       = "burlywood2"
      ),
      labels = c(
        "building_height_z" = "Building Height",
        "canopy_cooling_z"   = "Gaussian-filtered Canopy Height",
        "elevation_z"       = "Elevation"
      )
    ) +
    scale_color_manual(
      name = "",  # we'll merge this with the fill legend
      values = c("Model R²" = "black"),
      guide = guide_legend(override.aes = list(linetype = 1, fill = NA))
    ) +
    guides(
      fill = guide_legend(order = 1),
      color = guide_legend(order = 2)
    ) +
    labs(
      title    = sprintf("Daily Variation of LMG-Based Covariate Importance\nin Hourly Multiple Linear Regression Models"),
      caption = "LMG = Lindeman–Merenda–Gold relative importance measure.  
Each hour was modelled using a separate multiple linear regression with z-standardised covariates. 
Areas show the proportional contribution of each covariate to the total model R² for that hour.",
      x = "Hour of the Day",
      y = "Share of R² (LMG, Sum = 1)"
    ) +
    theme_minimal(base_size = 14) +
    theme(
      legend.position  = "bottom",
      legend.title     = element_text(face = "bold"),
      legend.key.width = unit(2, "lines"),
      strip.text       = element_text(face = "bold", size = 12),
      panel.spacing    = unit(0.8, "lines"),
      plot.title       = element_text(face = "bold", size = 20, hjust = 0.5, 
                                      margin = margin(t = 10, b = 10)),
      plot.subtitle    = element_text(size = 12, hjust = 0.5)
    ) +
    theme(
      plot.caption.position = "plot", 
      plot.caption = element_text(hjust = 0, margin = margin(t = 20))
    ) +
    theme(
      panel.grid.major.x = element_line(color = "grey90"),  
      panel.grid.major.y = element_line(color = "grey90"), 
      panel.grid.minor = element_blank()                   
    )

  # Save each page
  # ggsave(
  #   filename = file.path(output_plots, sprintf("covariate_importance.png")),
  #   plot = p_cov,
  #   width = 12, height = 10, dpi = 300
  # )
}

p_cov
```

```{r Loop to calculate Morans I on regression residuals}
# Import residuals
mlr_residuals <- read.csv(
  "../gaussian_MLR_07_2023_12days/mlr_residuals_all.csv",
  stringsAsFactors = FALSE
)

# Extract coordinates from station layer
station_coords_df <- station_coords_sf %>%
  mutate(
    lon = st_coordinates(.)[, 1],
    lat = st_coordinates(.)[, 2]
  ) %>%
  st_drop_geometry() %>%
  dplyr::select(entity_id, lon, lat)

# Parse timestamps and join coordinates
mlr_residuals <- mlr_residuals %>%
  mutate(
    hour = suppressWarnings(ymd_hms(timestamp, tz = "Europe/Berlin")),
    hour = ifelse(is.na(hour), ymd(timestamp, tz = "Europe/Berlin"), hour),
    hour = as.POSIXct(hour, origin = "1970-01-01", tz = "Europe/Berlin"),
    day  = as.Date(hour, tz = "Europe/Berlin")
  ) %>%
  left_join(station_coords_df, by = "entity_id")

# +Convert to sf object with CRS 32632
mlr_residuals_sf <- st_as_sf(
  mlr_residuals,
  coords = c("lon", "lat"),
  crs = 32632,      
  remove = FALSE
)

# Loop across days using the sf version
month_start <- as.Date("2023-07-01")
month_end   <- as.Date("2023-07-12")
all_days    <- seq(month_start, month_end, by = "day")

moran_summary_list <- list()

for (one_day in all_days) {
  message("Processing ", one_day)

  # Filter to that day directly from sf object
  df_day <- mlr_residuals_sf %>%
    filter(day == one_day)

  if (nrow(df_day) == 0) next

  # Compute Moran’s I for each timestamp
  moran_results <- df_day %>%
    group_split(hour) %>%
    map_dfr(function(df_slice) {

      xy <- st_coordinates(df_slice)

      knn <- knearneigh(xy, k = 8)
      nb  <- knn2nb(knn)
      lw  <- nb2listw(nb, style = "W", zero.policy = TRUE)

      mt <- moran.test(df_slice$residual, lw, zero.policy = TRUE)
      mc <- moran.mc(df_slice$residual, lw, nsim = 999, zero.policy = TRUE)
      q  <- quantile(mc$res, c(0.025, 0.975))

      tibble(
        hour     = unique(df_slice$hour),
        moran_i  = mt$estimate[["Moran I statistic"]],
        expected = mt$estimate[["Expectation"]],
        p_asym   = mt$p.value,
        p_mc     = mc$p.value,
        lo95     = q[1],
        hi95     = q[2]
      )
    })
  
  moran_results_clean <- moran_results %>%
  mutate(
    date = as.Date(hour, tz = "Europe/Berlin"),
    hour = lubridate::hour(hour),
    moran_sig = if_else(p_mc < 0.05, TRUE, FALSE)
  ) %>%
  dplyr::select(date, hour, moran_i, p_mc, moran_sig)

  moran_summary_list[[as.character(one_day)]] <- moran_results_clean

  # Convert one_day to proper Date for plotting and filenames
  day_label <- as.Date(one_day, origin = "1970-01-01")
  
  # Plot
  p_moran <- ggplot(moran_results, aes(x = hour, y = moran_i)) +
    geom_ribbon(aes(ymin = lo95, ymax = hi95, fill = "95% confidence envelope"), alpha = 0.5) +
    geom_hline(
      data = data.frame(y = 0, label = "No autocorrelation"),
      aes(yintercept = y, linetype = label),
      color = "black"
    ) +
    geom_hline(
      data = data.frame(y = unique(moran_results$expected), label = "Expected under randomness"),
      aes(yintercept = y, linetype = label),
      color = "black"
    ) +
    geom_line(color = "black", linewidth = 0.6) +
    geom_point(aes(color = p_mc < 0.05), size = 2.8) +
    scale_color_manual(
      name = "Significance (p < 0.05)",
      values = c("TRUE" = "#440154", "FALSE" = "#35b779"),
      labels = c("TRUE" = "Significant", "FALSE" = "Not significant")
    ) +
    scale_fill_manual(name = NULL, values = c("95% confidence envelope" = "grey80")) +
    scale_linetype_manual(
      name = NULL,
      values = c("No autocorrelation" = "dashed",
                 "Expected under randomness" = "dotted")
    ) +
    scale_x_datetime(
      date_labels = "%H:%M",
      date_breaks = "6 hours",
      expand = expansion(mult = c(0, 0.02))
    ) +
    coord_cartesian(ylim = c(-0.2, 0.6)) +
    labs(
      title = "Moran’s I of Regression Residuals",
      subtitle = paste("Date:", format(day_label, "%Y-%m-%d")),
      x = "Hour of Day",
      y = "Moran’s I"
    ) +
    guides(
      color = guide_legend(order = 1),
      fill = guide_legend(order = 2, override.aes = list(alpha = 0.5)),
      linetype = guide_legend(order = 3)
    ) +
    theme_minimal(base_size = 13) +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      legend.position = "bottom",
      legend.box = "horizontal",
      legend.title = element_text(size = 6, face = "bold"),
      legend.text = element_text(size = 6),
      plot.title = element_text(face = "bold", size = 14),
      plot.subtitle = element_text(size = 11, color = "grey20"),
      panel.grid.minor = element_blank()
    )

  # Save plot
  # out_file <- file.path("../plots/moransI_residuals", paste0("moransI_residuals_", one_day, ".png"))
  # ggsave(out_file, p_moran, width = 8, height = 5, dpi = 300)
}

moran_summary <- dplyr::bind_rows(moran_summary_list)
```

```{r Hourly Robust Significance of Model Covariates}
# Output directory for saving figures
#output_pval <- "../plots/figures_pvalues"
#if (!dir.exists(output_dir)) dir.create(output_dir)

# Categorize p-values before plotting
coef_all <- coef_all %>%
  mutate(
    p_category = case_when(
      p_robust < 0.001 ~ "< 0.001",
      p_robust < 0.01  ~ "< 0.01",
      p_robust < 0.05  ~ "< 0.05",
      TRUE             ~ "≥ 0.05 (n.s.)"
    )
  )

coef_all <- coef_all %>%
  left_join(moran_summary, by = c("date", "hour"))

# One row per (date, hour), keeping only hours with significant Moran's I
moran_band <- coef_all %>%
  dplyr::distinct(date, hour, moran_sig) %>%
  dplyr::filter(moran_sig)   # TRUE = significant spatial autocorrelation


for (i in seq_len(n_pages)) {
  p_pval <- ggplot(coef_all, aes(x = hour, y = term, fill = p_category)) +
    # PLOT CONTENT
    geom_tile(color = "grey85") +
    geom_rect(
      data = moran_band,
      aes(
        xmin = hour - 0.5, xmax = hour + 0.5,
        ymin = -Inf, ymax = Inf,
        color = "Moran’s I (p < 0.05)"
      ),
      inherit.aes = FALSE,
      fill = NA,
      linewidth = 0.4,
      alpha = 0.4
    ) +
    ggforce::facet_wrap_paginate(~ date, ncol = 3, nrow = 4, page = i) +
    scale_y_discrete(labels = c(
      "building_height_z" = "Building Height",
      "canopy_cooling_z"   = "Gaussian-filtered Canopy Height",
      "elevation_z"       = "Elevation"
    )) +
    scale_x_continuous(breaks = seq(0, 24, 6), limits = c(0, 24)) +
    scale_fill_manual(
      name = "Significance (robust p-value)",
      values = c(
        "< 0.001" = "#440154",
        "< 0.01"  = "#31688e",
        "< 0.05"  = "#35b779",
        "≥ 0.05 (n.s.)" = "grey85"
      )
    ) +
    scale_color_manual(
      name = "",
      values = c("Moran’s I (p < 0.05)" = "firebrick3")
    ) +
    labs(
      x = "Hour of the Day",
      y = "Covariates",
      title = "Hourly Robust Significance of Model Covariates",
      caption = "Robust p-values are based on hourly multiple linear regression models with z-standardised covariates.
Darker colors represent higher statistical significance. 
Red outlines highlight hours where regression residuals show significant spatial autocorrelation (Moran’s I, p < 0.05)."
    ) +
    theme_minimal(base_size = 14) +
    theme(
      plot.title = element_text(
        face = "bold", size = 20, hjust = 0.5,
        margin = margin(t = 10, b = 10)
      ),
      plot.subtitle = element_text(size = 12, hjust = 0.5),
      plot.caption.position = "plot",
      plot.caption = element_text(hjust = 0, margin = margin(t = 20)),
      strip.text = element_text(face = "bold", size = 12),
      legend.position = "bottom",
      legend.title = element_text(face = "bold"),
      legend.key.width = unit(2, "lines"),
      legend.text = element_text(size = 11),
      legend.box = "horizontal",
      panel.spacing = unit(0.8, "lines"),
      panel.grid = element_blank(),
      panel.grid.major.x = element_line(color = "grey90"),
      panel.grid.major.y = element_line(color = "grey90"),
      panel.grid.minor = element_blank()
    ) +
    guides(
      fill = guide_legend(order = 1),
      color = guide_legend(order = 2,
                           override.aes = list(fill = NA, linewidth = 0.9, size = 5))
    )

  # Save each page
  ggsave(
    filename = file.path("../plots/gaussian_covariate_pvalues.png"),
    plot = p_pval,
    width = 12, height = 10, dpi = 300
  )
}

p_pval
```

