---
title: "MLR_loop_hourly_timestamps"
author: "Johannes Gruenewald"
date: "2025-04-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r Load Required Libraries}

# Spatial Data Handling & Processing
library(sf)
library(sp)
library(terra)
library(raster)
library(stars)
library(spdep)

# Spatial Analysis & Interpolation
library(gstat)
library(randomForest)
library(nlme)
library(automap)
library(mgcv)
library(lmtest)
library(sandwich)
library(broom)
library(relaimpo)

# Data Manipulation & Wrangling
library(tidyverse)

# Data Visualization
library(ggplot2)
library(scales)
library(gridExtra)
library(corrplot)

# Mapping & Interactive Visualization
library(tmap)

# Additional
library(lattice)
library(car)
```

--------------------------------------------------------------------------------

```{r Load temp data}

# Complete data import and pre-processing of the temperature (July 2023)

# Read temperature data for the entire year of 2023
temp <- sf::read_sf('../data/temp_HD/sensor_data_20230107_31days.gpkg')

# Removes rows where temp is NA
temp <- temp %>% filter(!is.na(temperature))

# Reproject the data for Germany
temp <- st_transform(temp, crs="EPSG:32632")

# Clean the data from outliers -> currently working with hard, manual breaks
# Currently nothing is removed by this step because no measurements are above 40°C for the year 2023
temp_clean <- temp %>% filter(temperature >= -35, temperature <= 40)

# Drop all measurement stations which didn't collect data the entire year around
temp_clean_station <- temp_clean %>%
  mutate(dateobserved = as.POSIXct(dateobserved, tz = "Europe/Berlin")) %>%
  filter(!entity_id %in% c(
    "hd:DE_Heidelberg_69120_12:WeatherObserved",
    "hd:DE_Heidelberg_69120_34:WeatherObserved",
    "hd:DE_Heidelberg_69123_41:WeatherObserved",
    "hd:DE_Gaiberg_69251_21:WeatherObserved",
    "hd:DE_Heidelberg_46:WeatherObserved"
  ))

# Check if exactly 5 stations were dropped
cat("Number of stations BEFORE cleaning: ",
    length(unique(na.omit(temp_clean$entity_id))), "\n")

cat("Number of stations AFTER cleaning:  ",
    length(unique(na.omit(temp_clean_station$entity_id))), "\n")

# Calculate hourly mean temp for cleaned data
temp_clean_station_hm <- temp_clean_station %>%
    mutate(hour = floor_date(dateobserved, unit = "hour")) %>%   # round down
    group_by(entity_id, hour) %>%
    summarise(hm_temp = mean(temperature, na.rm = TRUE), .groups = "drop")
```

--------------------------------------------------------------------------------

```{r Load and preprocess covariates}

# Import of pre-processed temp data and import and pre-processing of covariates 

# Import AOI and station coords
hd_bounds <- sf::read_sf('../data/AOI/OSM_boundaries_HD.geojson')
station_coords_sf <- sf::read_sf('../data/temp_HD/station_coords.gpkg')

# Import the covariates
dem <- terra::rast("../data/DEM/hd_elevation_4326.tif")
tch <- terra::rast("../data/canopy_height/hd_canopy_height_4326.tif")
ghs_bh <- terra::rast("../data/building_height/hd_building_height_4326.tif")

# Mask out all values which are assigned to water and have value 101
# Not part of this area. But needs to be masked as well: 102 Snow/ice; 103 No data
tch[tch == 101] <- NA

# Reproject the data
hd_bounds <- st_transform(hd_bounds, crs = st_crs(temp_clean_station_hm))
station_coords_sf <- st_transform(station_coords_sf, crs = st_crs(temp_clean_station_hm))
dem <- terra::project(dem, y = crs(temp_clean_station_hm))
tch <- terra::project(tch, y = crs(temp_clean_station_hm))
ghs_bh <- terra::project(ghs_bh, y = crs(temp_clean_station_hm))

# Check if any CRS mismatches
if (!identical(crs(tch), crs(dem))) {
  stop("CRS mismatch: Tree canopy height and DEM don't have the same CRS.")
}
if (!identical(crs(ghs_bh), crs(dem))) {
  stop("CRS mismatch: Building height and DEM don't have the same CRS.")
}
cat("All covariates share the same CRS.\n")

# Convert to SpatialPointsDataFrame
station_coords <- as(station_coords_sf, "Spatial")

# Create a Raster Stack for all covariate information
cat("Checking input raster resolutions in metres:\n")
cat(sprintf("DEM:    %.2f x %.2f\n", terra::res(dem)[1], terra::res(dem)[2]))
cat(sprintf("TCH:    %.2f x %.2f\n", terra::res(tch)[1], terra::res(tch)[2]))
cat(sprintf("GHS_BH: %.2f x %.2f\n\n", terra::res(ghs_bh)[1], terra::res(ghs_bh)[2]))

# Align canopy height and building height to DEM resolution and extent
tch_aligned    <- terra::resample(tch, dem, method = "bilinear")
ghs_bh_aligned <- terra::resample(ghs_bh, dem, method = "near")

# Check alignment after resampling
if (!terra::compareGeom(dem, tch_aligned, ghs_bh_aligned, stopOnError = FALSE)) {
  stop("Covariate rasters are not perfectly aligned after resampling. Check extent/resolution/CRS.")
}

# Combine layers into a single SpatRaster stack
env_stack <- c(dem, tch_aligned, ghs_bh_aligned)

# Rename layers for clarity
names(env_stack) <- c("elevation", "canopy_height", "building_height")

# Code to include lat and lon information in raster stack

# Also add lat and lon to the raster stack
# Create coordinate rasters
lon <- terra::xFromCell(env_stack, 1:ncell(env_stack))
lat <- terra::yFromCell(env_stack, 1:ncell(env_stack))

# Convert to raster layers
lon_raster <- dem
lat_raster <- dem
values(lon_raster) <- lon
values(lat_raster) <- lat
names(lon_raster) <- "lon"
names(lat_raster) <- "lat"

# Stack the three raster layers
grids <- c(env_stack, lon_raster, lat_raster)

# Preparation of grid as a covariate for running RK
grids_ras <- stack(grids)

# Convert to SpatialGridDataFrame
grids_sp <- as(grids_ras, "SpatialGridDataFrame")
```

--------------------------------------------------------------------------------

```{r Create output folder}

output_dir <- "../MLR_07_2023_12days"
if (!dir.exists(output_dir)) dir.create(output_dir)

# Define subdirectories for specific output types
lm_dir        <- file.path(output_dir, "lm_metrics")
lm_dir_z      <- file.path(output_dir, "lm_metrics_z")
variogram_dir <- file.path(output_dir, "variogram")

# Create subdirectories if they don't exist
dir.create(lm_dir, showWarnings = FALSE)
dir.create(lm_dir_z, showWarnings = FALSE)
dir.create(variogram_dir, showWarnings = FALSE)


# Datetime helpers
ts_to_posix <- function(ch) {
  if (inherits(ch, "POSIXt")) {
    # already a datetime — just return it
    return(ch)
  } else {
    # numeric hours → convert to POSIXct
    return(as.POSIXct(ch * 3600, origin = "1970-01-01", tz = tz_berlin))
  }
}
nice_dt     <- function(x) format(x, "%Y-%m-%d %H:00 %Z")                                   
file_dt     <- function(x) format(x, "%Y-%m-%d_%H00")    

# Define output file and remove any old version
metrics_file <- file.path(lm_dir, "lm_model_performance_summary.csv")
if (file.exists(metrics_file)) {
  file.remove(metrics_file)
  cat("Old metrics file removed — starting fresh.\n")
}

metrics_file_z <- file.path(lm_dir_z, "lm_z_model_performance_summary.csv")
if (file.exists(metrics_file_z)) {
  file.remove(metrics_file_z)
  cat("Old metrics file removed — starting fresh.\n")
}

# where to store per-covariate results from the z-MLR
coef_file_z <- file.path(lm_dir_z, "lm_z_coefficients_by_timestamp.csv")
```

--------------------------------------------------------------------------------

```{r Main loop}

# Set timestamps and analysis period
timestamps_all <- sort(unique(temp_clean_station_hm$hour))
start_date <- as.POSIXct("2023-07-01 00:00:00", tz = "Europe/Berlin")
end_date   <- as.POSIXct("2023-07-12 23:00:00", tz = "Europe/Berlin")
timestamps <- timestamps_all[timestamps_all >= start_date & timestamps_all <= end_date]
cat("Looping from:", min(timestamps), "to", max(timestamps), "\n")

# Counters and progress bar
total     <- length(timestamps)
completed <- 0L
skipped   <- 0L
pb <- txtProgressBar(min = 0, max = total, style = 3)
cat(sprintf("Processing %d timestamps…\n", total))

# Init empty containers
total_metrics <- data.frame()
all_temp_data <- data.frame()
mlr_residuals_all <- data.frame()

# Main loop over timestamps
for (i in seq_along(timestamps)) {
  ts <- timestamps[i]                                    
  dt_local <- ts_to_posix(ts)                     
  dt_lab   <- nice_dt(dt_local)                   
  dt_file  <- file_dt(dt_local)                   
  cat(sprintf("\n[%d/%d] Processing: timestamp = %s\n", i, total, dt_lab))  

  # ------------------------------------------------
  # STEP 1: Filter hourly data and extract covariates from spatial grid
  # ------------------------------------------------
  
  sel_temp_hm <- temp_clean_station_hm %>%
    filter(hour == ts)
  
  # Skip if fewer than 20 data points for current hour
  if (nrow(sel_temp_hm) < 20) {                                 
    skipped <- skipped + 1L                                       
    cat("  → skipped (n < 20)\n")                          
    next                                                         
  }

  # Convert Spatial object to RasterBrick
  r_brick <- brick(grids_sp)
  
  # Convert RasterBrick to terra SpatRaster object
  grid_terra <- rast(r_brick)
  
  # Extract raster values at the point locations
  extracted_vals <- terra::extract(grid_terra, sel_temp_hm)
  
  # Remove the first column (ID)
  extracted_vals <- extracted_vals[,-1]
  
  # Combine the point data with the extracted raster values
  temp_shm.ex <- cbind(as.data.frame(sel_temp_hm), extracted_vals)
  
  # Choose the covariates
  covars <- c("elevation", "canopy_height", "building_height")
  covars <- intersect(covars, names(temp_shm.ex))
  
  # Safe z-scaling within timestamp
  safe_z <- function(x) {
    m <- mean(x, na.rm = TRUE)
    s <- sd(x,   na.rm = TRUE)
    if (is.na(s) || s == 0) return(rep(0, length(x)))  
    (x - m) / s
  }
  
  # Add *_z columns
  for (nm in covars) {
    temp_shm.ex[[paste0(nm, "_z")]] <- safe_z(temp_shm.ex[[nm]])
  }
  
  # Keep the z parameters
  z_params_this <- data.frame(
    timestamp = ts,
    covariate = covars,
    mean      = sapply(temp_shm.ex[covars], function(v) mean(v, na.rm = TRUE)),
    sd        = sapply(temp_shm.ex[covars], function(v) sd(v,   na.rm = TRUE))
  )
  # Append to csv
  write.table(z_params_this, file = file.path(lm_dir_z, "z_params_by_timestamp.csv"),
              sep = ",", row.names = FALSE,
              col.names = !file.exists(file.path(lm_dir_z,"z_params_by_timestamp.csv")),
              append = TRUE)
  
  # ------------------------------------------------
  # STEP 2: Fit MLR
  # ------------------------------------------------
  
  # Fit multiple linear regression model
  reg_model <- lm(hm_temp ~ elevation + canopy_height + building_height, data = temp_shm.ex)
  summary_lm <- summary(reg_model)
  
  # Compute residuals and predictions
  pred <- predict(reg_model, temp_shm.ex)
  resid <- temp_shm.ex$hm_temp - pred
  
  # Extract per-timestamp residuals
  mlr_residuals_df <- temp_shm.ex %>%
    dplyr::transmute(
      timestamp   = ts,
      entity_id   = entity_id,      # station id; adjust if your column is named differently
      hm_temp_obs = hm_temp,        # observed temperature
      hm_temp_pred = pred,          # predicted by regular MLR
      residual    = resid,          # obs - pred
      elevation   = elevation,
      canopy_height = canopy_height,
      building_height = building_height
    )
  
  # Accumulate
  if (!exists("mlr_residuals_all") || nrow(mlr_residuals_all) == 0) {
    mlr_residuals_all <- mlr_residuals_df
  } else {
    mlr_residuals_all <- dplyr::bind_rows(mlr_residuals_all, mlr_residuals_df)
  }
    
  # Collect regression metrics
  model_metrics <- data.frame(
    timestamp      = ts,
    r_squared      = summary_lm$r.squared,
    adj_r_squared  = summary_lm$adj.r.squared,
    f_statistic    = summary_lm$fstatistic[1],
    f_pvalue       = pf(summary_lm$fstatistic[1],
                        summary_lm$fstatistic[2],
                        summary_lm$fstatistic[3],
                        lower.tail = FALSE),
    residual_se    = summary_lm$sigma,
    rmse           = sqrt(mean(resid^2)),
    mae            = mean(abs(resid)),
    AIC            = AIC(reg_model),
    BIC            = BIC(reg_model)
  )
  
  write.table(model_metrics, file = metrics_file, sep = ",", row.names = FALSE,
              col.names = !file.exists(metrics_file), append = TRUE)

  # Attach residuals to dataset and convert to sf for spatial operations
  temp_shm.ex$residuals <- resid
  temp_shm.sf <- st_as_sf(temp_shm.ex) |> st_cast("POINT")
  
  # ------------------------------------------------
  # STEP 3: Fit z-standardized MLR
  # ------------------------------------------------
  
  # Fit MLR with z-standardized predictors
  reg_model_z  <- lm(hm_temp ~ elevation_z + canopy_height_z + building_height_z,
                     data = temp_shm.ex)
  summary_lm_z <- summary(reg_model_z)
  
  # Predictions & Residuals for z-MLR
  pred_z  <- predict(reg_model_z, temp_shm.ex)
  resid_z <- temp_shm.ex$hm_temp - pred_z
  
  # Model-level metrics for z-MLR
  model_metrics_z <- data.frame(
    timestamp      = ts,
    r_squared      = summary_lm_z$r.squared,
    adj_r_squared  = summary_lm_z$adj.r.squared,
    f_statistic    = summary_lm_z$fstatistic[1],
    f_pvalue       = pf(summary_lm_z$fstatistic[1],
                        summary_lm_z$fstatistic[2],
                        summary_lm_z$fstatistic[3],
                        lower.tail = FALSE),
    residual_se    = summary_lm_z$sigma,
    rmse           = sqrt(mean(resid_z^2)),
    mae            = mean(abs(resid_z)),
    AIC            = AIC(reg_model_z),
    BIC            = BIC(reg_model_z)
  )
  
  write.table(model_metrics_z, file = metrics_file_z, sep = ",", 
              row.names = FALSE, col.names = !file.exists(metrics_file_z), append = TRUE)
  
  # robust SEs (HC3) for safer p-values
  rob <- coeftest(reg_model_z, vcov = vcovHC(reg_model_z, type = "HC3"))
  
  coef_df <- broom::tidy(rob) |>
    dplyr::filter(term != "(Intercept)") |>
    dplyr::transmute(
      timestamp     = ts,
      term,
      estimate_semi = estimate,      # °C per 1 SD of predictor (since X are z)
      se_robust     = std.error,
      t_robust      = statistic,
      p_robust      = p.value
    )
  
  # unitless β for cross-time comparability
  y_sd <- sd(temp_shm.ex$hm_temp, na.rm = TRUE)
  coef_df$beta_std <- coef_df$estimate_semi / y_sd
  
  # relative importance (LMG): who explains most variance at this hour?
  imp <- calc.relimp(reg_model_z, type = "lmg", rela = TRUE)
  coef_df$lmg <- imp$lmg[match(coef_df$term, names(imp$lmg))]  # sums to 1 per timestamp
  
  # append to tidy CSV
  write.table(coef_df, file = coef_file_z, sep = ",", row.names = FALSE,
              col.names = !file.exists(coef_file_z), append = TRUE)

  # ------------------------------------------------
  # STEP 4: Compute variogram and fit model
  # ------------------------------------------------
  
  # Compute empirical variogram map of model residuals
  # This helps visualize spatial autocorrelation in all directions
  varmap <- variogram(residuals ~ 1, data = temp_shm.sf, map = TRUE,
                    # cutoff is half the spatial extent (or diameter) of the study area
                    cutoff = sqrt(areaSpatialGrid(grids_sp)) / 2, 
                    # scaled relative to the resolution of the grid
                    width = 30 * grids_sp@grid@cellsize[1])

  # Store cutoff and bin width values for reuse and labeling
  cutoff_val <- sqrt(areaSpatialGrid(grids_sp)) / 2
  width_val  <- 30 * grids_sp@grid@cellsize[1]
  
  # Report how many rows are in variogram map
  cat("Variogram map has", nrow(as.data.frame(varmap)), "rows\n")
  
  # Define output filename for the variogram map plot
  file_varmap <- file.path(variogram_dir, paste0(dt_file, "_variogram_map.png"))  
  
  # Plot and save the directional variogram map
  png(file_varmap, width = 8, height = 6, units = "in", res = 300)
  print(
    plot(varmap,
         col.regions = grey(rev(seq(0, 1, 0.025))),
         main = sprintf("Directional Variogram Map (cutoff: %.1f km, width: %.1f km)\n%s",  
                        cutoff_val / 1000, width_val / 1000, dt_lab))
  )
  dev.off()
  
  # Compute directional variograms for North–South (0°) and East–West (90°)
  rv.temp <- variogram(residuals ~ 1, temp_shm.sf, alpha = c(0, 45, 90, 135))

  # Fit theoretical exponential variogram model to the directional variogram
  rvgm.temp <- fit.variogram(rv.temp,
      vgm(psill=var(temp_shm.sf$residuals),
          "Exp", nugget=0, anis= c(p=90, s=0.5))) # p = direction, s = anisotropy ratio
  
  # Define output filename for directional variogram fit plot
  file_varfit <- file.path(variogram_dir, paste0(dt_file, "_directional_variogram_fit.png"))  
 
  # Plot the empirical and fitted directional variograms and save
  png(file_varfit, width = 8, height = 6, units = "in", res = 300)
  print(
    plot(rv.temp, rvgm.temp,
         main = paste0("Directional Variogram Fit (0°, 45°, 90°, 135°)\n", dt_lab),   
         plot.nu = FALSE, cex = 1.5, pch = "+", col = "black")
  )
  dev.off()
    
  # Update progress bar
  completed <- completed + 1L 
  setTxtProgressBar(pb, completed)
  cat("\n")
  cat(sprintf("  → done. Completed: %d/%d (skipped: %d)\n",      
              completed, total, skipped))                        
}

# Close progress bar + final summary
close(pb)                                                        
cat(sprintf("\nFinished. Completed: %d/%d | Skipped: %d\n",       
            completed, total, skipped))                         

# Save MLR metrics
mlr_residuals_file <- file.path(output_dir, "mlr_residuals_all.csv")
if (exists("mlr_residuals_all") && nrow(mlr_residuals_all) > 0) {
  mlr_residuals_all$datetime_local <- ts_to_posix(mlr_residuals_all$timestamp)
  mlr_residuals_all <- dplyr::relocate(mlr_residuals_all, datetime_local, .after = timestamp)
  write.csv(mlr_residuals_all, mlr_residuals_file, row.names = FALSE)
}
```
