---
title: "collinearity_covariates"
author: "Johannes Gruenewald"
date: "2025-07-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Load Required Libraries

# Spatial Data Handling & Processing
library(sf)
library(sp)
library(terra)
library(raster)
library(stars)
library(spdep)

# Spatial Analysis & Interpolation
library(gstat)
library(randomForest)
library(nlme)
library(automap)
library(mgcv)

# Data Manipulation & Wrangling
library(tidyverse)

# Data Visualization
library(ggplot2)
library(scales)
library(gridExtra)
library(corrplot)

# Mapping & Interactive Visualization
library(tmap)

# Additional
library(lattice)
library(car)
```

------------------------------------------------------------------------

```{r Load and preprocess data}
# ================================================
# DATA LOADING, CLEANING, AND COVARIATE PREPARATION
# ================================================

# ================================================
# Complete data import and pre-processing

# # Read temperature data for the entire year of 2023
# temp <- sf::read_sf('../data/temp_HD/sensor_data_year_2023.geojson')
# 
# # Removes rows where temp is NA
# temp <- temp %>% filter(!is.na(temperature))
# 
# # Reproject the data for Germany
# temp <- st_transform(temp, crs="EPSG:32632")
# 
# # Clean the data from outliers -> currently working with hard, manual breaks
# # Currently nothing is removed by this step because no measurements are above 40°C for the year 2023
# temp_clean <- temp %>% filter(temperature >= -35, temperature <= 40)
# 
# # Calculate hour count since epoch, rounded down to current hour
# temp_clean$chour <- floor(as.numeric(temp_clean$dateobserved) / 3600)
# 
# # Drop all measurement stations which didn't collect data the entire year around
# temp_clean_station <- temp_clean %>%
#   mutate(dateobserved = as.POSIXct(dateobserved, tz = "UTC")) %>%
#   filter(!entity_id %in% c(
#     "hd:DE_Heidelberg_69120_12:WeatherObserved",
#     "hd:DE_Heidelberg_69120_34:WeatherObserved",
#     "hd:DE_Heidelberg_69123_41:WeatherObserved",
#     "hd:DE_Heidelberg_46:WeatherObserved"
#   ))
# 
# # Check if exactly 4 stations were dropped
# length(unique(na.omit(temp_clean$entity_id)))
# length(unique(na.omit(temp_clean_station$entity_id)))

# Calculate hourly mean temp for cleaned data
# temp_clean_station_hm <- temp_clean_station %>%
#     group_by(entity_id, chour) %>%
#     summarise(hm_temp = mean(temperature, na.rm = TRUE), .groups = "drop") %>%
#     mutate(hour = as.POSIXct(chour * 3600, origin = "1970-01-01", tz = "Europe/Berlin"))

# ================================================
# Import of pre-processed temp data and import and pre-processing of covariates 

# consistent timezone for the whole script
tz_berlin <- "Europe/Berlin"

#Shortcut -> import pre-processed hourly means
temp_clean_station_hm <- sf::read_sf('../data/temp_HD/temp_clean_station_hm.gpkg')

temp_clean_station_hm <- st_transform(temp_clean_station_hm, crs="EPSG:32632")

# Read admin boundaries of HD
hd_bounds <- sf::read_sf('../data/AOI/OSM_boundaries_HD.geojson')

# Reproject
hd_bounds <- st_transform(hd_bounds, crs="EPSG:32632")

# Load the elevation data
dem <- terra::rast("../data/DEM/hd_elevation_4326.tif")

# Reproject the elevation data to match correct CRS
dem <- terra::project(dem, y = crs(temp_clean_station_hm))

# Load the elevation data
tch <- terra::rast("../data/canopy_height/hd_canopy_height_4326.tif")

# Mask out all values which are assigned to water and have value 101
# Not part of this area. But needs to be masked as well: 102 Snow/ice; 103 No data
tch[tch == 101] <- NA

# Reproject the elevation data to match correct CRS
tch <- terra::project(tch, y = crs(temp_clean_station_hm))

# Check if the two covariates have the same crs
crs(tch) == crs(dem)

# Load the elevation data
ghs_bh <- terra::rast("../data/building_height/hd_building_height_4326.tif")

# Reproject the elevation data to match correct CRS
ghs_bh <- terra::project(ghs_bh, y = crs(temp_clean_station_hm))

# Check if CRS is correct
crs(ghs_bh) == crs(dem)

# ================================================
# Create a Raster Stack for all covariate information

# Check resolution of each raster
terra::res(dem)
terra::res(tch)
terra::res(ghs_bh)

# Align TCH and building height to DEM resolution and extent
tch_aligned    <- terra::resample(tch, dem, method = "bilinear")
ghs_bh_aligned <- terra::resample(ghs_bh, dem, method = "near")

# Stack all layers
env_stack <- c(dem, tch_aligned, ghs_bh_aligned)

# Rename layers for clarity
names(env_stack) <- c("elevation", "canopy_height", "building_height")

# ================================================
# Code to include lat and lon information in raster stack

# Also add lat and lon to the raster stack
# Create coordinate rasters
lon <- terra::xFromCell(env_stack, 1:ncell(env_stack))
lat <- terra::yFromCell(env_stack, 1:ncell(env_stack))

# Convert to raster layers
lon_raster <- dem
lat_raster <- dem
values(lon_raster) <- lon
values(lat_raster) <- lat
names(lon_raster) <- "lon"
names(lat_raster) <- "lat"

# Stack the three raster layers
grids <- c(env_stack, lon_raster, lat_raster)

# ================================================
# Conversion when running the analysis without proximity to water
# Preparation of grid as a covariate for running RK

# Convert SpatRaster to RasterLayer (raster package)
grids_ras <- stack(grids)

# Now convert to SpatialGridDataFrame
grids_sp <- as(grids_ras, "SpatialGridDataFrame") 

grids_sp@data

# Extract coordinates of the measurement stations
# maybe use them as covariates

# ================================================
# # Extract coordinates and data into a data frame
# coords <- st_coordinates(temp_clean_station_hm)
# locs <- cbind(temp_clean_station_hm, coords)
# locs <- as.data.frame(locs)
# 
# # Keep only one row per station and select relevant columns
# station_coords <- locs %>%
#   dplyr::distinct(entity_id, .keep_all = TRUE) %>%
#   dplyr::select(entity_id, X, Y)
# 
# coordinates(station_coords) <- ~ X + Y
# proj4string(station_coords) <- CRS(proj4string(grids_sp))
# 
# length(unique(stations$entity_id))

# Shortcut: load the pre-processed layer
station_coords_sf <- sf::read_sf('../data/temp_HD/station_coords.gpkg')

# Reproject
station_coords_sf <- st_transform(station_coords_sf, crs="EPSG:32632")

# Convert to SpatialPointsDataFrame
station_coords <- as(station_coords_sf, "Spatial")
```

Set Gaussian filter

```{r Set Gaussian filter}
# The logic: Trees have a cooling effect on the surrounding area via shade, evapotranspiration, etc.
# That effect is strongest closest to the tree and fades with distance.
# A Gaussian filter models this natural distance decay very well.

#set margins for plots
par(mfrow = c(2, 1), oma = c(0, 1, 0, 1))

# Gaussian filter with radius 60 m and default sigma (≈ radius/3)
w <- focalWeight(tch, d = 60, type = "Gauss")

# Apply to tree canopy height to model decaying influence
tch <- focal(tch, w = w, fun = sum, na.rm = TRUE)

plot(tch, main ="Original Input Raster Tree Canopy Height ") #plot orig
plot(tch, main="Gaussian Tree Cooling Effect (60 m)") #plot Gaussian filter in 60 m distaince
```

--------------------------------------------------

```{r Plot all covariates}

# Add Heidelberg district boundaries to plots
hd_bounds_sp <- as(hd_bounds, "Spatial")
hd_bounds_sp <- spTransform(as(hd_bounds, "Spatial"), crs(grids_sp))

bounds.layer <- list("sp.polygons", hd_bounds_sp, lwd = 2, col = "black", first = FALSE)

# Elevation
p1 <- spplot(grids_sp["elevation"],    
             main = "Elevation\nat 23,5 m spatial resolution",
             sub = list("Source: Data from Shuttle Radar Topography Mission (SRTM)", cex = 0.8),
             col.regions = viridis::viridis(100),
             sp.layout = list(bounds.layer),
             colorkey = list(space = "right", title = "Elevation [m]"))

# Tree Canopy
p2 <- spplot(grids_sp["canopy_height"],  
             main = "Global Forest Canopy Height\nresampled to 23,5 m spatial resolution",
             sub = list("Source: Potapov et al. (2021)", cex = 0.8),
             col.regions = rev(hcl.colors(100, "YlGn")),
             sp.layout = list(bounds.layer),
             colorkey = list(space = "right", title = "Forest Canopy Height [m]"))

# Building Height
p3 <- spplot(grids_sp["building_height"],    
             main = "Global Building Height\nresampled to 23,5 m spatial resolution",
             sub = list("Source: GHS-BUILT-H - R2023A from Global Human Settlement Layer", cex = 0.8),
             col.regions = hcl.colors(100, "YlOrBr"),
             sp.layout = list(bounds.layer),
             colorkey = list(space = "right", title = "Building Height [m]"))

p1
p2
p3

# Save as png
# Elevation
png("../plots/elevation.png", width = 2000, height = 1600, res = 300)
print(p1)
dev.off()

# Tree Canopy
png("../plots/canopy.png", width = 2000, height = 1600, res = 300)
print(p2)
dev.off()

# Building Height
png("../plots/building_height.png", width = 2000, height = 1600, res = 300)
print(p3)
dev.off()
```

------------------------------------------------------------------------

Calculate correlation between the three covariates

```{r Calculate correlation between the three covariates}
# Convert Spatial object into plain data.frame
covariates_df <- as.data.frame(grids_sp)  

# Show descriptive statistics to check for NAs
summary(covariates_df[c("elevation", "canopy_height", "building_height")])  

# Correlations (Pearson + Spearman)
corr_p <- cor(covariates_df[c("elevation","canopy_height","building_height")],
              use="pairwise.complete.obs", method="pearson")
corr_s <- cor(covariates_df[c("elevation","canopy_height","building_height")],
              use="pairwise.complete.obs", method="spearman")

corr_p
corr_s

# Visualize the correlation matrix as a color heatmap, with values and formatted labels
png("../plots/correlation_covariates_pearson.png", width = 2000, height = 1600, res = 300)

corrplot(corr_p, method = "color", addCoef.col = "black", tl.col = "black", tl.srt = 45,
         number.cex = 0.8)

dev.off()

# same for spearman
png("../plots/correlation_covariates_spearman.png", width = 2000, height = 1600, res = 300)

corrplot(corr_s, method = "color", addCoef.col = "black", tl.col = "black", tl.srt = 45,
         number.cex = 0.8)

dev.off()
```

The results fit really well to Heidelberg and its surroundings and describe how they look like.

-	Elevation vs. Canopy (r ≈ 0.83): Strong positive correlation — higher hills (e.g., Heiligenberg, Königstuhl) tend to have taller/denser canopy.
-	Elevation vs. Building height (r ≈ −0.34): Moderate negative correlation — taller buildings are concentrated at lower elevations (valley floor).
-	Canopy vs. Building height (r ≈ −0.36): Moderate negative correlation — built-up areas generally coincide with less canopy, and vice versa (they tend not to co-occur).

--------------------------------------------------

```{r Distribution of covariates}
par(mfrow = c(1,3))
terra::hist(env_stack$elevation,       main = "Elevation",       xlab = "m")
terra::hist(env_stack$canopy_height,   main = "Canopy height",   xlab = "m")
terra::hist(env_stack$building_height, main = "Building height", xlab = "m")
par(mfrow = c(1,1))
```


------------------------------------------------------------------------

```{r Adding covariates to temperature data}

# Convert Spatial object to RasterBrick -> SpatRaster
r_brick   <- brick(grids_sp)
grid_terra <- rast(r_brick)

# Extract raster values at point features
ex <- terra::extract(grid_terra, temp_clean_station_hm)

# Aggregate duplicates to a single row per original feature
# Use mean
ex_agg <- aggregate(. ~ ID, data = ex, FUN = mean, na.rm = TRUE)

# Reorder aggregated rows to match original feature order
ex_agg <- ex_agg[match(seq_len(nrow(temp_clean_station_hm)), ex_agg$ID), -1]

# Bind attributes + extracted covariates
temp_clean_station_hm_cov <- cbind(as.data.frame(temp_clean_station_hm), ex_agg)

str(temp_clean_station_hm_cov)
```

Z-score normalization: mean of 0 and standard deviation of 1

```{r}
covars <- c("elevation", "canopy_height", "building_height")   # add "lon","lat" if you want

# learn mean/sd
mu  <- sapply(temp_clean_station_hm_cov[, covars], function(x) mean(x, na.rm = TRUE))
sdv <- sapply(temp_clean_station_hm_cov[, covars], function(x) sd(x,   na.rm = TRUE))

# guard against zero-variance columns
zero_sd <- sdv == 0 | is.na(sdv)
if (any(zero_sd)) {
  warning("Zero-variance covariate(s) set to 0: ",
          paste(names(sdv)[zero_sd], collapse = ", "))
  sdv[zero_sd] <- 1
}

# add *_z columns
for (nm in covars) {
  temp_clean_station_hm_cov[[paste0(nm, "_z")]] <-
    (temp_clean_station_hm_cov[[nm]] - mu[[nm]]) / sdv[[nm]]
}

# (optional) keep the parameters for reproducibility
z_params <- data.frame(covariate = names(mu), mean = as.numeric(mu), sd = as.numeric(sdv))
```

```{r}
# long format for faceting
df_long <- temp_clean_station_hm_cov %>%
  select(all_of(covars)) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "value") %>%
  filter(!is.na(value))

# histograms
p_cov <- ggplot(df_long, aes(x = value)) +
  geom_histogram(bins = 40, alpha = 0.85) +
  facet_wrap(~ variable, scales = "free", ncol = 1) +
  labs(
    title = "Distributions of Covariates",
    x = NULL, y = "Count"
  ) +
  theme_minimal(base_size = 13) +
  theme(panel.grid.minor = element_blank())

p_cov
```


------------------------------------------------------------------------

Calculate correlation between the three covariates and the dependent variable temperature using a linear regression model

```{r Calculate correlation between the three covariates and the dependent variable temperature}

# Pick relevant columns
df <- temp_clean_station_hm_cov %>%
  st_drop_geometry() %>%   # remove geometry column
  dplyr::select(entity_id, hm_temp, elevation, canopy_height, building_height)

# Aggregate temperature per station (mean across time)
station_df <- df %>%
  group_by(entity_id, elevation, canopy_height, building_height) %>%
  summarise(mean_temp = mean(hm_temp, na.rm = TRUE), .groups = "drop")

# Compute correlation matrix (Pearson + Spearman)
vars <- c("mean_temp", "elevation", "canopy_height", "building_height")

corr_p <- cor(station_df[vars], use = "pairwise.complete.obs", method = "pearson")
corr_s <- cor(station_df[vars], use = "pairwise.complete.obs", method = "spearman")

print("Pearson correlation:")
print(corr_p)

print("Spearman correlation:")
print(corr_s)

# reshape to long format
station_long <- station_df %>%
  pivot_longer(cols = c(elevation, canopy_height, building_height),
               names_to = "covariate", values_to = "value")

p_corr_temp_cov <- ggplot(station_long, aes(x = value, y = mean_temp)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = TRUE, color = "darkred") +
  facet_wrap(~ covariate, scales = "free_x", nrow = 1,
             labeller = labeller(
               covariate = c(elevation = "Elevation [m]",
                             canopy_height = "Canopy Height [m]",
                             building_height = "Building Height [m]")
             )) +
  scale_y_continuous(limits = c(9, 15)) +
  labs(title = "Yearly Mean Temperature per Station vs Environmental Covariates",
       x = NULL, y = "Mean Temperature [°C]") +
  theme_bw() +
  theme(strip.text = element_text(face = "bold"))

p_corr_temp_cov

# Save output to file
# ggsave("../plots/correlation_temperature_vs_covariates.png", p_corr_temp_cov, width = 12, height = 4, dpi = 300)
```

------------------------------------------------------------------------

Calculate correlation between the three covariates and the dependent variable temperature using a GAM model

```{r}
p_corr_temp_cov_gam <- ggplot(station_long, aes(x = value, y = mean_temp)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "gam", formula = y ~ s(x, k = 5), 
              se = TRUE, color = "darkblue") +
  facet_wrap(~ covariate, scales = "free_x", nrow = 1,
             labeller = labeller(
               covariate = c(elevation = "Elevation [m]",
                             canopy_height = "Canopy Height [m]",
                             building_height = "Building Height [m]")
             )) +
  scale_y_continuous(limits = c(9, 15)) +
  labs(title = "Yearly Mean Temperature per Station vs Environmental Covariates (GAM fit)",
       x = NULL, y = "Mean Temperature [°C]") +
  theme_bw() +
  theme(strip.text = element_text(face = "bold"))

p_corr_temp_cov_gam
```

------------------------------------------------------------------------

Comparison between linear and non-linear model

```{r}
# Elevation
lm(mean_temp ~ elevation, data = station_df)
gam(mean_temp ~ s(elevation, k = 5), data = station_df)

# Building height
lm(mean_temp ~ building_height, data = station_df)
gam(mean_temp ~ s(building_height, k = 5), data = station_df)

# Canopy height
lm(mean_temp ~ canopy_height, data = station_df)
gam(mean_temp ~ s(canopy_height, k = 5), data = station_df)
```

------------------------------------------------------------------------

Multicollinearity

Test for simple Linear Regression Model

In regression analysis, multicollinearity occurs when two or more predictor variables are highly correlated with each other, such that they do not provide unique or independent information in the regression model.

If the degree of correlation is high enough between predictor variables, it can cause problems when fitting and interpreting the regression model. 

The most straightforward way to detect multicollinearity in a regression model is by calculating a metric known as the variance inflation factor, often abbreviated VIF.

VIF measures the strength of correlation between predictor variables in a model. It takes on a value between 1 and positive infinity.

We use the following rules of thumb for interpreting VIF values:

- VIF = 1: There is no correlation between a given predictor variable and any other predictor variables in the model.
- VIF between 1 and 5: There is moderate correlation between a given predictor variable and other predictor variables in the model.
- VIF > 5: There is severe correlation between a given predictor variable and other predictor variables in the model.

Source: https://www.statology.org/multicollinearity-in-r/

```{r Check for multicollineraity}

#define multiple linear regression model
mlr_coll <- lm(hm_temp ~ elevation + canopy_height + building_height, data = temp_clean_station_hm_cov)

mlr_coll

#calculate the VIF for each predictor variable in the model
vif(mlr_coll)
```

None of the predictors (elevation, canopy height, building height) show problematic multicollinearity. All VIFs are under 5, suggesting the model is stable with respect to predictor relationships.

------------------------------------------------------------------------

Spatial Autocorrelation of MLR model residuals

```{r Loop to calculate Morans I on regression residuals}
# Import residuals
mlr_residuals <- read.csv(
  "../regression_test_residuals/mlr_residuals_all.csv",
  stringsAsFactors = FALSE
)

# Extract coordinates from station layer
station_coords_df <- station_coords_sf %>%
  st_drop_geometry() %>%
  mutate(
    lon = st_coordinates(station_coords_sf)[, 1],
    lat = st_coordinates(station_coords_sf)[, 2]
  ) %>%
  select(entity_id, lon, lat)

# Parse timestamps and join coordinates
mlr_residuals <- mlr_residuals %>%
  mutate(
    hour = suppressWarnings(ymd_hms(timestamp, tz = "Europe/Berlin")),
    hour = ifelse(is.na(hour), ymd(timestamp, tz = "Europe/Berlin"), hour),
    hour = as.POSIXct(hour, origin = "1970-01-01", tz = "Europe/Berlin"),
    day  = as.Date(hour, tz = "Europe/Berlin")
  ) %>%
  left_join(station_coords_df, by = "entity_id")

# +Convert to sf object with CRS 32632
mlr_residuals_sf <- st_as_sf(
  mlr_residuals,
  coords = c("lon", "lat"),
  crs = 32632,      
  remove = FALSE
)

# Loop across days using the sf version
month_start <- as.Date("2023-07-01")
month_end   <- as.Date("2023-07-31")
all_days    <- seq(month_start, month_end, by = "day")

for (one_day in all_days) {
  message("Processing ", one_day)

  # Filter to that day directly from sf object
  df_day <- mlr_residuals_sf %>%
    filter(day == one_day)

  if (nrow(df_day) == 0) next

  # Compute Moran’s I for each timestamp
  moran_results <- df_day %>%
    group_split(hour) %>%
    map_dfr(function(df_slice) {

      xy <- st_coordinates(df_slice)

      knn <- knearneigh(xy, k = 8)
      nb  <- knn2nb(knn)
      lw  <- nb2listw(nb, style = "W", zero.policy = TRUE)

      mt <- moran.test(df_slice$residual, lw, zero.policy = TRUE)
      mc <- moran.mc(df_slice$residual, lw, nsim = 999, zero.policy = TRUE)
      q  <- quantile(mc$res, c(0.025, 0.975))

      tibble(
        hour     = unique(df_slice$hour),
        moran_i  = mt$estimate[["Moran I statistic"]],
        expected = mt$estimate[["Expectation"]],
        p_asym   = mt$p.value,
        p_mc     = mc$p.value,
        lo95     = q[1],
        hi95     = q[2]
      )
    })
  
  # Convert one_day to proper Date for plotting and filenames
  day_label <- as.Date(one_day, origin = "1970-01-01")
  
  # Plot
  p_moran <- ggplot(moran_results, aes(x = hour, y = moran_i)) +
    geom_ribbon(aes(ymin = lo95, ymax = hi95, fill = "95% confidence envelope"), alpha = 0.5) +
    geom_hline(
      data = data.frame(y = 0, label = "No autocorrelation"),
      aes(yintercept = y, linetype = label),
      color = "black"
    ) +
    geom_hline(
      data = data.frame(y = unique(moran_results$expected), label = "Expected under randomness"),
      aes(yintercept = y, linetype = label),
      color = "black"
    ) +
    geom_line(color = "black", linewidth = 0.6) +
    geom_point(aes(color = p_mc < 0.05), size = 2.8) +
    scale_color_manual(
      name = "Significance (p < 0.05)",
      values = c("TRUE" = "#440154", "FALSE" = "#35b779"),
      labels = c("TRUE" = "Significant", "FALSE" = "Not significant")
    ) +
    scale_fill_manual(name = NULL, values = c("95% confidence envelope" = "grey80")) +
    scale_linetype_manual(
      name = NULL,
      values = c("No autocorrelation" = "dashed",
                 "Expected under randomness" = "dotted")
    ) +
    scale_x_datetime(
      date_labels = "%H:%M",
      date_breaks = "6 hours",
      expand = expansion(mult = c(0, 0.02))
    ) +
    coord_cartesian(ylim = c(-0.2, 0.6)) +
    labs(
      title = "Moran’s I of Regression Residuals",
      subtitle = paste("Date:", format(day_label, "%Y-%m-%d")),
      x = "Hour of Day",
      y = "Moran’s I"
    ) +
    guides(
      color = guide_legend(order = 1),
      fill = guide_legend(order = 2, override.aes = list(alpha = 0.5)),
      linetype = guide_legend(order = 3)
    ) +
    theme_minimal(base_size = 13) +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      legend.position = "bottom",
      legend.box = "horizontal",
      legend.title = element_text(size = 6, face = "bold"),
      legend.text = element_text(size = 6),
      plot.title = element_text(face = "bold", size = 14),
      plot.subtitle = element_text(size = 11, color = "grey20"),
      panel.grid.minor = element_blank()
    )

  # Save plot
  out_file <- file.path("../plots/moransI_residuals", paste0("moransI_residuals_", one_day, ".png"))
  ggsave(out_file, p_moran, width = 8, height = 5, dpi = 300)
}
```

------------------------------------------------------------------------

GAM for all three covariates

Comment: A general rule of thumb is to use a high number of splines and use cross-validation of lambda (λ) values to find the model that generalises best. Remember we can have different splines and lambda values for every variable in our model.

```{r}
gam_all <- gam(mh_temp ~ 
                  s(elev) + 
                  s(canopy) + 
                  s(bldg), 
                data = temp_shm.ex)
summary(gam_all)

concurvity(gam_all, full = TRUE)
```



Example: Visualize the results

```{r}
# Vary elevation across its range; fix the others at their median
predict_temp <- data.frame(
  elevation = seq(min(temp_shm.ex$elev), max(temp_shm.ex$elev), length.out = 20),
  tree_canopy_height = median(temp_shm.ex$canopy),
  building_height = median(temp_shm.ex$bldg)
)

# Predict using the full GAM model
predictions <- predict(gam_all, newdata = predict_temp, type = "response", se.fit = TRUE)

# Plot the data and the GAM fit
ggplot() +
  geom_point(data = temp_shm.ex, aes(x = elevation, y = mh_temp)) +
  geom_line(data = data.frame(elevation = predict_temp$elevation, mh_temp = predictions$fit), 
            aes(x = elevation, y = mh_temp), color = "blue", size = 1) +
  geom_ribbon(data = data.frame(elevation = predict_temp$elevation, fit = predictions$fit, 
                                se = predictions$se.fit), aes(x = elevation, 
                                ymin = fit - 1.96 * se, 
                                ymax = fit + 1.96 * se), alpha = 0.3) +
  labs(title = "GAM Fit for Monthly Mean Temperature vs. Elevation", 
       x = "Elevation [m]", y = "Monthly Mean Temperature [°C]") +
  theme_minimal()
```
